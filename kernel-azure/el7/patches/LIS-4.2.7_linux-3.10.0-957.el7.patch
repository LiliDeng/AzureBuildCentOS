diff -Naur linux-3.10.0-957.el7.orig/arch/x86/hyperv/hv_init.c linux-3.10.0-957.el7.lis/arch/x86/hyperv/hv_init.c
--- linux-3.10.0-957.el7.orig/arch/x86/hyperv/hv_init.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/hyperv/hv_init.c	2018-12-12 00:51:40.659785603 +0000
@@ -19,14 +19,14 @@
 
 #include <linux/types.h>
 #include <asm/hypervisor.h>
-#include <asm/hyperv.h>
-#include <asm/mshyperv.h>
+#include "include/asm/hyperv.h"
+#include "include/asm/mshyperv.h"
 #include <asm/fixmap.h>
 #include <linux/version.h>
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
 #include <linux/clockchips.h>
-#include <linux/hyperv.h>
+#include "include/linux/hyperv.h"
 #include <linux/slab.h>
 #include <linux/cpu.h>
 #include <linux/kaiser.h>
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/asm/hyperv.h linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/asm/hyperv.h
--- linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/asm/hyperv.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/asm/hyperv.h	2018-12-12 00:51:40.634785762 +0000
@@ -0,0 +1,389 @@
+#ifndef _ASM_X86_HYPERV_H
+#define _ASM_X86_HYPERV_H
+
+#include <linux/types.h>
+
+/*
+ * The below CPUID leaves are present if VersionAndFeatures.HypervisorPresent
+ * is set by CPUID(HvCpuIdFunctionVersionAndFeatures).
+ */
+#define HYPERV_CPUID_VENDOR_AND_MAX_FUNCTIONS	0x40000000
+#define HYPERV_CPUID_INTERFACE			0x40000001
+#define HYPERV_CPUID_VERSION			0x40000002
+#define HYPERV_CPUID_FEATURES			0x40000003
+#define HYPERV_CPUID_ENLIGHTMENT_INFO		0x40000004
+#define HYPERV_CPUID_IMPLEMENT_LIMITS		0x40000005
+
+#define HYPERV_HYPERVISOR_PRESENT_BIT		0x80000000
+#define HYPERV_CPUID_MIN			0x40000005
+#define HYPERV_CPUID_MAX			0x4000ffff
+
+/*
+ * Feature identification. EAX indicates which features are available
+ * to the partition based upon the current partition privileges.
+ */
+
+/* VP Runtime (HV_X64_MSR_VP_RUNTIME) available */
+#define HV_X64_MSR_VP_RUNTIME_AVAILABLE		(1 << 0)
+/* Partition Reference Counter (HV_X64_MSR_TIME_REF_COUNT) available*/
+#define HV_X64_MSR_TIME_REF_COUNT_AVAILABLE	(1 << 1)
+/* Partition reference TSC MSR is available */
+#define HV_X64_MSR_REFERENCE_TSC_AVAILABLE              (1 << 9)
+
+/* A partition's reference time stamp counter (TSC) page */
+#define HV_X64_MSR_REFERENCE_TSC		0x40000021
+
+/*
+ * There is a single feature flag that signifies if the partition has access
+ * to MSRs with local APIC and TSC frequencies.
+ */
+#define HV_X64_ACCESS_FREQUENCY_MSRS		(1 << 11)
+
+/*
+ * Basic SynIC MSRs (HV_X64_MSR_SCONTROL through HV_X64_MSR_EOM
+ * and HV_X64_MSR_SINT0 through HV_X64_MSR_SINT15) available
+ */
+#define HV_X64_MSR_SYNIC_AVAILABLE		(1 << 2)
+/*
+ * Synthetic Timer MSRs (HV_X64_MSR_STIMER0_CONFIG through
+ * HV_X64_MSR_STIMER3_COUNT) available
+ */
+#define HV_X64_MSR_SYNTIMER_AVAILABLE		(1 << 3)
+/*
+ * APIC access MSRs (HV_X64_MSR_EOI, HV_X64_MSR_ICR and HV_X64_MSR_TPR)
+ * are available
+ */
+#define HV_X64_MSR_APIC_ACCESS_AVAILABLE	(1 << 4)
+/* Hypercall MSRs (HV_X64_MSR_GUEST_OS_ID and HV_X64_MSR_HYPERCALL) available*/
+#define HV_X64_MSR_HYPERCALL_AVAILABLE		(1 << 5)
+/* Access virtual processor index MSR (HV_X64_MSR_VP_INDEX) available*/
+#define HV_X64_MSR_VP_INDEX_AVAILABLE		(1 << 6)
+/* Virtual system reset MSR (HV_X64_MSR_RESET) is available*/
+#define HV_X64_MSR_RESET_AVAILABLE		(1 << 7)
+ /*
+  * Access statistics pages MSRs (HV_X64_MSR_STATS_PARTITION_RETAIL_PAGE,
+  * HV_X64_MSR_STATS_PARTITION_INTERNAL_PAGE, HV_X64_MSR_STATS_VP_RETAIL_PAGE,
+  * HV_X64_MSR_STATS_VP_INTERNAL_PAGE) available
+  */
+#define HV_X64_MSR_STAT_PAGES_AVAILABLE		(1 << 8)
+
+/* Frequency MSRs available */
+#define HV_FEATURE_FREQUENCY_MSRS_AVAILABLE	(1 << 8)
+
+/* Crash MSR available */
+#define HV_FEATURE_GUEST_CRASH_MSR_AVAILABLE (1 << 10)
+
+/*
+ * Feature identification: EBX indicates which flags were specified at
+ * partition creation. The format is the same as the partition creation
+ * flag structure defined in section Partition Creation Flags.
+ */
+#define HV_X64_CREATE_PARTITIONS		(1 << 0)
+#define HV_X64_ACCESS_PARTITION_ID		(1 << 1)
+#define HV_X64_ACCESS_MEMORY_POOL		(1 << 2)
+#define HV_X64_ADJUST_MESSAGE_BUFFERS		(1 << 3)
+#define HV_X64_POST_MESSAGES			(1 << 4)
+#define HV_X64_SIGNAL_EVENTS			(1 << 5)
+#define HV_X64_CREATE_PORT			(1 << 6)
+#define HV_X64_CONNECT_PORT			(1 << 7)
+#define HV_X64_ACCESS_STATS			(1 << 8)
+#define HV_X64_DEBUGGING			(1 << 11)
+#define HV_X64_CPU_POWER_MANAGEMENT		(1 << 12)
+#define HV_X64_CONFIGURE_PROFILER		(1 << 13)
+
+/*
+ * Feature identification. EDX indicates which miscellaneous features
+ * are available to the partition.
+ */
+/* The MWAIT instruction is available (per section MONITOR / MWAIT) */
+#define HV_X64_MWAIT_AVAILABLE				(1 << 0)
+/* Guest debugging support is available */
+#define HV_X64_GUEST_DEBUGGING_AVAILABLE		(1 << 1)
+/* Performance Monitor support is available*/
+#define HV_X64_PERF_MONITOR_AVAILABLE			(1 << 2)
+/* Support for physical CPU dynamic partitioning events is available*/
+#define HV_X64_CPU_DYNAMIC_PARTITIONING_AVAILABLE	(1 << 3)
+/*
+ * Support for passing hypercall input parameter block via XMM
+ * registers is available
+ */
+#define HV_X64_HYPERCALL_PARAMS_XMM_AVAILABLE		(1 << 4)
+/* Support for a virtual guest idle state is available */
+#define HV_X64_GUEST_IDLE_STATE_AVAILABLE		(1 << 5)
+
+/*
+ * Implementation recommendations. Indicates which behaviors the hypervisor
+ * recommends the OS implement for optimal performance.
+ */
+ /*
+  * Recommend using hypercall for address space switches rather
+  * than MOV to CR3 instruction
+  */
+#define HV_X64_AS_SWITCH_RECOMMENDED		(1 << 0)
+/* Recommend using hypercall for local TLB flushes rather
+ * than INVLPG or MOV to CR3 instructions */
+#define HV_X64_LOCAL_TLB_FLUSH_RECOMMENDED	(1 << 1)
+/*
+ * Recommend using hypercall for remote TLB flushes rather
+ * than inter-processor interrupts
+ */
+#define HV_X64_REMOTE_TLB_FLUSH_RECOMMENDED	(1 << 2)
+/*
+ * Recommend using MSRs for accessing APIC registers
+ * EOI, ICR and TPR rather than their memory-mapped counterparts
+ */
+#define HV_X64_APIC_ACCESS_RECOMMENDED		(1 << 3)
+/* Recommend using the hypervisor-provided MSR to initiate a system RESET */
+#define HV_X64_SYSTEM_RESET_RECOMMENDED		(1 << 4)
+/*
+ * Recommend using relaxed timing for this partition. If used,
+ * the VM should disable any watchdog timeouts that rely on the
+ * timely delivery of external interrupts
+ */
+#define HV_X64_RELAXED_TIMING_RECOMMENDED	(1 << 5)
+
+/*
+ * HV_VP_SET available
+ */
+#define HV_X64_EX_PROCESSOR_MASKS_RECOMMENDED	(1 << 11)
+/*
+ * Virtual APIC support
+ */
+#define HV_X64_DEPRECATING_AEOI_RECOMMENDED	(1 << 9)
+
+/* Recommend using the newer ExProcessorMasks interface */
+#define HV_X64_EX_PROCESSOR_MASKS_RECOMMENDED	(1 << 11)
+
+/*
+ * Crash notification flag.
+ */
+#define HV_CRASH_CTL_CRASH_NOTIFY (1ULL << 63)
+
+/* MSR used to identify the guest OS. */
+#define HV_X64_MSR_GUEST_OS_ID			0x40000000
+
+/* MSR used to setup pages used to communicate with the hypervisor. */
+#define HV_X64_MSR_HYPERCALL			0x40000001
+
+/* MSR used to provide vcpu index */
+#define HV_X64_MSR_VP_INDEX			0x40000002
+
+/* MSR used to reset the guest OS. */
+#define HV_X64_MSR_RESET			0x40000003
+
+/* MSR used to provide vcpu runtime in 100ns units */
+#define HV_X64_MSR_VP_RUNTIME			0x40000010
+
+/* MSR used to read the per-partition time reference counter */
+#define HV_X64_MSR_TIME_REF_COUNT		0x40000020
+
+/* MSR used to retrieve the TSC frequency */
+#define HV_X64_MSR_TSC_FREQUENCY		0x40000022
+
+/* MSR used to retrieve the local APIC timer frequency */
+#define HV_X64_MSR_APIC_FREQUENCY		0x40000023
+
+/* Define the virtual APIC registers */
+#define HV_X64_MSR_EOI				0x40000070
+#define HV_X64_MSR_ICR				0x40000071
+#define HV_X64_MSR_TPR				0x40000072
+#define HV_X64_MSR_APIC_ASSIST_PAGE		0x40000073
+
+/* Define synthetic interrupt controller model specific registers. */
+#define HV_X64_MSR_SCONTROL			0x40000080
+#define HV_X64_MSR_SVERSION			0x40000081
+#define HV_X64_MSR_SIEFP			0x40000082
+#define HV_X64_MSR_SIMP				0x40000083
+#define HV_X64_MSR_EOM				0x40000084
+#define HV_X64_MSR_SINT0			0x40000090
+#define HV_X64_MSR_SINT1			0x40000091
+#define HV_X64_MSR_SINT2			0x40000092
+#define HV_X64_MSR_SINT3			0x40000093
+#define HV_X64_MSR_SINT4			0x40000094
+#define HV_X64_MSR_SINT5			0x40000095
+#define HV_X64_MSR_SINT6			0x40000096
+#define HV_X64_MSR_SINT7			0x40000097
+#define HV_X64_MSR_SINT8			0x40000098
+#define HV_X64_MSR_SINT9			0x40000099
+#define HV_X64_MSR_SINT10			0x4000009A
+#define HV_X64_MSR_SINT11			0x4000009B
+#define HV_X64_MSR_SINT12			0x4000009C
+#define HV_X64_MSR_SINT13			0x4000009D
+#define HV_X64_MSR_SINT14			0x4000009E
+#define HV_X64_MSR_SINT15			0x4000009F
+
+/*
+ * Synthetic Timer MSRs. Four timers per vcpu.
+ */
+#define HV_X64_MSR_STIMER0_CONFIG		0x400000B0
+#define HV_X64_MSR_STIMER0_COUNT		0x400000B1
+#define HV_X64_MSR_STIMER1_CONFIG		0x400000B2
+#define HV_X64_MSR_STIMER1_COUNT		0x400000B3
+#define HV_X64_MSR_STIMER2_CONFIG		0x400000B4
+#define HV_X64_MSR_STIMER2_COUNT		0x400000B5
+#define HV_X64_MSR_STIMER3_CONFIG		0x400000B6
+#define HV_X64_MSR_STIMER3_COUNT		0x400000B7
+
+/* Hyper-V guest crash notification MSR's */
+#define HV_X64_MSR_CRASH_P0			0x40000100
+#define HV_X64_MSR_CRASH_P1			0x40000101
+#define HV_X64_MSR_CRASH_P2			0x40000102
+#define HV_X64_MSR_CRASH_P3			0x40000103
+#define HV_X64_MSR_CRASH_P4			0x40000104
+#define HV_X64_MSR_CRASH_CTL			0x40000105
+#define HV_X64_MSR_CRASH_CTL_NOTIFY		(1ULL << 63)
+#define HV_X64_MSR_CRASH_PARAMS		\
+		(1 + (HV_X64_MSR_CRASH_P4 - HV_X64_MSR_CRASH_P0))
+
+#define HV_X64_MSR_HYPERCALL_ENABLE		0x00000001
+#define HV_X64_MSR_HYPERCALL_PAGE_ADDRESS_SHIFT	12
+#define HV_X64_MSR_HYPERCALL_PAGE_ADDRESS_MASK	\
+		(~((1ull << HV_X64_MSR_HYPERCALL_PAGE_ADDRESS_SHIFT) - 1))
+
+/* Declare the various hypercall operations. */
+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE	0x0002
+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST	0x0003
+#define HVCALL_NOTIFY_LONG_SPIN_WAIT		0x0008
+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX  0x0013
+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST_EX   0x0014
+#define HVCALL_POST_MESSAGE			0x005c
+#define HVCALL_SIGNAL_EVENT			0x005d
+
+#define HV_X64_MSR_APIC_ASSIST_PAGE_ENABLE		0x00000001
+#define HV_X64_MSR_APIC_ASSIST_PAGE_ADDRESS_SHIFT	12
+#define HV_X64_MSR_APIC_ASSIST_PAGE_ADDRESS_MASK	\
+		(~((1ull << HV_X64_MSR_APIC_ASSIST_PAGE_ADDRESS_SHIFT) - 1))
+
+#define HV_X64_MSR_TSC_REFERENCE_ENABLE		0x00000001
+#define HV_X64_MSR_TSC_REFERENCE_ADDRESS_SHIFT	12
+
+#define HV_PROCESSOR_POWER_STATE_C0		0
+#define HV_PROCESSOR_POWER_STATE_C1		1
+#define HV_PROCESSOR_POWER_STATE_C2		2
+#define HV_PROCESSOR_POWER_STATE_C3		3
+
+#define HV_FLUSH_ALL_PROCESSORS			BIT(0)
+#define HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES	BIT(1)
+#define HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY	BIT(2)
+#define HV_FLUSH_USE_EXTENDED_RANGE_FORMAT	BIT(3)
+
+enum HV_GENERIC_SET_FORMAT {
+	HV_GENERIC_SET_SPARCE_4K,
+	HV_GENERIC_SET_ALL,
+};
+
+/* hypercall status code */
+#define HV_STATUS_SUCCESS			0
+#define HV_STATUS_INVALID_HYPERCALL_CODE	2
+#define HV_STATUS_INVALID_HYPERCALL_INPUT	3
+#define HV_STATUS_INVALID_ALIGNMENT		4
+#define HV_STATUS_INSUFFICIENT_MEMORY		11
+#define HV_STATUS_INVALID_CONNECTION_ID		18
+#define HV_STATUS_INSUFFICIENT_BUFFERS		19
+
+typedef struct _HV_REFERENCE_TSC_PAGE {
+	__u32 tsc_sequence;
+	__u32 res1;
+	__u64 tsc_scale;
+	__s64 tsc_offset;
+} HV_REFERENCE_TSC_PAGE, *PHV_REFERENCE_TSC_PAGE;
+
+/* Define the number of synthetic interrupt sources. */
+#define HV_SYNIC_SINT_COUNT		(16)
+/* Define the expected SynIC version. */
+#define HV_SYNIC_VERSION_1		(0x1)
+
+#define HV_SYNIC_CONTROL_ENABLE		(1ULL << 0)
+#define HV_SYNIC_SIMP_ENABLE		(1ULL << 0)
+#define HV_SYNIC_SIEFP_ENABLE		(1ULL << 0)
+#define HV_SYNIC_SINT_MASKED		(1ULL << 16)
+#define HV_SYNIC_SINT_AUTO_EOI		(1ULL << 17)
+#define HV_SYNIC_SINT_VECTOR_MASK	(0xFF)
+
+#define HV_SYNIC_STIMER_COUNT		(4)
+
+/* Define synthetic interrupt controller message constants. */
+#define HV_MESSAGE_SIZE			(256)
+#define HV_MESSAGE_PAYLOAD_BYTE_COUNT	(240)
+#define HV_MESSAGE_PAYLOAD_QWORD_COUNT	(30)
+
+/* Define hypervisor message types. */
+enum hv_message_type {
+	HVMSG_NONE			= 0x00000000,
+
+	/* Memory access messages. */
+	HVMSG_UNMAPPED_GPA		= 0x80000000,
+	HVMSG_GPA_INTERCEPT		= 0x80000001,
+
+	/* Timer notification messages. */
+	HVMSG_TIMER_EXPIRED			= 0x80000010,
+
+	/* Error messages. */
+	HVMSG_INVALID_VP_REGISTER_VALUE	= 0x80000020,
+	HVMSG_UNRECOVERABLE_EXCEPTION	= 0x80000021,
+	HVMSG_UNSUPPORTED_FEATURE		= 0x80000022,
+
+	/* Trace buffer complete messages. */
+	HVMSG_EVENTLOG_BUFFERCOMPLETE	= 0x80000040,
+
+	/* Platform-specific processor intercept messages. */
+	HVMSG_X64_IOPORT_INTERCEPT		= 0x80010000,
+	HVMSG_X64_MSR_INTERCEPT		= 0x80010001,
+	HVMSG_X64_CPUID_INTERCEPT		= 0x80010002,
+	HVMSG_X64_EXCEPTION_INTERCEPT	= 0x80010003,
+	HVMSG_X64_APIC_EOI			= 0x80010004,
+	HVMSG_X64_LEGACY_FP_ERROR		= 0x80010005
+};
+
+/* Define synthetic interrupt controller message flags. */
+union hv_message_flags {
+	__u8 asu8;
+	struct {
+		__u8 msg_pending:1;
+		__u8 reserved:7;
+	};
+};
+
+/* Define port identifier type. */
+union hv_port_id {
+	__u32 asu32;
+	struct {
+		__u32 id:24;
+		__u32 reserved:8;
+	} u;
+};
+
+/* Define synthetic interrupt controller message header. */
+struct hv_message_header {
+	__u32 message_type;
+	__u8 payload_size;
+	union hv_message_flags message_flags;
+	__u8 reserved[2];
+	union {
+		__u64 sender;
+		union hv_port_id port;
+	};
+};
+
+/* Define synthetic interrupt controller message format. */
+struct hv_message {
+	struct hv_message_header header;
+	union {
+		__u64 payload[HV_MESSAGE_PAYLOAD_QWORD_COUNT];
+	} u;
+};
+
+/* Define the synthetic interrupt message page layout. */
+struct hv_message_page {
+	struct hv_message sint_message[HV_SYNIC_SINT_COUNT];
+};
+
+/* Define timer message payload structure. */
+struct hv_timer_message_payload {
+	__u32 timer_index;
+	__u32 reserved;
+	__u64 expiration_time;	/* When the timer expired */
+	__u64 delivery_time;	/* When the message was delivered */
+};
+
+#endif
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/asm/mshyperv.h linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/asm/mshyperv.h
--- linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/asm/mshyperv.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/asm/mshyperv.h	2018-12-12 00:51:40.652785647 +0000
@@ -0,0 +1,379 @@
+#ifndef _ASM_X86_MSHYPER_H
+#define _ASM_X86_MSHYPER_H
+
+#include <linux/types.h>
+#include <linux/atomic.h>
+#include <linux/nmi.h>
+#include <asm/io.h>
+#include "hyperv.h"
+#include <asm/nospec-branch.h>
+
+/*
+ * The below CPUID leaves are present if VersionAndFeatures.HypervisorPresent
+ * is set by CPUID(HVCPUID_VERSION_FEATURES).
+ */
+enum hv_cpuid_function {
+	HVCPUID_VERSION_FEATURES		= 0x00000001,
+	HVCPUID_VENDOR_MAXFUNCTION		= 0x40000000,
+	HVCPUID_INTERFACE			= 0x40000001,
+
+	/*
+	 * The remaining functions depend on the value of
+	 * HVCPUID_INTERFACE
+	 */
+	HVCPUID_VERSION				= 0x40000002,
+	HVCPUID_FEATURES			= 0x40000003,
+	HVCPUID_ENLIGHTENMENT_INFO		= 0x40000004,
+	HVCPUID_IMPLEMENTATION_LIMITS		= 0x40000005,
+};
+
+struct ms_hyperv_info {
+	u32 features;
+	u32 misc_features;
+	u32 hints;
+};
+
+extern struct ms_hyperv_info ms_hyperv;
+
+/*
+ * Declare the MSR used to setup pages used to communicate with the hypervisor.
+ */
+union hv_x64_msr_hypercall_contents {
+	u64 as_uint64;
+	struct {
+		u64 enable:1;
+		u64 reserved:11;
+		u64 guest_physical_address:52;
+	};
+};
+
+/*
+ * TSC page layout.
+ */
+
+struct ms_hyperv_tsc_page {
+	volatile u32 tsc_sequence;
+	u32 reserved1;
+	volatile u64 tsc_scale;
+	volatile s64 tsc_offset;
+	u64 reserved2[509];
+};
+
+/*
+ * The guest OS needs to register the guest ID with the hypervisor.
+ * The guest ID is a 64 bit entity and the structure of this ID is
+ * specified in the Hyper-V specification:
+ *
+ * msdn.microsoft.com/en-us/library/windows/hardware/ff542653%28v=vs.85%29.aspx
+ *
+ * While the current guideline does not specify how Linux guest ID(s)
+ * need to be generated, our plan is to publish the guidelines for
+ * Linux and other guest operating systems that currently are hosted
+ * on Hyper-V. The implementation here conforms to this yet
+ * unpublished guidelines.
+ *
+ *
+ * Bit(s)
+ * 63 - Indicates if the OS is Open Source or not; 1 is Open Source
+ * 62:56 - Os Type; Linux is 0x100
+ * 55:48 - Distro specific identification
+ * 47:16 - Linux kernel version number
+ * 15:0  - Distro specific identification
+ *
+ *
+ */
+
+#define HV_LINUX_VENDOR_ID              0x8100
+
+/*
+ * Generate the guest ID based on the guideline described above.
+ */
+
+static inline  __u64 generate_guest_id(__u64 d_info1, __u64 kernel_version,
+				       __u64 d_info2)
+{
+	__u64 guest_id = 0;
+
+	guest_id = (((__u64)HV_LINUX_VENDOR_ID) << 48);
+	guest_id |= (d_info1 << 48);
+	guest_id |= (kernel_version << 16);
+	guest_id |= d_info2;
+
+	return guest_id;
+}
+
+
+/* Free the message slot and signal end-of-message if required */
+static inline void vmbus_signal_eom(struct hv_message *msg, u32 old_msg_type)
+{
+	/*
+	 * On crash we're reading some other CPU's message page and we need
+	 * to be careful: this other CPU may already had cleared the header
+	 * and the host may already had delivered some other message there.
+	 * In case we blindly write msg->header.message_type we're going
+	 * to lose it. We can still lose a message of the same type but
+	 * we count on the fact that there can only be one
+	 * CHANNELMSG_UNLOAD_RESPONSE and we don't care about other messages
+	 * on crash.
+	 */
+	if (cmpxchg(&msg->header.message_type, old_msg_type,
+		    HVMSG_NONE) != old_msg_type)
+		return;
+
+	/*
+	 * Make sure the write to MessageType (ie set to
+	 * HVMSG_NONE) happens before we read the
+	 * MessagePending and EOMing. Otherwise, the EOMing
+	 * will not deliver any more messages since there is
+	 * no empty slot
+	 */
+	mb();
+
+	if (msg->header.message_flags.msg_pending) {
+		/*
+		 * This will cause message queue rescan to
+		 * possibly deliver another msg from the
+		 * hypervisor
+		 */
+		wrmsrl(HV_X64_MSR_EOM, 0);
+	}
+}
+
+#define hv_init_timer(timer, tick) wrmsrl(timer, tick)
+#define hv_init_timer_config(config, val) wrmsrl(config, val)
+
+#define hv_get_simp(val) rdmsrl(HV_X64_MSR_SIMP, val)
+#define hv_set_simp(val) wrmsrl(HV_X64_MSR_SIMP, val)
+
+#define hv_get_siefp(val) rdmsrl(HV_X64_MSR_SIEFP, val)
+#define hv_set_siefp(val) wrmsrl(HV_X64_MSR_SIEFP, val)
+
+#define hv_get_synic_state(val) rdmsrl(HV_X64_MSR_SCONTROL, val)
+#define hv_set_synic_state(val) wrmsrl(HV_X64_MSR_SCONTROL, val)
+
+#define hv_get_vp_index(index) rdmsrl(HV_X64_MSR_VP_INDEX, index)
+
+#define hv_get_synint_state(int_num, val) rdmsrl(int_num, val)
+#define hv_set_synint_state(int_num, val) wrmsrl(int_num, val)
+
+void hyperv_callback_vector(void);
+#ifdef CONFIG_TRACING
+#define trace_hyperv_callback_vector hyperv_callback_vector
+#endif
+void hyperv_vector_handler(struct pt_regs *regs);
+void hv_setup_vmbus_irq(void (*handler)(void));
+void hv_remove_vmbus_irq(void);
+
+void hv_setup_kexec_handler(void (*handler)(void));
+void hv_remove_kexec_handler(void);
+void hv_setup_crash_handler(void (*handler)(struct pt_regs *regs));
+void hv_remove_crash_handler(void);
+
+#if IS_ENABLED(CONFIG_HYPERV)
+extern struct clocksource *hyperv_cs;
+extern void *hv_hypercall_pg;
+
+static inline u64 hv_do_hypercall(u64 control, void *input, void *output)
+{
+	u64 input_address = input ? virt_to_phys(input) : 0;
+	u64 output_address = output ? virt_to_phys(output) : 0;
+	u64 hv_status;
+	register void *__sp asm(_ASM_SP);
+
+#ifdef CONFIG_X86_64
+	if (!hv_hypercall_pg)
+		return U64_MAX;
+
+	__asm__ __volatile__("mov %4, %%r8\n"
+			     CALL_NOSPEC
+			     : "=a" (hv_status), "+r" (__sp),
+			       "+c" (control), "+d" (input_address)
+			     :  "r" (output_address),
+				THUNK_TARGET(hv_hypercall_pg)
+			     : "cc", "memory", "r8", "r9", "r10", "r11");
+#else
+	u32 input_address_hi = upper_32_bits(input_address);
+	u32 input_address_lo = lower_32_bits(input_address);
+	u32 output_address_hi = upper_32_bits(output_address);
+	u32 output_address_lo = lower_32_bits(output_address);
+
+	if (!hv_hypercall_pg)
+		return U64_MAX;
+
+	__asm__ __volatile__(CALL_NOSPEC
+			     : "=A" (hv_status),
+			       "+c" (input_address_lo), "+r" (__sp)
+			     : "A" (control),
+			       "b" (input_address_hi),
+			       "D"(output_address_hi), "S"(output_address_lo),
+			       THUNK_TARGET(hv_hypercall_pg)
+			     : "cc", "memory");
+#endif /* !x86_64 */
+	return hv_status;
+}
+
+#define HV_HYPERCALL_RESULT_MASK	GENMASK_ULL(15, 0)
+#define HV_HYPERCALL_FAST_BIT		BIT(16)
+#define HV_HYPERCALL_VARHEAD_OFFSET	17
+#define HV_HYPERCALL_REP_COMP_OFFSET	32
+#define HV_HYPERCALL_REP_COMP_MASK	GENMASK_ULL(43, 32)
+#define HV_HYPERCALL_REP_START_OFFSET	48
+#define HV_HYPERCALL_REP_START_MASK	GENMASK_ULL(59, 48)
+
+/* Fast hypercall with 8 bytes of input and no output */
+static inline u64 hv_do_fast_hypercall8(u16 code, u64 input1)
+{
+	u64 hv_status, control = (u64)code | HV_HYPERCALL_FAST_BIT;
+	register void *__sp asm(_ASM_SP);
+
+#ifdef CONFIG_X86_64
+	{
+		__asm__ __volatile__(CALL_NOSPEC
+				     : "=a" (hv_status), "+r" (__sp),
+				       "+c" (control), "+d" (input1)
+				     : THUNK_TARGET(hv_hypercall_pg)
+				     : "cc", "r8", "r9", "r10", "r11");
+	}
+#else
+	{
+		u32 input1_hi = upper_32_bits(input1);
+		u32 input1_lo = lower_32_bits(input1);
+
+		__asm__ __volatile__ (CALL_NOSPEC
+				      : "=A"(hv_status),
+					"+c"(input1_lo),
+					"+r"(__sp)
+				      :	"A" (control),
+					"b" (input1_hi),
+					THUNK_TARGET(hv_hypercall_pg)
+				      : "cc", "edi", "esi");
+	}
+#endif
+		return hv_status;
+}
+
+/*
+ * Rep hypercalls. Callers of this functions are supposed to ensure that
+ * rep_count and varhead_size comply with Hyper-V hypercall definition.
+ */
+static inline u64 hv_do_rep_hypercall(u16 code, u16 rep_count, u16 varhead_size,
+				      void *input, void *output)
+{
+	u64 control = code;
+	u64 status;
+	u16 rep_comp;
+
+	control |= (u64)varhead_size << HV_HYPERCALL_VARHEAD_OFFSET;
+	control |= (u64)rep_count << HV_HYPERCALL_REP_COMP_OFFSET;
+
+	do {
+		status = hv_do_hypercall(control, input, output);
+		if ((status & HV_HYPERCALL_RESULT_MASK) != HV_STATUS_SUCCESS)
+			return status;
+
+		/* Bits 32-43 of status have 'Reps completed' data. */
+		rep_comp = (status & HV_HYPERCALL_REP_COMP_MASK) >>
+			HV_HYPERCALL_REP_COMP_OFFSET;
+
+		control &= ~HV_HYPERCALL_REP_START_MASK;
+		control |= (u64)rep_comp << HV_HYPERCALL_REP_START_OFFSET;
+
+		touch_nmi_watchdog();
+	} while (rep_comp < rep_count);
+
+	return status;
+}
+
+/*
+ * Hypervisor's notion of virtual processor ID is different from
+ * Linux' notion of CPU ID. This information can only be retrieved
+ * in the context of the calling CPU. Setup a map for easy access
+ * to this information.
+ */
+extern u32 *hv_vp_index;
+extern u32 hv_max_vp_index;
+
+/**
+ * hv_cpu_number_to_vp_number() - Map CPU to VP.
+ * @cpu_number: CPU number in Linux terms
+ *
+ * This function returns the mapping between the Linux processor
+ * number and the hypervisor's virtual processor number, useful
+ * in making hypercalls and such that talk about specific
+ * processors.
+ *
+ * Return: Virtual processor number in Hyper-V terms
+ */
+static inline int hv_cpu_number_to_vp_number(int cpu_number)
+{
+	return hv_vp_index[cpu_number];
+}
+
+void hyperv_init(void);
+void hyperv_setup_mmu_ops(void);
+void hyper_alloc_mmu(void);
+void hyperv_report_panic(struct pt_regs *regs, long err);
+bool hv_is_hyperv_initialized(void);
+void hyperv_cleanup(void);
+#else /* CONFIG_HYPERV */
+static inline void hyperv_init(void) {}
+static inline bool hv_is_hyperv_initialized(void) { return false; }
+static inline void hyperv_cleanup(void) {}
+static inline void hyperv_setup_mmu_ops(void) {}
+#endif /* CONFIG_HYPERV */
+
+#ifdef CONFIG_HYPERV_TSCPAGE
+struct ms_hyperv_tsc_page *hv_get_tsc_page(void);
+static inline u64 hv_read_tsc_page(const struct ms_hyperv_tsc_page *tsc_pg)
+{
+	u64 scale, offset, cur_tsc;
+	u32 sequence;
+
+	/*
+	 * The protocol for reading Hyper-V TSC page is specified in Hypervisor
+	 * Top-Level Functional Specification ver. 3.0 and above. To get the
+	 * reference time we must do the following:
+	 * - READ ReferenceTscSequence
+	 *   A special '0' value indicates the time source is unreliable and we
+	 *   need to use something else. The currently published specification
+	 *   versions (up to 4.0b) contain a mistake and wrongly claim '-1'
+	 *   instead of '0' as the special value, see commit c35b82ef0294.
+	 * - ReferenceTime =
+	 *        ((RDTSC() * ReferenceTscScale) >> 64) + ReferenceTscOffset
+	 * - READ ReferenceTscSequence again. In case its value has changed
+	 *   since our first reading we need to discard ReferenceTime and repeat
+	 *   the whole sequence as the hypervisor was updating the page in
+	 *   between.
+	 */
+	do {
+		sequence = READ_ONCE(tsc_pg->tsc_sequence);
+		if (!sequence)
+			return U64_MAX;
+		/*
+		 * Make sure we read sequence before we read other values from
+		 * TSC page.
+		 */
+		smp_rmb();
+
+		scale = READ_ONCE(tsc_pg->tsc_scale);
+		offset = READ_ONCE(tsc_pg->tsc_offset);
+		cur_tsc = rdtsc_ordered();
+
+		/*
+		 * Make sure we read sequence after we read all other values
+		 * from TSC page.
+		 */
+		smp_rmb();
+
+	} while (READ_ONCE(tsc_pg->tsc_sequence) != sequence);
+
+	return mul_u64_u64_shr(cur_tsc, scale, 64) + offset;
+}
+
+#else
+static inline struct ms_hyperv_tsc_page *hv_get_tsc_page(void)
+{
+	return NULL;
+}
+#endif
+#endif
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/asm/trace/hyperv.h linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/asm/trace/hyperv.h
--- linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/asm/trace/hyperv.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/asm/trace/hyperv.h	2018-12-12 00:51:40.636785749 +0000
@@ -0,0 +1,42 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM hyperv
+
+#if !defined(_TRACE_HYPERV_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_HYPERV_H
+
+#include <linux/tracepoint.h>
+
+#if IS_ENABLED(CONFIG_HYPERV)
+
+TRACE_EVENT(hyperv_mmu_flush_tlb_others,
+	    TP_PROTO(const struct cpumask *cpus,
+		     struct mm_struct *mm,
+		     unsigned long start,
+		     unsigned long end),
+	    TP_ARGS(cpus, mm, start, end),
+	    TP_STRUCT__entry(
+		    __field(unsigned int, ncpus)
+		    __field(struct mm_struct *, mm)
+		    __field(unsigned long, addr)
+		    __field(unsigned long, end)
+		    ),
+	    TP_fast_assign(__entry->ncpus = cpumask_weight(cpus);
+			   __entry->mm = mm;
+			   __entry->addr = start;
+			   __entry->end = end;
+		    ),
+	    TP_printk("ncpus %d mm %p addr %lx, end %lx",
+		      __entry->ncpus, __entry->mm,
+		      __entry->addr, __entry->end)
+	);
+
+#endif /* CONFIG_HYPERV */
+
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH asm/trace/
+#undef TRACE_INCLUDE_FILE
+#define TRACE_INCLUDE_FILE hyperv
+#endif /* _TRACE_HYPERV_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/linux/hyperv.h linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/linux/hyperv.h
--- linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/linux/hyperv.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/linux/hyperv.h	2018-12-12 00:51:40.649785666 +0000
@@ -0,0 +1,1519 @@
+/*
+ *
+ * Copyright (c) 2011, Microsoft Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
+ * Place - Suite 330, Boston, MA 02111-1307 USA.
+ *
+ * Authors:
+ *   Haiyang Zhang <haiyangz@microsoft.com>
+ *   Hank Janssen  <hjanssen@microsoft.com>
+ *   K. Y. Srinivasan <kys@microsoft.com>
+ *
+ */
+
+#ifndef _HYPERV_H
+#define _HYPERV_H
+
+#include "uapi/hyperv.h"
+#include "../asm/hyperv.h"
+
+#include <linux/types.h>
+#include <linux/scatterlist.h>
+#include <linux/list.h>
+#include <linux/timer.h>
+#include <linux/completion.h>
+#include <linux/device.h>
+#include <linux/mod_devicetable.h>
+#include <linux/interrupt.h>
+#include <linux/reciprocal_div.h>
+
+#define MAX_PAGE_BUFFER_COUNT				32
+#define MAX_MULTIPAGE_BUFFER_COUNT			32 /* 128K */
+
+#pragma pack(push, 1)
+
+/* Single-page buffer */
+struct hv_page_buffer {
+	u32 len;
+	u32 offset;
+	u64 pfn;
+};
+
+/* Multiple-page buffer */
+struct hv_multipage_buffer {
+	/* Length and Offset determines the # of pfns in the array */
+	u32 len;
+	u32 offset;
+	u64 pfn_array[MAX_MULTIPAGE_BUFFER_COUNT];
+};
+
+/*
+ * Multiple-page buffer array; the pfn array is variable size:
+ * The number of entries in the PFN array is determined by
+ * "len" and "offset".
+ */
+struct hv_mpb_array {
+	/* Length and Offset determines the # of pfns in the array */
+	u32 len;
+	u32 offset;
+	u64 pfn_array[];
+};
+
+/* 0x18 includes the proprietary packet header */
+#define MAX_PAGE_BUFFER_PACKET		(0x18 +			\
+					(sizeof(struct hv_page_buffer) * \
+					 MAX_PAGE_BUFFER_COUNT))
+#define MAX_MULTIPAGE_BUFFER_PACKET	(0x18 +			\
+					 sizeof(struct hv_multipage_buffer))
+
+
+#pragma pack(pop)
+
+struct hv_ring_buffer {
+	/* Offset in bytes from the start of ring data below */
+	u32 write_index;
+
+	/* Offset in bytes from the start of ring data below */
+	u32 read_index;
+
+	u32 interrupt_mask;
+
+	/*
+	 * Win8 uses some of the reserved bits to implement
+	 * interrupt driven flow management. On the send side
+	 * we can request that the receiver interrupt the sender
+	 * when the ring transitions from being full to being able
+	 * to handle a message of size "pending_send_sz".
+	 *
+	 * Add necessary state for this enhancement.
+	 */
+	u32 pending_send_sz;
+
+	u32 reserved1[12];
+
+	union {
+		struct {
+			u32 feat_pending_send_sz:1;
+		};
+		u32 value;
+	} feature_bits;
+
+	/* Pad it to PAGE_SIZE so that data starts on page boundary */
+	u8	reserved2[4028];
+
+	/*
+	 * Ring data starts here + RingDataStartOffset
+	 * !!! DO NOT place any fields below this !!!
+	 */
+	u8 buffer[0];
+} __packed;
+
+struct hv_ring_buffer_info {
+	struct hv_ring_buffer *ring_buffer;
+	u32 ring_size;			/* Include the shared header */
+	struct reciprocal_value ring_size_div10_reciprocal;
+	spinlock_t ring_lock;
+
+	u32 ring_datasize;		/* < ring_size */
+	u32 ring_data_startoffset;
+	u32 priv_write_index;
+	u32 priv_read_index;
+};
+
+
+static inline u32 hv_get_bytes_to_read(const struct hv_ring_buffer_info *rbi)
+{
+	u32 read_loc, write_loc, dsize, read;
+
+	dsize = rbi->ring_datasize;
+	read_loc = rbi->ring_buffer->read_index;
+	write_loc = READ_ONCE(rbi->ring_buffer->write_index);
+
+	read = write_loc >= read_loc ? (write_loc - read_loc) :
+		(dsize - read_loc) + write_loc;
+
+	return read;
+}
+
+static inline u32 hv_get_bytes_to_write(const struct hv_ring_buffer_info *rbi)
+{
+	u32 read_loc, write_loc, dsize, write;
+
+	dsize = rbi->ring_datasize;
+	read_loc = READ_ONCE(rbi->ring_buffer->read_index);
+	write_loc = rbi->ring_buffer->write_index;
+
+	write = write_loc >= read_loc ? dsize - (write_loc - read_loc) :
+		read_loc - write_loc;
+	return write;
+}
+
+static inline u32 hv_get_avail_to_write_percent(
+		const struct hv_ring_buffer_info *rbi)
+{
+	u32 avail_write = hv_get_bytes_to_write(rbi);
+
+	return reciprocal_divide(
+			(avail_write  << 3) + (avail_write << 1),
+			rbi->ring_size_div10_reciprocal);
+}
+
+/*
+ * VMBUS version is 32 bit entity broken up into
+ * two 16 bit quantities: major_number. minor_number.
+ *
+ * 0 . 13 (Windows Server 2008)
+ * 1 . 1  (Windows 7)
+ * 2 . 4  (Windows 8)
+ * 3 . 0  (Windows 8 R2)
+ * 4 . 0  (Windows 10)
+ */
+
+#define VERSION_WS2008  ((0 << 16) | (13))
+#define VERSION_WIN7    ((1 << 16) | (1))
+#define VERSION_WIN8    ((2 << 16) | (4))
+#define VERSION_WIN8_1    ((3 << 16) | (0))
+#define VERSION_WIN10	((4 << 16) | (0))
+
+#define VERSION_INVAL -1
+
+#define VERSION_CURRENT VERSION_WIN10
+
+/* Make maximum size of pipe payload of 16K */
+#define MAX_PIPE_DATA_PAYLOAD		(sizeof(u8) * 16384)
+
+/* Define PipeMode values. */
+#define VMBUS_PIPE_TYPE_BYTE		0x00000000
+#define VMBUS_PIPE_TYPE_MESSAGE		0x00000004
+
+/* The size of the user defined data buffer for non-pipe offers. */
+#define MAX_USER_DEFINED_BYTES		120
+
+/* The size of the user defined data buffer for pipe offers. */
+#define MAX_PIPE_USER_DEFINED_BYTES	116
+
+/*
+ * At the center of the Channel Management library is the Channel Offer. This
+ * struct contains the fundamental information about an offer.
+ */
+struct vmbus_channel_offer {
+	uuid_le if_type;
+	uuid_le if_instance;
+
+	/*
+	 * These two fields are not currently used.
+	 */
+	u64 reserved1;
+	u64 reserved2;
+
+	u16 chn_flags;
+	u16 mmio_megabytes;		/* in bytes * 1024 * 1024 */
+
+	union {
+		/* Non-pipes: The user has MAX_USER_DEFINED_BYTES bytes. */
+		struct {
+			unsigned char user_def[MAX_USER_DEFINED_BYTES];
+		} std;
+
+		/*
+		 * Pipes:
+		 * The following sructure is an integrated pipe protocol, which
+		 * is implemented on top of standard user-defined data. Pipe
+		 * clients have MAX_PIPE_USER_DEFINED_BYTES left for their own
+		 * use.
+		 */
+		struct {
+			u32  pipe_mode;
+			unsigned char user_def[MAX_PIPE_USER_DEFINED_BYTES];
+		} pipe;
+	} u;
+	/*
+	 * The sub_channel_index is defined in win8.
+	 */
+	u16 sub_channel_index;
+	u16 reserved3;
+} __packed;
+
+/* Server Flags */
+#define VMBUS_CHANNEL_ENUMERATE_DEVICE_INTERFACE	1
+#define VMBUS_CHANNEL_SERVER_SUPPORTS_TRANSFER_PAGES	2
+#define VMBUS_CHANNEL_SERVER_SUPPORTS_GPADLS		4
+#define VMBUS_CHANNEL_NAMED_PIPE_MODE			0x10
+#define VMBUS_CHANNEL_LOOPBACK_OFFER			0x100
+#define VMBUS_CHANNEL_PARENT_OFFER			0x200
+#define VMBUS_CHANNEL_REQUEST_MONITORED_NOTIFICATION	0x400
+#define VMBUS_CHANNEL_TLNPI_PROVIDER_OFFER		0x2000
+
+struct vmpacket_descriptor {
+	u16 type;
+	u16 offset8;
+	u16 len8;
+	u16 flags;
+	u64 trans_id;
+} __packed;
+
+struct vmpacket_header {
+	u32 prev_pkt_start_offset;
+	struct vmpacket_descriptor descriptor;
+} __packed;
+
+struct vmtransfer_page_range {
+	u32 byte_count;
+	u32 byte_offset;
+} __packed;
+
+struct vmtransfer_page_packet_header {
+	struct vmpacket_descriptor d;
+	u16 xfer_pageset_id;
+	u8  sender_owns_set;
+	u8 reserved;
+	u32 range_cnt;
+	struct vmtransfer_page_range ranges[1];
+} __packed;
+
+struct vmgpadl_packet_header {
+	struct vmpacket_descriptor d;
+	u32 gpadl;
+	u32 reserved;
+} __packed;
+
+struct vmadd_remove_transfer_page_set {
+	struct vmpacket_descriptor d;
+	u32 gpadl;
+	u16 xfer_pageset_id;
+	u16 reserved;
+} __packed;
+
+/*
+ * This structure defines a range in guest physical space that can be made to
+ * look virtually contiguous.
+ */
+struct gpa_range {
+	u32 byte_count;
+	u32 byte_offset;
+	u64 pfn_array[0];
+};
+
+/*
+ * This is the format for an Establish Gpadl packet, which contains a handle by
+ * which this GPADL will be known and a set of GPA ranges associated with it.
+ * This can be converted to a MDL by the guest OS.  If there are multiple GPA
+ * ranges, then the resulting MDL will be "chained," representing multiple VA
+ * ranges.
+ */
+struct vmestablish_gpadl {
+	struct vmpacket_descriptor d;
+	u32 gpadl;
+	u32 range_cnt;
+	struct gpa_range range[1];
+} __packed;
+
+/*
+ * This is the format for a Teardown Gpadl packet, which indicates that the
+ * GPADL handle in the Establish Gpadl packet will never be referenced again.
+ */
+struct vmteardown_gpadl {
+	struct vmpacket_descriptor d;
+	u32 gpadl;
+	u32 reserved;	/* for alignment to a 8-byte boundary */
+} __packed;
+
+/*
+ * This is the format for a GPA-Direct packet, which contains a set of GPA
+ * ranges, in addition to commands and/or data.
+ */
+struct vmdata_gpa_direct {
+	struct vmpacket_descriptor d;
+	u32 reserved;
+	u32 range_cnt;
+	struct gpa_range range[1];
+} __packed;
+
+/* This is the format for a Additional Data Packet. */
+struct vmadditional_data {
+	struct vmpacket_descriptor d;
+	u64 total_bytes;
+	u32 offset;
+	u32 byte_cnt;
+	unsigned char data[1];
+} __packed;
+
+union vmpacket_largest_possible_header {
+	struct vmpacket_descriptor simple_hdr;
+	struct vmtransfer_page_packet_header xfer_page_hdr;
+	struct vmgpadl_packet_header gpadl_hdr;
+	struct vmadd_remove_transfer_page_set add_rm_xfer_page_hdr;
+	struct vmestablish_gpadl establish_gpadl_hdr;
+	struct vmteardown_gpadl teardown_gpadl_hdr;
+	struct vmdata_gpa_direct data_gpa_direct_hdr;
+};
+
+#define VMPACKET_DATA_START_ADDRESS(__packet)	\
+	(void *)(((unsigned char *)__packet) +	\
+	 ((struct vmpacket_descriptor)__packet)->offset8 * 8)
+
+#define VMPACKET_DATA_LENGTH(__packet)		\
+	((((struct vmpacket_descriptor)__packet)->len8 -	\
+	  ((struct vmpacket_descriptor)__packet)->offset8) * 8)
+
+#define VMPACKET_TRANSFER_MODE(__packet)	\
+	(((struct IMPACT)__packet)->type)
+
+enum vmbus_packet_type {
+	VM_PKT_INVALID				= 0x0,
+	VM_PKT_SYNCH				= 0x1,
+	VM_PKT_ADD_XFER_PAGESET			= 0x2,
+	VM_PKT_RM_XFER_PAGESET			= 0x3,
+	VM_PKT_ESTABLISH_GPADL			= 0x4,
+	VM_PKT_TEARDOWN_GPADL			= 0x5,
+	VM_PKT_DATA_INBAND			= 0x6,
+	VM_PKT_DATA_USING_XFER_PAGES		= 0x7,
+	VM_PKT_DATA_USING_GPADL			= 0x8,
+	VM_PKT_DATA_USING_GPA_DIRECT		= 0x9,
+	VM_PKT_CANCEL_REQUEST			= 0xa,
+	VM_PKT_COMP				= 0xb,
+	VM_PKT_DATA_USING_ADDITIONAL_PKT	= 0xc,
+	VM_PKT_ADDITIONAL_DATA			= 0xd
+};
+
+#define VMBUS_DATA_PACKET_FLAG_COMPLETION_REQUESTED	1
+
+
+/* Version 1 messages */
+enum vmbus_channel_message_type {
+	CHANNELMSG_INVALID			=  0,
+	CHANNELMSG_OFFERCHANNEL		=  1,
+	CHANNELMSG_RESCIND_CHANNELOFFER	=  2,
+	CHANNELMSG_REQUESTOFFERS		=  3,
+	CHANNELMSG_ALLOFFERS_DELIVERED	=  4,
+	CHANNELMSG_OPENCHANNEL		=  5,
+	CHANNELMSG_OPENCHANNEL_RESULT		=  6,
+	CHANNELMSG_CLOSECHANNEL		=  7,
+	CHANNELMSG_GPADL_HEADER		=  8,
+	CHANNELMSG_GPADL_BODY			=  9,
+	CHANNELMSG_GPADL_CREATED		= 10,
+	CHANNELMSG_GPADL_TEARDOWN		= 11,
+	CHANNELMSG_GPADL_TORNDOWN		= 12,
+	CHANNELMSG_RELID_RELEASED		= 13,
+	CHANNELMSG_INITIATE_CONTACT		= 14,
+	CHANNELMSG_VERSION_RESPONSE		= 15,
+	CHANNELMSG_UNLOAD			= 16,
+	CHANNELMSG_UNLOAD_RESPONSE		= 17,
+	CHANNELMSG_18				= 18,
+	CHANNELMSG_19				= 19,
+	CHANNELMSG_20				= 20,
+	CHANNELMSG_TL_CONNECT_REQUEST		= 21,
+	CHANNELMSG_COUNT
+};
+
+struct vmbus_channel_message_header {
+	enum vmbus_channel_message_type msgtype;
+	u32 padding;
+} __packed;
+
+/* Query VMBus Version parameters */
+struct vmbus_channel_query_vmbus_version {
+	struct vmbus_channel_message_header header;
+	u32 version;
+} __packed;
+
+/* VMBus Version Supported parameters */
+struct vmbus_channel_version_supported {
+	struct vmbus_channel_message_header header;
+	u8 version_supported;
+} __packed;
+
+/* Offer Channel parameters */
+struct vmbus_channel_offer_channel {
+	struct vmbus_channel_message_header header;
+	struct vmbus_channel_offer offer;
+	u32 child_relid;
+	u8 monitorid;
+	/*
+	 * win7 and beyond splits this field into a bit field.
+	 */
+	u8 monitor_allocated:1;
+	u8 reserved:7;
+	/*
+	 * These are new fields added in win7 and later.
+	 * Do not access these fields without checking the
+	 * negotiated protocol.
+	 *
+	 * If "is_dedicated_interrupt" is set, we must not set the
+	 * associated bit in the channel bitmap while sending the
+	 * interrupt to the host.
+	 *
+	 * connection_id is to be used in signaling the host.
+	 */
+	u16 is_dedicated_interrupt:1;
+	u16 reserved1:15;
+	u32 connection_id;
+} __packed;
+
+/* Rescind Offer parameters */
+struct vmbus_channel_rescind_offer {
+	struct vmbus_channel_message_header header;
+	u32 child_relid;
+} __packed;
+
+static inline u32
+hv_ringbuffer_pending_size(const struct hv_ring_buffer_info *rbi)
+{
+	return rbi->ring_buffer->pending_send_sz;
+}
+
+/*
+ * Request Offer -- no parameters, SynIC message contains the partition ID
+ * Set Snoop -- no parameters, SynIC message contains the partition ID
+ * Clear Snoop -- no parameters, SynIC message contains the partition ID
+ * All Offers Delivered -- no parameters, SynIC message contains the partition
+ *		           ID
+ * Flush Client -- no parameters, SynIC message contains the partition ID
+ */
+
+/* Open Channel parameters */
+struct vmbus_channel_open_channel {
+	struct vmbus_channel_message_header header;
+
+	/* Identifies the specific VMBus channel that is being opened. */
+	u32 child_relid;
+
+	/* ID making a particular open request at a channel offer unique. */
+	u32 openid;
+
+	/* GPADL for the channel's ring buffer. */
+	u32 ringbuffer_gpadlhandle;
+
+	/*
+	 * Starting with win8, this field will be used to specify
+	 * the target virtual processor on which to deliver the interrupt for
+	 * the host to guest communication.
+	 * Prior to win8, incoming channel interrupts would only
+	 * be delivered on cpu 0. Setting this value to 0 would
+	 * preserve the earlier behavior.
+	 */
+	u32 target_vp;
+
+	/*
+	 * The upstream ring buffer begins at offset zero in the memory
+	 * described by RingBufferGpadlHandle. The downstream ring buffer
+	 * follows it at this offset (in pages).
+	 */
+	u32 downstream_ringbuffer_pageoffset;
+
+	/* User-specific data to be passed along to the server endpoint. */
+	unsigned char userdata[MAX_USER_DEFINED_BYTES];
+} __packed;
+
+/* Open Channel Result parameters */
+struct vmbus_channel_open_result {
+	struct vmbus_channel_message_header header;
+	u32 child_relid;
+	u32 openid;
+	u32 status;
+} __packed;
+
+/* Close channel parameters; */
+struct vmbus_channel_close_channel {
+	struct vmbus_channel_message_header header;
+	u32 child_relid;
+} __packed;
+
+/* Channel Message GPADL */
+#define GPADL_TYPE_RING_BUFFER		1
+#define GPADL_TYPE_SERVER_SAVE_AREA	2
+#define GPADL_TYPE_TRANSACTION		8
+
+/*
+ * The number of PFNs in a GPADL message is defined by the number of
+ * pages that would be spanned by ByteCount and ByteOffset.  If the
+ * implied number of PFNs won't fit in this packet, there will be a
+ * follow-up packet that contains more.
+ */
+struct vmbus_channel_gpadl_header {
+	struct vmbus_channel_message_header header;
+	u32 child_relid;
+	u32 gpadl;
+	u16 range_buflen;
+	u16 rangecount;
+	struct gpa_range range[0];
+} __packed;
+
+/* This is the followup packet that contains more PFNs. */
+struct vmbus_channel_gpadl_body {
+	struct vmbus_channel_message_header header;
+	u32 msgnumber;
+	u32 gpadl;
+	u64 pfn[0];
+} __packed;
+
+struct vmbus_channel_gpadl_created {
+	struct vmbus_channel_message_header header;
+	u32 child_relid;
+	u32 gpadl;
+	u32 creation_status;
+} __packed;
+
+struct vmbus_channel_gpadl_teardown {
+	struct vmbus_channel_message_header header;
+	u32 child_relid;
+	u32 gpadl;
+} __packed;
+
+struct vmbus_channel_gpadl_torndown {
+	struct vmbus_channel_message_header header;
+	u32 gpadl;
+} __packed;
+
+struct vmbus_channel_relid_released {
+	struct vmbus_channel_message_header header;
+	u32 child_relid;
+} __packed;
+
+struct vmbus_channel_initiate_contact {
+	struct vmbus_channel_message_header header;
+	u32 vmbus_version_requested;
+	u32 target_vcpu; /* The VCPU the host should respond to */
+	u64 interrupt_page;
+	u64 monitor_page1;
+	u64 monitor_page2;
+} __packed;
+
+/* Hyper-V socket: guest's connect()-ing to host */
+struct vmbus_channel_tl_connect_request {
+	struct vmbus_channel_message_header header;
+	uuid_le guest_endpoint_id;
+	uuid_le host_service_id;
+} __packed;
+
+struct vmbus_channel_version_response {
+	struct vmbus_channel_message_header header;
+	u8 version_supported;
+} __packed;
+
+enum vmbus_channel_state {
+	CHANNEL_OFFER_STATE,
+	CHANNEL_OPENING_STATE,
+	CHANNEL_OPEN_STATE,
+	CHANNEL_OPENED_STATE,
+};
+
+/*
+ * Represents each channel msg on the vmbus connection This is a
+ * variable-size data structure depending on the msg type itself
+ */
+struct vmbus_channel_msginfo {
+	/* Bookkeeping stuff */
+	struct list_head msglistentry;
+
+	/* So far, this is only used to handle gpadl body message */
+	struct list_head submsglist;
+
+	/* Synchronize the request/response if needed */
+	struct completion  waitevent;
+	struct vmbus_channel *waiting_channel;
+	union {
+		struct vmbus_channel_version_supported version_supported;
+		struct vmbus_channel_open_result open_result;
+		struct vmbus_channel_gpadl_torndown gpadl_torndown;
+		struct vmbus_channel_gpadl_created gpadl_created;
+		struct vmbus_channel_version_response version_response;
+	} response;
+
+	u32 msgsize;
+	/*
+	 * The channel message that goes out on the "wire".
+	 * It will contain at minimum the VMBUS_CHANNEL_MESSAGE_HEADER header
+	 */
+	unsigned char msg[0];
+};
+
+struct vmbus_close_msg {
+	struct vmbus_channel_msginfo info;
+	struct vmbus_channel_close_channel msg;
+};
+
+/* Define connection identifier type. */
+union hv_connection_id {
+	u32 asu32;
+	struct {
+		u32 id:24;
+		u32 reserved:8;
+	} u;
+};
+
+enum hv_numa_policy {
+	HV_BALANCED = 0,
+	HV_LOCALIZED,
+};
+
+enum vmbus_device_type {
+	HV_IDE = 0,
+	HV_SCSI,
+	HV_FC,
+	HV_NIC,
+	HV_ND,
+	HV_PCIE,
+	HV_FB,
+	HV_KBD,
+	HV_MOUSE,
+	HV_KVP,
+	HV_TS,
+	HV_HB,
+	HV_SHUTDOWN,
+	HV_FCOPY,
+	HV_BACKUP,
+	HV_DM,
+	HV_UNKNOWN,
+};
+
+struct vmbus_device {
+	u16  dev_type;
+	uuid_le guid;
+	bool perf_device;
+};
+
+struct vmbus_channel {
+	struct list_head listentry;
+
+	struct hv_device *device_obj;
+
+	enum vmbus_channel_state state;
+
+	struct vmbus_channel_offer_channel offermsg;
+	/*
+	 * These are based on the OfferMsg.MonitorId.
+	 * Save it here for easy access.
+	 */
+	u8 monitor_grp;
+	u8 monitor_bit;
+
+	bool rescind; /* got rescind msg */
+	struct completion rescind_event;
+
+	u32 ringbuffer_gpadlhandle;
+
+	/* Allocated memory for ring buffer */
+	void *ringbuffer_pages;
+	u32 ringbuffer_pagecount;
+	struct hv_ring_buffer_info outbound;	/* send to parent */
+	struct hv_ring_buffer_info inbound;	/* receive from parent */
+
+	struct vmbus_close_msg close_msg;
+
+	/* Statistics */
+	u64	interrupts;	/* Host to Guest interrupts */
+	u64	sig_events;	/* Guest to Host events */
+
+	/* Channel callback's invoked in softirq context */
+	struct tasklet_struct callback_event;
+	void (*onchannel_callback)(void *context);
+	void *channel_callback_context;
+
+	/*
+	 * A channel can be marked for one of three modes of reading:
+	 *   BATCHED - callback called from taslket and should read
+	 *            channel until empty. Interrupts from the host
+	 *            are masked while read is in process (default).
+	 *   DIRECT - callback called from tasklet (softirq).
+	 *   ISR - callback called in interrupt context and must
+	 *         invoke its own deferred processing.
+	 *         Host interrupts are disabled and must be re-enabled
+	 *         when ring is empty.
+	 */
+	enum hv_callback_mode {
+		HV_CALL_BATCHED,
+		HV_CALL_DIRECT,
+		HV_CALL_ISR
+	} callback_mode;
+
+	bool is_dedicated_interrupt;
+	u64 sig_event;
+
+	/*
+	 * Starting with win8, this field will be used to specify
+	 * the target virtual processor on which to deliver the interrupt for
+	 * the host to guest communication.
+	 * Prior to win8, incoming channel interrupts would only
+	 * be delivered on cpu 0. Setting this value to 0 would
+	 * preserve the earlier behavior.
+	 */
+	u32 target_vp;
+	/* The corresponding CPUID in the guest */
+	u32 target_cpu;
+	/*
+	 * State to manage the CPU affiliation of channels.
+	 */
+	struct cpumask alloced_cpus_in_node;
+	int numa_node;
+	/*
+	 * Support for sub-channels. For high performance devices,
+	 * it will be useful to have multiple sub-channels to support
+	 * a scalable communication infrastructure with the host.
+	 * The support for sub-channels is implemented as an extention
+	 * to the current infrastructure.
+	 * The initial offer is considered the primary channel and this
+	 * offer message will indicate if the host supports sub-channels.
+	 * The guest is free to ask for sub-channels to be offerred and can
+	 * open these sub-channels as a normal "primary" channel. However,
+	 * all sub-channels will have the same type and instance guids as the
+	 * primary channel. Requests sent on a given channel will result in a
+	 * response on the same channel.
+	 */
+
+	/*
+	 * Sub-channel creation callback. This callback will be called in
+	 * process context when a sub-channel offer is received from the host.
+	 * The guest can open the sub-channel in the context of this callback.
+	 */
+	void (*sc_creation_callback)(struct vmbus_channel *new_sc);
+
+	/*
+	 * Channel rescind callback. Some channels (the hvsock ones), need to
+	 * register a callback which is invoked in vmbus_onoffer_rescind().
+	 */
+	void (*chn_rescind_callback)(struct vmbus_channel *channel);
+
+	/*
+	 * The spinlock to protect the structure. It is being used to protect
+	 * test-and-set access to various attributes of the structure as well
+	 * as all sc_list operations.
+	 */
+	spinlock_t lock;
+	/*
+	 * All Sub-channels of a primary channel are linked here.
+	 */
+	struct list_head sc_list;
+	/*
+	 * Current number of sub-channels.
+	 */
+	int num_sc;
+	/*
+	 * Number of a sub-channel (position within sc_list) which is supposed
+	 * to be used as the next outgoing channel.
+	 */
+	int next_oc;
+	/*
+	 * The primary channel this sub-channel belongs to.
+	 * This will be NULL for the primary channel.
+	 */
+	struct vmbus_channel *primary_channel;
+	/*
+	 * Support per-channel state for use by vmbus drivers.
+	 */
+	void *per_channel_state;
+	/*
+	 * To support per-cpu lookup mapping of relid to channel,
+	 * link up channels based on their CPU affinity.
+	 */
+	struct list_head percpu_list;
+
+	/*
+	 * Defer freeing channel until after all cpu's have
+	 * gone through grace period.
+	 */
+	struct rcu_head rcu;
+
+	/*
+	 * For sysfs per-channel properties.
+	 */
+	struct kobject			kobj;
+
+	/*
+	 * For performance critical channels (storage, networking
+	 * etc,), Hyper-V has a mechanism to enhance the throughput
+	 * at the expense of latency:
+	 * When the host is to be signaled, we just set a bit in a shared page
+	 * and this bit will be inspected by the hypervisor within a certain
+	 * window and if the bit is set, the host will be signaled. The window
+	 * of time is the monitor latency - currently around 100 usecs. This
+	 * mechanism improves throughput by:
+	 *
+	 * A) Making the host more efficient - each time it wakes up,
+	 *    potentially it will process morev number of packets. The
+	 *    monitor latency allows a batch to build up.
+	 * B) By deferring the hypercall to signal, we will also minimize
+	 *    the interrupts.
+	 *
+	 * Clearly, these optimizations improve throughput at the expense of
+	 * latency. Furthermore, since the channel is shared for both
+	 * control and data messages, control messages currently suffer
+	 * unnecessary latency adversley impacting performance and boot
+	 * time. To fix this issue, permit tagging the channel as being
+	 * in "low latency" mode. In this mode, we will bypass the monitor
+	 * mechanism.
+	 */
+	bool low_latency;
+
+	/*
+	 * NUMA distribution policy:
+	 * We support two policies:
+	 * 1) Balanced: Here all performance critical channels are
+	 *    distributed evenly amongst all the NUMA nodes.
+	 *    This policy will be the default policy.
+	 * 2) Localized: All channels of a given instance of a
+	 *    performance critical service will be assigned CPUs
+	 *    within a selected NUMA node.
+	 */
+	enum hv_numa_policy affinity_policy;
+
+	bool probe_done;
+
+};
+
+static inline bool is_hvsock_channel(const struct vmbus_channel *c)
+{
+	return !!(c->offermsg.offer.chn_flags &
+		  VMBUS_CHANNEL_TLNPI_PROVIDER_OFFER);
+}
+
+static inline void set_channel_affinity_state(struct vmbus_channel *c,
+					      enum hv_numa_policy policy)
+{
+	c->affinity_policy = policy;
+}
+
+static inline void set_channel_read_mode(struct vmbus_channel *c,
+					enum hv_callback_mode mode)
+{
+	c->callback_mode = mode;
+}
+
+static inline void set_per_channel_state(struct vmbus_channel *c, void *s)
+{
+	c->per_channel_state = s;
+}
+
+static inline void *get_per_channel_state(struct vmbus_channel *c)
+{
+	return c->per_channel_state;
+}
+
+static inline void set_channel_pending_send_size(struct vmbus_channel *c,
+						 u32 size)
+{
+	c->outbound.ring_buffer->pending_send_sz = size;
+}
+
+static inline void set_low_latency_mode(struct vmbus_channel *c)
+{
+	c->low_latency = true;
+}
+
+static inline void clear_low_latency_mode(struct vmbus_channel *c)
+{
+	c->low_latency = false;
+}
+
+void vmbus_onmessage(void *context);
+
+int vmbus_request_offers(void);
+
+/*
+ * APIs for managing sub-channels.
+ */
+
+void vmbus_set_sc_create_callback(struct vmbus_channel *primary_channel,
+			void (*sc_cr_cb)(struct vmbus_channel *new_sc));
+
+void vmbus_set_chn_rescind_callback(struct vmbus_channel *channel,
+		void (*chn_rescind_cb)(struct vmbus_channel *));
+
+/*
+ * Retrieve the (sub) channel on which to send an outgoing request.
+ * When a primary channel has multiple sub-channels, we choose a
+ * channel whose VCPU binding is closest to the VCPU on which
+ * this call is being made.
+ */
+struct vmbus_channel *vmbus_get_outgoing_channel(struct vmbus_channel *primary);
+
+/*
+ * Check if sub-channels have already been offerred. This API will be useful
+ * when the driver is unloaded after establishing sub-channels. In this case,
+ * when the driver is re-loaded, the driver would have to check if the
+ * subchannels have already been established before attempting to request
+ * the creation of sub-channels.
+ * This function returns TRUE to indicate that subchannels have already been
+ * created.
+ * This function should be invoked after setting the callback function for
+ * sub-channel creation.
+ */
+bool vmbus_are_subchannels_present(struct vmbus_channel *primary);
+
+/* The format must be the same as struct vmdata_gpa_direct */
+struct vmbus_channel_packet_page_buffer {
+	u16 type;
+	u16 dataoffset8;
+	u16 length8;
+	u16 flags;
+	u64 transactionid;
+	u32 reserved;
+	u32 rangecount;
+	struct hv_page_buffer range[MAX_PAGE_BUFFER_COUNT];
+} __packed;
+
+/* The format must be the same as struct vmdata_gpa_direct */
+struct vmbus_channel_packet_multipage_buffer {
+	u16 type;
+	u16 dataoffset8;
+	u16 length8;
+	u16 flags;
+	u64 transactionid;
+	u32 reserved;
+	u32 rangecount;		/* Always 1 in this case */
+	struct hv_multipage_buffer range;
+} __packed;
+
+/* The format must be the same as struct vmdata_gpa_direct */
+struct vmbus_packet_mpb_array {
+	u16 type;
+	u16 dataoffset8;
+	u16 length8;
+	u16 flags;
+	u64 transactionid;
+	u32 reserved;
+	u32 rangecount;         /* Always 1 in this case */
+	struct hv_mpb_array range;
+} __packed;
+
+
+extern int vmbus_open(struct vmbus_channel *channel,
+			    u32 send_ringbuffersize,
+			    u32 recv_ringbuffersize,
+			    void *userdata,
+			    u32 userdatalen,
+			    void (*onchannel_callback)(void *context),
+			    void *context);
+
+extern void vmbus_close(struct vmbus_channel *channel);
+
+extern int vmbus_sendpacket(struct vmbus_channel *channel,
+				  void *buffer,
+				  u32 bufferLen,
+				  u64 requestid,
+				  enum vmbus_packet_type type,
+				  u32 flags);
+
+extern int vmbus_sendpacket_pagebuffer(struct vmbus_channel *channel,
+					    struct hv_page_buffer pagebuffers[],
+					    u32 pagecount,
+					    void *buffer,
+					    u32 bufferlen,
+					    u64 requestid);
+
+extern int vmbus_sendpacket_mpb_desc(struct vmbus_channel *channel,
+				     struct vmbus_packet_mpb_array *mpb,
+				     u32 desc_size,
+				     void *buffer,
+				     u32 bufferlen,
+				     u64 requestid);
+
+extern int vmbus_establish_gpadl(struct vmbus_channel *channel,
+				      void *kbuffer,
+				      u32 size,
+				      u32 *gpadl_handle);
+
+extern int vmbus_teardown_gpadl(struct vmbus_channel *channel,
+				     u32 gpadl_handle);
+
+extern int vmbus_recvpacket(struct vmbus_channel *channel,
+				  void *buffer,
+				  u32 bufferlen,
+				  u32 *buffer_actual_len,
+				  u64 *requestid);
+
+extern int vmbus_recvpacket_raw(struct vmbus_channel *channel,
+				     void *buffer,
+				     u32 bufferlen,
+				     u32 *buffer_actual_len,
+				     u64 *requestid);
+
+
+extern void vmbus_ontimer(unsigned long data);
+
+/* Base driver object */
+struct hv_driver {
+	const char *name;
+
+	/*
+	 * A hvsock offer, which has a VMBUS_CHANNEL_TLNPI_PROVIDER_OFFER
+	 * channel flag, actually doesn't mean a synthetic device because the
+	 * offer's if_type/if_instance can change for every new hvsock
+	 * connection.
+	 *
+	 * However, to facilitate the notification of new-offer/rescind-offer
+	 * from vmbus driver to hvsock driver, we can handle hvsock offer as
+	 * a special vmbus device, and hence we need the below flag to
+	 * indicate if the driver is the hvsock driver or not: we need to
+	 * specially treat the hvosck offer & driver in vmbus_match().
+	 */
+	bool hvsock;
+
+	/* the device type supported by this driver */
+	uuid_le dev_type;
+	const struct hv_vmbus_device_id *id_table;
+
+	struct device_driver driver;
+
+	/* dynamic device GUID's */
+	struct  {
+		spinlock_t lock;
+		struct list_head list;
+	} dynids;
+
+	int (*probe)(struct hv_device *, const struct hv_vmbus_device_id *);
+	int (*remove)(struct hv_device *);
+	void (*shutdown)(struct hv_device *);
+
+};
+
+/* Base device object */
+struct hv_device {
+	/* the device type id of this device */
+	uuid_le dev_type;
+
+	/* the device instance id of this device */
+	uuid_le dev_instance;
+	u16 vendor_id;
+	u16 device_id;
+
+	struct device device;
+
+	struct vmbus_channel *channel;
+	struct kset	     *channels_kset;
+};
+
+
+static inline struct hv_device *device_to_hv_device(struct device *d)
+{
+	return container_of(d, struct hv_device, device);
+}
+
+static inline struct hv_driver *drv_to_hv_drv(struct device_driver *d)
+{
+	return container_of(d, struct hv_driver, driver);
+}
+
+static inline void hv_set_drvdata(struct hv_device *dev, void *data)
+{
+	dev_set_drvdata(&dev->device, data);
+}
+
+static inline void *hv_get_drvdata(struct hv_device *dev)
+{
+	return dev_get_drvdata(&dev->device);
+}
+
+struct hv_ring_buffer_debug_info {
+	u32 current_interrupt_mask;
+	u32 current_read_index;
+	u32 current_write_index;
+	u32 bytes_avail_toread;
+	u32 bytes_avail_towrite;
+};
+
+void hv_ringbuffer_get_debuginfo(const struct hv_ring_buffer_info *ring_info,
+			    struct hv_ring_buffer_debug_info *debug_info);
+
+/* Vmbus interface */
+#define vmbus_driver_register(driver)	\
+	__vmbus_driver_register(driver, THIS_MODULE, KBUILD_MODNAME)
+int __must_check __vmbus_driver_register(struct hv_driver *hv_driver,
+					 struct module *owner,
+					 const char *mod_name);
+void vmbus_driver_unregister(struct hv_driver *hv_driver);
+
+void vmbus_hvsock_device_unregister(struct vmbus_channel *channel);
+
+int vmbus_allocate_mmio(struct resource **new, struct hv_device *device_obj,
+			resource_size_t min, resource_size_t max,
+			resource_size_t size, resource_size_t align,
+			bool fb_overlap_ok);
+void vmbus_free_mmio(resource_size_t start, resource_size_t size);
+
+/**
+ * VMBUS_DEVICE - macro used to describe a specific hyperv vmbus device
+ *
+ * This macro is used to create a struct hv_vmbus_device_id that matches a
+ * specific device.
+ */
+#define VMBUS_DEVICE(g0, g1, g2, g3, g4, g5, g6, g7,	\
+		     g8, g9, ga, gb, gc, gd, ge, gf)	\
+	.guid = { g0, g1, g2, g3, g4, g5, g6, g7,	\
+		  g8, g9, ga, gb, gc, gd, ge, gf },
+
+
+
+/*
+ * GUID definitions of various offer types - services offered to the guest.
+ */
+
+/*
+ * Network GUID
+ * {f8615163-df3e-46c5-913f-f2d2f965ed0e}
+ */
+#define HV_NIC_GUID \
+	.guid = UUID_LE(0xf8615163, 0xdf3e, 0x46c5, 0x91, 0x3f, \
+			0xf2, 0xd2, 0xf9, 0x65, 0xed, 0x0e)
+
+/*
+ * IDE GUID
+ * {32412632-86cb-44a2-9b5c-50d1417354f5}
+ */
+#define HV_IDE_GUID \
+	.guid = UUID_LE(0x32412632, 0x86cb, 0x44a2, 0x9b, 0x5c, \
+			0x50, 0xd1, 0x41, 0x73, 0x54, 0xf5)
+
+/*
+ * SCSI GUID
+ * {ba6163d9-04a1-4d29-b605-72e2ffb1dc7f}
+ */
+#define HV_SCSI_GUID \
+	.guid = UUID_LE(0xba6163d9, 0x04a1, 0x4d29, 0xb6, 0x05, \
+			0x72, 0xe2, 0xff, 0xb1, 0xdc, 0x7f)
+
+/*
+ * Shutdown GUID
+ * {0e0b6031-5213-4934-818b-38d90ced39db}
+ */
+#define HV_SHUTDOWN_GUID \
+	.guid = UUID_LE(0x0e0b6031, 0x5213, 0x4934, 0x81, 0x8b, \
+			0x38, 0xd9, 0x0c, 0xed, 0x39, 0xdb)
+
+/*
+ * Time Synch GUID
+ * {9527E630-D0AE-497b-ADCE-E80AB0175CAF}
+ */
+#define HV_TS_GUID \
+	.guid = UUID_LE(0x9527e630, 0xd0ae, 0x497b, 0xad, 0xce, \
+			0xe8, 0x0a, 0xb0, 0x17, 0x5c, 0xaf)
+
+/*
+ * Heartbeat GUID
+ * {57164f39-9115-4e78-ab55-382f3bd5422d}
+ */
+#define HV_HEART_BEAT_GUID \
+	.guid = UUID_LE(0x57164f39, 0x9115, 0x4e78, 0xab, 0x55, \
+			0x38, 0x2f, 0x3b, 0xd5, 0x42, 0x2d)
+
+/*
+ * KVP GUID
+ * {a9a0f4e7-5a45-4d96-b827-8a841e8c03e6}
+ */
+#define HV_KVP_GUID \
+	.guid = UUID_LE(0xa9a0f4e7, 0x5a45, 0x4d96, 0xb8, 0x27, \
+			0x8a, 0x84, 0x1e, 0x8c, 0x03, 0xe6)
+
+/*
+ * Dynamic memory GUID
+ * {525074dc-8985-46e2-8057-a307dc18a502}
+ */
+#define HV_DM_GUID \
+	.guid = UUID_LE(0x525074dc, 0x8985, 0x46e2, 0x80, 0x57, \
+			0xa3, 0x07, 0xdc, 0x18, 0xa5, 0x02)
+
+/*
+ * Mouse GUID
+ * {cfa8b69e-5b4a-4cc0-b98b-8ba1a1f3f95a}
+ */
+#define HV_MOUSE_GUID \
+	.guid = UUID_LE(0xcfa8b69e, 0x5b4a, 0x4cc0, 0xb9, 0x8b, \
+			0x8b, 0xa1, 0xa1, 0xf3, 0xf9, 0x5a)
+
+/*
+ * Keyboard GUID
+ * {f912ad6d-2b17-48ea-bd65-f927a61c7684}
+ */
+#define HV_KBD_GUID \
+	.guid = UUID_LE(0xf912ad6d, 0x2b17, 0x48ea, 0xbd, 0x65, \
+			0xf9, 0x27, 0xa6, 0x1c, 0x76, 0x84)
+
+/*
+ * VSS (Backup/Restore) GUID
+ */
+#define HV_VSS_GUID \
+	.guid = UUID_LE(0x35fa2e29, 0xea23, 0x4236, 0x96, 0xae, \
+			0x3a, 0x6e, 0xba, 0xcb, 0xa4, 0x40)
+/*
+ * Synthetic Video GUID
+ * {DA0A7802-E377-4aac-8E77-0558EB1073F8}
+ */
+#define HV_SYNTHVID_GUID \
+	.guid = UUID_LE(0xda0a7802, 0xe377, 0x4aac, 0x8e, 0x77, \
+			0x05, 0x58, 0xeb, 0x10, 0x73, 0xf8)
+
+/*
+ * Synthetic FC GUID
+ * {2f9bcc4a-0069-4af3-b76b-6fd0be528cda}
+ */
+#define HV_SYNTHFC_GUID \
+	.guid = UUID_LE(0x2f9bcc4a, 0x0069, 0x4af3, 0xb7, 0x6b, \
+			0x6f, 0xd0, 0xbe, 0x52, 0x8c, 0xda)
+
+/*
+ * Guest File Copy Service
+ * {34D14BE3-DEE4-41c8-9AE7-6B174977C192}
+ */
+
+#define HV_FCOPY_GUID \
+	.guid = UUID_LE(0x34d14be3, 0xdee4, 0x41c8, 0x9a, 0xe7, \
+			0x6b, 0x17, 0x49, 0x77, 0xc1, 0x92)
+
+/*
+ * NetworkDirect. This is the guest RDMA service.
+ * {8c2eaf3d-32a7-4b09-ab99-bd1f1c86b501}
+ */
+#define HV_ND_GUID \
+	.guid = UUID_LE(0x8c2eaf3d, 0x32a7, 0x4b09, 0xab, 0x99, \
+			0xbd, 0x1f, 0x1c, 0x86, 0xb5, 0x01)
+
+/*
+ * PCI Express Pass Through
+ * {44C4F61D-4444-4400-9D52-802E27EDE19F}
+ */
+
+#define HV_PCIE_GUID \
+	.guid = UUID_LE(0x44c4f61d, 0x4444, 0x4400, 0x9d, 0x52, \
+			0x80, 0x2e, 0x27, 0xed, 0xe1, 0x9f)
+
+/*
+ * Linux doesn't support the 3 devices: the first two are for
+ * Automatic Virtual Machine Activation, and the third is for
+ * Remote Desktop Virtualization.
+ * {f8e65716-3cb3-4a06-9a60-1889c5cccab5}
+ * {3375baf4-9e15-4b30-b765-67acb10d607b}
+ * {276aacf4-ac15-426c-98dd-7521ad3f01fe}
+ */
+
+#define HV_AVMA1_GUID \
+	.guid = UUID_LE(0xf8e65716, 0x3cb3, 0x4a06, 0x9a, 0x60, \
+			0x18, 0x89, 0xc5, 0xcc, 0xca, 0xb5)
+
+#define HV_AVMA2_GUID \
+	.guid = UUID_LE(0x3375baf4, 0x9e15, 0x4b30, 0xb7, 0x65, \
+			0x67, 0xac, 0xb1, 0x0d, 0x60, 0x7b)
+
+#define HV_RDV_GUID \
+	.guid = UUID_LE(0x276aacf4, 0xac15, 0x426c, 0x98, 0xdd, \
+			0x75, 0x21, 0xad, 0x3f, 0x01, 0xfe)
+
+/*
+ * Common header for Hyper-V ICs
+ */
+
+#define ICMSGTYPE_NEGOTIATE		0
+#define ICMSGTYPE_HEARTBEAT		1
+#define ICMSGTYPE_KVPEXCHANGE		2
+#define ICMSGTYPE_SHUTDOWN		3
+#define ICMSGTYPE_TIMESYNC		4
+#define ICMSGTYPE_VSS			5
+
+#define ICMSGHDRFLAG_TRANSACTION	1
+#define ICMSGHDRFLAG_REQUEST		2
+#define ICMSGHDRFLAG_RESPONSE		4
+
+
+/*
+ * While we want to handle util services as regular devices,
+ * there is only one instance of each of these services; so
+ * we statically allocate the service specific state.
+ */
+
+struct hv_util_service {
+	u8 *recv_buffer;
+	void *channel;
+	void (*util_cb)(void *);
+	int (*util_init)(struct hv_util_service *);
+	void (*util_deinit)(void);
+};
+
+struct vmbuspipe_hdr {
+	u32 flags;
+	u32 msgsize;
+} __packed;
+
+struct ic_version {
+	u16 major;
+	u16 minor;
+} __packed;
+
+struct icmsg_hdr {
+	struct ic_version icverframe;
+	u16 icmsgtype;
+	struct ic_version icvermsg;
+	u16 icmsgsize;
+	u32 status;
+	u8 ictransaction_id;
+	u8 icflags;
+	u8 reserved[2];
+} __packed;
+
+struct icmsg_negotiate {
+	u16 icframe_vercnt;
+	u16 icmsg_vercnt;
+	u32 reserved;
+	struct ic_version icversion_data[1]; /* any size array */
+} __packed;
+
+struct shutdown_msg_data {
+	u32 reason_code;
+	u32 timeout_seconds;
+	u32 flags;
+	u8  display_message[2048];
+} __packed;
+
+struct heartbeat_msg_data {
+	u64 seq_num;
+	u32 reserved[8];
+} __packed;
+
+/* Time Sync IC defs */
+#define ICTIMESYNCFLAG_PROBE	0
+#define ICTIMESYNCFLAG_SYNC	1
+#define ICTIMESYNCFLAG_SAMPLE	2
+
+#ifdef __x86_64__
+#define WLTIMEDELTA	116444736000000000L	/* in 100ns unit */
+#else
+#define WLTIMEDELTA	116444736000000000LL
+#endif
+
+struct ictimesync_data {
+	u64 parenttime;
+	u64 childtime;
+	u64 roundtriptime;
+	u8 flags;
+} __packed;
+
+struct ictimesync_ref_data {
+	u64 parenttime;
+	u64 vmreferencetime;
+	u8 flags;
+	char leapflags;
+	char stratum;
+	u8 reserved[3];
+} __packed;
+
+struct hyperv_service_callback {
+	u8 msg_type;
+	char *log_msg;
+	uuid_le data;
+	struct vmbus_channel *channel;
+	void (*callback)(void *context);
+};
+
+#define MAX_SRV_VER	0x7ffffff
+extern bool vmbus_prep_negotiate_resp(struct icmsg_hdr *icmsghdrp, u8 *buf,
+				const int *fw_version, int fw_vercnt,
+				const int *srv_version, int srv_vercnt,
+				int *nego_fw_version, int *nego_srv_version);
+
+void hv_process_channel_removal(u32 relid);
+
+void vmbus_setevent(struct vmbus_channel *channel);
+/*
+ * Negotiated version with the Host.
+ */
+
+extern __u32 vmbus_proto_version;
+
+int vmbus_send_tl_connect_request(const uuid_le *shv_guest_servie_id,
+				  const uuid_le *shv_host_servie_id);
+void vmbus_set_event(struct vmbus_channel *channel);
+
+/* Get the start of the ring buffer. */
+static inline void *
+hv_get_ring_buffer(const struct hv_ring_buffer_info *ring_info)
+{
+	return ring_info->ring_buffer->buffer;
+}
+
+/*
+ * Mask off host interrupt callback notifications
+ */
+static inline void hv_begin_read(struct hv_ring_buffer_info *rbi)
+{
+	rbi->ring_buffer->interrupt_mask = 1;
+
+	/* make sure mask update is not reordered */
+	mb();
+}
+
+/*
+ * Re-enable host callback and return number of outstanding bytes
+ */
+static inline u32 hv_end_read(struct hv_ring_buffer_info *rbi)
+{
+
+	rbi->ring_buffer->interrupt_mask = 0;
+
+	/* make sure mask update is not reordered */
+	mb();
+
+	/*
+	 * Now check to see if the ring buffer is still empty.
+	 * If it is not, we raced and we need to process new
+	 * incoming messages.
+	 */
+	return hv_get_bytes_to_read(rbi);
+}
+
+/*
+ * An API to support in-place processing of incoming VMBUS packets.
+ */
+
+/* Get data payload associated with descriptor */
+static inline void *hv_pkt_data(const struct vmpacket_descriptor *desc)
+{
+	return (void *)((unsigned long)desc + (desc->offset8 << 3));
+}
+
+/* Get data size associated with descriptor */
+static inline u32 hv_pkt_datalen(const struct vmpacket_descriptor *desc)
+{
+	return (desc->len8 << 3) - (desc->offset8 << 3);
+}
+
+
+struct vmpacket_descriptor *
+hv_pkt_iter_first(struct vmbus_channel *channel);
+
+struct vmpacket_descriptor *
+__hv_pkt_iter_next(struct vmbus_channel *channel,
+		   const struct vmpacket_descriptor *pkt);
+
+void hv_pkt_iter_close(struct vmbus_channel *channel);
+
+/*
+ * Get next packet descriptor from iterator
+ * If at end of list, return NULL and update host.
+ */
+static inline struct vmpacket_descriptor *
+hv_pkt_iter_next(struct vmbus_channel *channel,
+		 const struct vmpacket_descriptor *pkt)
+{
+	struct vmpacket_descriptor *nxt;
+
+	nxt = __hv_pkt_iter_next(channel, pkt);
+	if (!nxt)
+		hv_pkt_iter_close(channel);
+
+	return nxt;
+}
+
+#define foreach_vmbus_pkt(pkt, channel) \
+	for (pkt = hv_pkt_iter_first(channel); pkt; \
+	    pkt = hv_pkt_iter_next(channel, pkt))
+
+#endif /* _HYPERV_H */
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/linux/uapi/hyperv.h linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/linux/uapi/hyperv.h
--- linux-3.10.0-957.el7.orig/arch/x86/hyperv/include/linux/uapi/hyperv.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/hyperv/include/linux/uapi/hyperv.h	2018-12-12 00:51:40.638785736 +0000
@@ -0,0 +1,398 @@
+/*
+ *
+ * Copyright (c) 2011, Microsoft Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
+ * Place - Suite 330, Boston, MA 02111-1307 USA.
+ *
+ * Authors:
+ *   Haiyang Zhang <haiyangz@microsoft.com>
+ *   Hank Janssen  <hjanssen@microsoft.com>
+ *   K. Y. Srinivasan <kys@microsoft.com>
+ *
+ */
+
+#ifndef _UAPI_HYPERV_H
+#define _UAPI_HYPERV_H
+
+#include <linux/uuid.h>
+
+/*
+ * Framework version for util services.
+ */
+#define UTIL_FW_MINOR  0
+
+#define UTIL_WS2K8_FW_MAJOR  1
+#define UTIL_WS2K8_FW_VERSION     (UTIL_WS2K8_FW_MAJOR << 16 | UTIL_FW_MINOR)
+
+#define UTIL_FW_MAJOR  3
+#define UTIL_FW_VERSION     (UTIL_FW_MAJOR << 16 | UTIL_FW_MINOR)
+
+
+/*
+ * Implementation of host controlled snapshot of the guest.
+ */
+
+#define VSS_OP_REGISTER 128
+
+/*
+  Daemon code with full handshake support.
+ */
+#define VSS_OP_REGISTER1 129
+
+enum hv_vss_op {
+	VSS_OP_CREATE = 0,
+	VSS_OP_DELETE,
+	VSS_OP_HOT_BACKUP,
+	VSS_OP_GET_DM_INFO,
+	VSS_OP_BU_COMPLETE,
+	/*
+	 * Following operations are only supported with IC version >= 5.0
+	 */
+	VSS_OP_FREEZE, /* Freeze the file systems in the VM */
+	VSS_OP_THAW, /* Unfreeze the file systems */
+	VSS_OP_AUTO_RECOVER,
+	VSS_OP_COUNT /* Number of operations, must be last */
+};
+
+
+/*
+ * Header for all VSS messages.
+ */
+struct hv_vss_hdr {
+	__u8 operation;
+	__u8 reserved[7];
+} __attribute__((packed));
+
+
+/*
+ * Flag values for the hv_vss_check_feature. Linux supports only
+ * one value.
+ */
+#define VSS_HBU_NO_AUTO_RECOVERY	0x00000005
+
+struct hv_vss_check_feature {
+	__u32 flags;
+} __attribute__((packed));
+
+struct hv_vss_check_dm_info {
+	__u32 flags;
+} __attribute__((packed));
+
+struct hv_vss_msg {
+	union {
+		struct hv_vss_hdr vss_hdr;
+		int error;
+	};
+	union {
+		struct hv_vss_check_feature vss_cf;
+		struct hv_vss_check_dm_info dm_info;
+	};
+} __attribute__((packed));
+
+/*
+ * Implementation of a host to guest copy facility.
+ */
+
+#define FCOPY_VERSION_0 0
+#define FCOPY_VERSION_1 1
+#define FCOPY_CURRENT_VERSION FCOPY_VERSION_1
+#define W_MAX_PATH 260
+
+enum hv_fcopy_op {
+	START_FILE_COPY = 0,
+	WRITE_TO_FILE,
+	COMPLETE_FCOPY,
+	CANCEL_FCOPY,
+};
+
+struct hv_fcopy_hdr {
+	__u32 operation;
+	uuid_le service_id0; /* currently unused */
+	uuid_le service_id1; /* currently unused */
+} __attribute__((packed));
+
+#define OVER_WRITE	0x1
+#define CREATE_PATH	0x2
+
+struct hv_start_fcopy {
+	struct hv_fcopy_hdr hdr;
+	__u16 file_name[W_MAX_PATH];
+	__u16 path_name[W_MAX_PATH];
+	__u32 copy_flags;
+	__u64 file_size;
+} __attribute__((packed));
+
+/*
+ * The file is chunked into fragments.
+ */
+#define DATA_FRAGMENT	(6 * 1024)
+
+struct hv_do_fcopy {
+	struct hv_fcopy_hdr hdr;
+	__u64	offset;
+	__u32	size;
+	__u8	data[DATA_FRAGMENT];
+};
+
+/*
+ * An implementation of HyperV key value pair (KVP) functionality for Linux.
+ *
+ *
+ * Copyright (C) 2010, Novell, Inc.
+ * Author : K. Y. Srinivasan <ksrinivasan@novell.com>
+ *
+ */
+
+/*
+ * Maximum value size - used for both key names and value data, and includes
+ * any applicable NULL terminators.
+ *
+ * Note:  This limit is somewhat arbitrary, but falls easily within what is
+ * supported for all native guests (back to Win 2000) and what is reasonable
+ * for the IC KVP exchange functionality.  Note that Windows Me/98/95 are
+ * limited to 255 character key names.
+ *
+ * MSDN recommends not storing data values larger than 2048 bytes in the
+ * registry.
+ *
+ * Note:  This value is used in defining the KVP exchange message - this value
+ * cannot be modified without affecting the message size and compatibility.
+ */
+
+/*
+ * bytes, including any null terminators
+ */
+#define HV_KVP_EXCHANGE_MAX_VALUE_SIZE          (2048)
+
+
+/*
+ * Maximum key size - the registry limit for the length of an entry name
+ * is 256 characters, including the null terminator
+ */
+
+#define HV_KVP_EXCHANGE_MAX_KEY_SIZE            (512)
+
+/*
+ * In Linux, we implement the KVP functionality in two components:
+ * 1) The kernel component which is packaged as part of the hv_utils driver
+ * is responsible for communicating with the host and responsible for
+ * implementing the host/guest protocol. 2) A user level daemon that is
+ * responsible for data gathering.
+ *
+ * Host/Guest Protocol: The host iterates over an index and expects the guest
+ * to assign a key name to the index and also return the value corresponding to
+ * the key. The host will have atmost one KVP transaction outstanding at any
+ * given point in time. The host side iteration stops when the guest returns
+ * an error. Microsoft has specified the following mapping of key names to
+ * host specified index:
+ *
+ *	Index		Key Name
+ *	0		FullyQualifiedDomainName
+ *	1		IntegrationServicesVersion
+ *	2		NetworkAddressIPv4
+ *	3		NetworkAddressIPv6
+ *	4		OSBuildNumber
+ *	5		OSName
+ *	6		OSMajorVersion
+ *	7		OSMinorVersion
+ *	8		OSVersion
+ *	9		ProcessorArchitecture
+ *
+ * The Windows host expects the Key Name and Key Value to be encoded in utf16.
+ *
+ * Guest Kernel/KVP Daemon Protocol: As noted earlier, we implement all of the
+ * data gathering functionality in a user mode daemon. The user level daemon
+ * is also responsible for binding the key name to the index as well. The
+ * kernel and user-level daemon communicate using a connector channel.
+ *
+ * The user mode component first registers with the
+ * the kernel component. Subsequently, the kernel component requests, data
+ * for the specified keys. In response to this message the user mode component
+ * fills in the value corresponding to the specified key. We overload the
+ * sequence field in the cn_msg header to define our KVP message types.
+ *
+ *
+ * The kernel component simply acts as a conduit for communication between the
+ * Windows host and the user-level daemon. The kernel component passes up the
+ * index received from the Host to the user-level daemon. If the index is
+ * valid (supported), the corresponding key as well as its
+ * value (both are strings) is returned. If the index is invalid
+ * (not supported), a NULL key string is returned.
+ */
+
+
+/*
+ * Registry value types.
+ */
+
+#define REG_SZ 1
+#define REG_U32 4
+#define REG_U64 8
+
+/*
+ * As we look at expanding the KVP functionality to include
+ * IP injection functionality, we need to maintain binary
+ * compatibility with older daemons.
+ *
+ * The KVP opcodes are defined by the host and it was unfortunate
+ * that I chose to treat the registration operation as part of the
+ * KVP operations defined by the host.
+ * Here is the level of compatibility
+ * (between the user level daemon and the kernel KVP driver) that we
+ * will implement:
+ *
+ * An older daemon will always be supported on a newer driver.
+ * A given user level daemon will require a minimal version of the
+ * kernel driver.
+ * If we cannot handle the version differences, we will fail gracefully
+ * (this can happen when we have a user level daemon that is more
+ * advanced than the KVP driver.
+ *
+ * We will use values used in this handshake for determining if we have
+ * workable user level daemon and the kernel driver. We begin by taking the
+ * registration opcode out of the KVP opcode namespace. We will however,
+ * maintain compatibility with the existing user-level daemon code.
+ */
+
+/*
+ * Daemon code not supporting IP injection (legacy daemon).
+ */
+
+#define KVP_OP_REGISTER	4
+
+/*
+ * Daemon code supporting IP injection.
+ * The KVP opcode field is used to communicate the
+ * registration information; so define a namespace that
+ * will be distinct from the host defined KVP opcode.
+ */
+
+#define KVP_OP_REGISTER1 100
+
+enum hv_kvp_exchg_op {
+	KVP_OP_GET = 0,
+	KVP_OP_SET,
+	KVP_OP_DELETE,
+	KVP_OP_ENUMERATE,
+	KVP_OP_GET_IP_INFO,
+	KVP_OP_SET_IP_INFO,
+	KVP_OP_COUNT /* Number of operations, must be last. */
+};
+
+enum hv_kvp_exchg_pool {
+	KVP_POOL_EXTERNAL = 0,
+	KVP_POOL_GUEST,
+	KVP_POOL_AUTO,
+	KVP_POOL_AUTO_EXTERNAL,
+	KVP_POOL_AUTO_INTERNAL,
+	KVP_POOL_COUNT /* Number of pools, must be last. */
+};
+
+/*
+ * Some Hyper-V status codes.
+ */
+
+#define HV_S_OK				0x00000000
+#define HV_E_FAIL			0x80004005
+#define HV_S_CONT			0x80070103
+#define HV_ERROR_NOT_SUPPORTED		0x80070032
+#define HV_ERROR_MACHINE_LOCKED		0x800704F7
+#define HV_ERROR_DEVICE_NOT_CONNECTED	0x8007048F
+#define HV_INVALIDARG			0x80070057
+#define HV_GUID_NOTFOUND		0x80041002
+#define HV_ERROR_ALREADY_EXISTS		0x80070050
+#define HV_ERROR_DISK_FULL		0x80070070
+
+#define ADDR_FAMILY_NONE	0x00
+#define ADDR_FAMILY_IPV4	0x01
+#define ADDR_FAMILY_IPV6	0x02
+
+#define MAX_ADAPTER_ID_SIZE	128
+#define MAX_IP_ADDR_SIZE	1024
+#define MAX_GATEWAY_SIZE	512
+
+
+struct hv_kvp_ipaddr_value {
+	__u16	adapter_id[MAX_ADAPTER_ID_SIZE];
+	__u8	addr_family;
+	__u8	dhcp_enabled;
+	__u16	ip_addr[MAX_IP_ADDR_SIZE];
+	__u16	sub_net[MAX_IP_ADDR_SIZE];
+	__u16	gate_way[MAX_GATEWAY_SIZE];
+	__u16	dns_addr[MAX_IP_ADDR_SIZE];
+} __attribute__((packed));
+
+
+struct hv_kvp_hdr {
+	__u8 operation;
+	__u8 pool;
+	__u16 pad;
+} __attribute__((packed));
+
+struct hv_kvp_exchg_msg_value {
+	__u32 value_type;
+	__u32 key_size;
+	__u32 value_size;
+	__u8 key[HV_KVP_EXCHANGE_MAX_KEY_SIZE];
+	union {
+		__u8 value[HV_KVP_EXCHANGE_MAX_VALUE_SIZE];
+		__u32 value_u32;
+		__u64 value_u64;
+	};
+} __attribute__((packed));
+
+struct hv_kvp_msg_enumerate {
+	__u32 index;
+	struct hv_kvp_exchg_msg_value data;
+} __attribute__((packed));
+
+struct hv_kvp_msg_get {
+	struct hv_kvp_exchg_msg_value data;
+};
+
+struct hv_kvp_msg_set {
+	struct hv_kvp_exchg_msg_value data;
+};
+
+struct hv_kvp_msg_delete {
+	__u32 key_size;
+	__u8 key[HV_KVP_EXCHANGE_MAX_KEY_SIZE];
+};
+
+struct hv_kvp_register {
+	__u8 version[HV_KVP_EXCHANGE_MAX_KEY_SIZE];
+};
+
+struct hv_kvp_msg {
+	union {
+		struct hv_kvp_hdr	kvp_hdr;
+		int error;
+	};
+	union {
+		struct hv_kvp_msg_get		kvp_get;
+		struct hv_kvp_msg_set		kvp_set;
+		struct hv_kvp_msg_delete	kvp_delete;
+		struct hv_kvp_msg_enumerate	kvp_enum_data;
+		struct hv_kvp_ipaddr_value      kvp_ip_val;
+		struct hv_kvp_register		kvp_register;
+	} body;
+} __attribute__((packed));
+
+struct hv_kvp_ip_msg {
+	__u8 operation;
+	__u8 pool;
+	struct hv_kvp_ipaddr_value      kvp_ip_val;
+} __attribute__((packed));
+
+#endif /* _UAPI_HYPERV_H */
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/hyperv/mmu.c linux-3.10.0-957.el7.lis/arch/x86/hyperv/mmu.c
--- linux-3.10.0-957.el7.orig/arch/x86/hyperv/mmu.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/hyperv/mmu.c	2018-12-12 00:51:40.667785552 +0000
@@ -1,16 +1,16 @@
 #define pr_fmt(fmt)  "Hyper-V: " fmt
 
-#include <linux/hyperv.h>
+#include "include/linux/hyperv.h"
 #include <linux/log2.h>
 #include <linux/slab.h>
 #include <linux/types.h>
 
-#include <asm/mshyperv.h>
+#include "include/asm/mshyperv.h"
 #include <asm/msr.h>
 #include <asm/tlbflush.h>
 
 #define CREATE_TRACE_POINTS
-#include <asm/trace/hyperv.h>
+#include "include/asm/trace/hyperv.h"
 
 /* HvFlushVirtualAddressSpace, HvFlushVirtualAddressList hypercalls */
 struct hv_flush_pcpu {
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/include/asm/mshyperv.h linux-3.10.0-957.el7.lis/arch/x86/include/asm/mshyperv.h
--- linux-3.10.0-957.el7.orig/arch/x86/include/asm/mshyperv.h	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/include/asm/mshyperv.h	2018-12-12 00:51:40.923783925 +0000
@@ -2,11 +2,12 @@
 #define _ASM_X86_MSHYPER_H
 
 #include <linux/types.h>
-#include <linux/atomic.h>
+#include <linux/interrupt.h>
+#include <linux/clocksource.h>
 #include <linux/nmi.h>
+#include <linux/hv_compat.h>
 #include <asm/io.h>
 #include <asm/hyperv.h>
-#include <asm/nospec-branch.h>
 
 /*
  * The below CPUID leaves are present if VersionAndFeatures.HypervisorPresent
@@ -27,17 +28,30 @@
 	HVCPUID_IMPLEMENTATION_LIMITS		= 0x40000005,
 };
 
-struct ms_hyperv_info {
+/*
+ * RHEL 7.x kernels (e.g. 7.0) have different formats of struct ms_hyperv_info,
+ * so we shouldn't use the in-kenrel formats.
+ *
+ * Let's define a new struct ms_hyperv_info_external and use it in the non-builtin
+ * LIS drivers.
+ *
+ * The new struct is initialized in hv-rhel*.x/hv/arch/x86/hyperv/ms_hyperv_ext.c:
+ * void init_ms_hyperv_ext(void)
+ */
+struct ms_hyperv_info_external {
 	u32 features;
 	u32 misc_features;
 	u32 hints;
 };
 
-extern struct ms_hyperv_info ms_hyperv;
+extern struct ms_hyperv_info_external ms_hyperv_ext;
+extern void init_ms_hyperv_ext(void);
 
 /*
- * Declare the MSR used to setup pages used to communicate with the hypervisor.
- */
+ *  * Declare the MSR used to setup pages used to communicate with the hypervisor.
+ *   */
+#define HV_X64_MSR_HYPERCALL	0x40000001
+
 union hv_x64_msr_hypercall_contents {
 	u64 as_uint64;
 	struct {
@@ -64,7 +78,7 @@
  * The guest ID is a 64 bit entity and the structure of this ID is
  * specified in the Hyper-V specification:
  *
- * msdn.microsoft.com/en-us/library/windows/hardware/ff542653%28v=vs.85%29.aspx
+ * http://msdn.microsoft.com/en-us/library/windows/hardware/ff542653%28v=vs.85%29.aspx
  *
  * While the current guideline does not specify how Linux guest ID(s)
  * need to be generated, our plan is to publish the guidelines for
@@ -80,29 +94,35 @@
  * 47:16 - Linux kernel version number
  * 15:0  - Distro specific identification
  *
- *
+ * Distro specific identification are defined as:
+ *   SuSE SLE      0x10
+ *   RHEL          0x20
+ *   CentOS        0x21
+ *   Oracle (RHCK) 0x22
+ *   Oracle (UEK)  0x40
+ *   Debian        0x60
+ *   Ubuntu        0x80
  */
 
-#define HV_LINUX_VENDOR_ID              0x8100
+#define HV_LINUX_VENDOR_ID		0x8100
 
 /*
  * Generate the guest ID based on the guideline described above.
  */
 
-static inline  __u64 generate_guest_id(__u64 d_info1, __u64 kernel_version,
-				       __u64 d_info2)
+static inline  __u64 generate_guest_id(__u8 d_info1, __u32 kernel_version,
+					__u16 d_info2)
 {
 	__u64 guest_id = 0;
 
 	guest_id = (((__u64)HV_LINUX_VENDOR_ID) << 48);
-	guest_id |= (d_info1 << 48);
-	guest_id |= (kernel_version << 16);
-	guest_id |= d_info2;
+	guest_id |= (((__u64)(d_info1)) << 48);
+	guest_id |= (((__u64)(kernel_version)) << 16);
+	guest_id |= ((__u64)(d_info2));
 
 	return guest_id;
 }
 
-
 /* Free the message slot and signal end-of-message if required */
 static inline void vmbus_signal_eom(struct hv_message *msg, u32 old_msg_type)
 {
@@ -142,20 +162,6 @@
 #define hv_init_timer(timer, tick) wrmsrl(timer, tick)
 #define hv_init_timer_config(config, val) wrmsrl(config, val)
 
-#define hv_get_simp(val) rdmsrl(HV_X64_MSR_SIMP, val)
-#define hv_set_simp(val) wrmsrl(HV_X64_MSR_SIMP, val)
-
-#define hv_get_siefp(val) rdmsrl(HV_X64_MSR_SIEFP, val)
-#define hv_set_siefp(val) wrmsrl(HV_X64_MSR_SIEFP, val)
-
-#define hv_get_synic_state(val) rdmsrl(HV_X64_MSR_SCONTROL, val)
-#define hv_set_synic_state(val) wrmsrl(HV_X64_MSR_SCONTROL, val)
-
-#define hv_get_vp_index(index) rdmsrl(HV_X64_MSR_VP_INDEX, index)
-
-#define hv_get_synint_state(int_num, val) rdmsrl(int_num, val)
-#define hv_set_synint_state(int_num, val) wrmsrl(int_num, val)
-
 void hyperv_callback_vector(void);
 #ifdef CONFIG_TRACING
 #define trace_hyperv_callback_vector hyperv_callback_vector
@@ -178,18 +184,16 @@
 	u64 input_address = input ? virt_to_phys(input) : 0;
 	u64 output_address = output ? virt_to_phys(output) : 0;
 	u64 hv_status;
-	register void *__sp asm(_ASM_SP);
 
 #ifdef CONFIG_X86_64
 	if (!hv_hypercall_pg)
 		return U64_MAX;
 
 	__asm__ __volatile__("mov %4, %%r8\n"
-			     CALL_NOSPEC
-			     : "=a" (hv_status), "+r" (__sp),
+			     "call *%5"
+                            : "=a" (hv_status), ASM_CALL_CONSTRAINT,
 			       "+c" (control), "+d" (input_address)
-			     :  "r" (output_address),
-				THUNK_TARGET(hv_hypercall_pg)
+			     :  "r" (output_address), "m" (hv_hypercall_pg)
 			     : "cc", "memory", "r8", "r9", "r10", "r11");
 #else
 	u32 input_address_hi = upper_32_bits(input_address);
@@ -200,13 +204,13 @@
 	if (!hv_hypercall_pg)
 		return U64_MAX;
 
-	__asm__ __volatile__(CALL_NOSPEC
+	__asm__ __volatile__("call *%7"
 			     : "=A" (hv_status),
-			       "+c" (input_address_lo), "+r" (__sp)
+                              "+c" (input_address_lo), ASM_CALL_CONSTRAINT
 			     : "A" (control),
 			       "b" (input_address_hi),
 			       "D"(output_address_hi), "S"(output_address_lo),
-			       THUNK_TARGET(hv_hypercall_pg)
+			       "m" (hv_hypercall_pg)
 			     : "cc", "memory");
 #endif /* !x86_64 */
 	return hv_status;
@@ -224,14 +228,13 @@
 static inline u64 hv_do_fast_hypercall8(u16 code, u64 input1)
 {
 	u64 hv_status, control = (u64)code | HV_HYPERCALL_FAST_BIT;
-	register void *__sp asm(_ASM_SP);
 
 #ifdef CONFIG_X86_64
 	{
-		__asm__ __volatile__(CALL_NOSPEC
-				     : "=a" (hv_status), "+r" (__sp),
+		__asm__ __volatile__("call *%4"
+                                    : "=a" (hv_status), ASM_CALL_CONSTRAINT,
 				       "+c" (control), "+d" (input1)
-				     : THUNK_TARGET(hv_hypercall_pg)
+				     : "m" (hv_hypercall_pg)
 				     : "cc", "r8", "r9", "r10", "r11");
 	}
 #else
@@ -239,13 +242,13 @@
 		u32 input1_hi = upper_32_bits(input1);
 		u32 input1_lo = lower_32_bits(input1);
 
-		__asm__ __volatile__ (CALL_NOSPEC
+		__asm__ __volatile__ ("call *%5"
 				      : "=A"(hv_status),
 					"+c"(input1_lo),
-					"+r"(__sp)
+                                       ASM_CALL_CONSTRAINT
 				      :	"A" (control),
 					"b" (input1_hi),
-					THUNK_TARGET(hv_hypercall_pg)
+					"m" (hv_hypercall_pg)
 				      : "cc", "edi", "esi");
 	}
 #endif
@@ -291,7 +294,6 @@
  * to this information.
  */
 extern u32 *hv_vp_index;
-extern u32 hv_max_vp_index;
 
 /**
  * hv_cpu_number_to_vp_number() - Map CPU to VP.
@@ -310,70 +312,24 @@
 }
 
 void hyperv_init(void);
-void hyperv_setup_mmu_ops(void);
-void hyper_alloc_mmu(void);
 void hyperv_report_panic(struct pt_regs *regs, long err);
-bool hv_is_hyperv_initialized(void);
+int hv_cpu_init(unsigned int cpu);
+bool hv_is_hypercall_page_setup(void);
 void hyperv_cleanup(void);
-#else /* CONFIG_HYPERV */
-static inline void hyperv_init(void) {}
-static inline bool hv_is_hyperv_initialized(void) { return false; }
-static inline void hyperv_cleanup(void) {}
-static inline void hyperv_setup_mmu_ops(void) {}
-#endif /* CONFIG_HYPERV */
-
-#ifdef CONFIG_HYPERV_TSCPAGE
-struct ms_hyperv_tsc_page *hv_get_tsc_page(void);
-static inline u64 hv_read_tsc_page(const struct ms_hyperv_tsc_page *tsc_pg)
-{
-	u64 scale, offset, cur_tsc;
-	u32 sequence;
-
-	/*
-	 * The protocol for reading Hyper-V TSC page is specified in Hypervisor
-	 * Top-Level Functional Specification ver. 3.0 and above. To get the
-	 * reference time we must do the following:
-	 * - READ ReferenceTscSequence
-	 *   A special '0' value indicates the time source is unreliable and we
-	 *   need to use something else. The currently published specification
-	 *   versions (up to 4.0b) contain a mistake and wrongly claim '-1'
-	 *   instead of '0' as the special value, see commit c35b82ef0294.
-	 * - ReferenceTime =
-	 *        ((RDTSC() * ReferenceTscScale) >> 64) + ReferenceTscOffset
-	 * - READ ReferenceTscSequence again. In case its value has changed
-	 *   since our first reading we need to discard ReferenceTime and repeat
-	 *   the whole sequence as the hypervisor was updating the page in
-	 *   between.
-	 */
-	do {
-		sequence = READ_ONCE(tsc_pg->tsc_sequence);
-		if (!sequence)
-			return U64_MAX;
-		/*
-		 * Make sure we read sequence before we read other values from
-		 * TSC page.
-		 */
-		smp_rmb();
-
-		scale = READ_ONCE(tsc_pg->tsc_scale);
-		offset = READ_ONCE(tsc_pg->tsc_offset);
-		cur_tsc = rdtsc_ordered();
+void hv_print_host_info(void);
+#endif
 
-		/*
-		 * Make sure we read sequence after we read all other values
-		 * from TSC page.
-		 */
-		smp_rmb();
+#define hv_get_synint_state(int_num, val) rdmsrl(int_num, val)
+#define hv_set_synint_state(int_num, val) wrmsrl(int_num, val)
 
-	} while (READ_ONCE(tsc_pg->tsc_sequence) != sequence);
+#define hv_get_simp(val) rdmsrl(HV_X64_MSR_SIMP, val)
+#define hv_set_simp(val) wrmsrl(HV_X64_MSR_SIMP, val)
 
-	return mul_u64_u64_shr(cur_tsc, scale, 64) + offset;
-}
+#define hv_get_siefp(val) rdmsrl(HV_X64_MSR_SIEFP, val)
+#define hv_set_siefp(val) wrmsrl(HV_X64_MSR_SIEFP, val)
+#define hv_get_synic_state(val) rdmsrl(HV_X64_MSR_SCONTROL, val)
+#define hv_set_synic_state(val) wrmsrl(HV_X64_MSR_SCONTROL, val)
 
-#else
-static inline struct ms_hyperv_tsc_page *hv_get_tsc_page(void)
-{
-	return NULL;
-}
-#endif
+#define hv_get_vp_index(index) rdmsrl(HV_X64_MSR_VP_INDEX, index)
+void hv_register_vmbus_handler(int irq, irq_handler_t handler);
 #endif
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/include/uapi/asm/hyperv.h linux-3.10.0-957.el7.lis/arch/x86/include/uapi/asm/hyperv.h
--- linux-3.10.0-957.el7.orig/arch/x86/include/uapi/asm/hyperv.h	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/include/uapi/asm/hyperv.h	2018-12-12 00:51:40.915783976 +0000
@@ -34,10 +34,16 @@
 #define HV_X64_MSR_REFERENCE_TSC		0x40000021
 
 /*
- * There is a single feature flag that signifies if the partition has access
- * to MSRs with local APIC and TSC frequencies.
+ * There is a single feature flag that signifies the presence of the MSR
+ * that can be used to retrieve both the local APIC Timer frequency as
+ * well as the TSC frequency.
  */
-#define HV_X64_ACCESS_FREQUENCY_MSRS		(1 << 11)
+
+/* Local APIC timer frequency MSR (HV_X64_MSR_APIC_FREQUENCY) is available */
+#define HV_X64_MSR_APIC_FREQUENCY_AVAILABLE (1 << 11)
+
+/* TSC frequency MSR (HV_X64_MSR_TSC_FREQUENCY) is available */
+#define HV_X64_MSR_TSC_FREQUENCY_AVAILABLE (1 << 11)
 
 /*
  * Basic SynIC MSRs (HV_X64_MSR_SCONTROL through HV_X64_MSR_EOM
@@ -67,9 +73,6 @@
   */
 #define HV_X64_MSR_STAT_PAGES_AVAILABLE		(1 << 8)
 
-/* Frequency MSRs available */
-#define HV_FEATURE_FREQUENCY_MSRS_AVAILABLE	(1 << 8)
-
 /* Crash MSR available */
 #define HV_FEATURE_GUEST_CRASH_MSR_AVAILABLE (1 << 10)
 
@@ -143,16 +146,15 @@
 #define HV_X64_RELAXED_TIMING_RECOMMENDED	(1 << 5)
 
 /*
- * HV_VP_SET available
- */
-#define HV_X64_EX_PROCESSOR_MASKS_RECOMMENDED	(1 << 11)
-/*
  * Virtual APIC support
  */
 #define HV_X64_DEPRECATING_AEOI_RECOMMENDED	(1 << 9)
 
-/* Recommend using the newer ExProcessorMasks interface */
-#define HV_X64_EX_PROCESSOR_MASKS_RECOMMENDED	(1 << 11)
+/*
+ * HV_VP_SET available
+ */
+#define HV_X64_EX_PROCESSOR_MASKS_RECOMMENDED	(1 < 11)
+
 
 /*
  * Crash notification flag.
@@ -168,12 +170,6 @@
 /* MSR used to provide vcpu index */
 #define HV_X64_MSR_VP_INDEX			0x40000002
 
-/* MSR used to reset the guest OS. */
-#define HV_X64_MSR_RESET			0x40000003
-
-/* MSR used to provide vcpu runtime in 100ns units */
-#define HV_X64_MSR_VP_RUNTIME			0x40000010
-
 /* MSR used to read the per-partition time reference counter */
 #define HV_X64_MSR_TIME_REF_COUNT		0x40000020
 
@@ -241,11 +237,7 @@
 		(~((1ull << HV_X64_MSR_HYPERCALL_PAGE_ADDRESS_SHIFT) - 1))
 
 /* Declare the various hypercall operations. */
-#define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE	0x0002
-#define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST	0x0003
-#define HVCALL_NOTIFY_LONG_SPIN_WAIT		0x0008
-#define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX  0x0013
-#define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST_EX   0x0014
+#define HV_X64_HV_NOTIFY_LONG_SPIN_WAIT		0x0008
 #define HVCALL_POST_MESSAGE			0x005c
 #define HVCALL_SIGNAL_EVENT			0x005d
 
@@ -262,16 +254,6 @@
 #define HV_PROCESSOR_POWER_STATE_C2		2
 #define HV_PROCESSOR_POWER_STATE_C3		3
 
-#define HV_FLUSH_ALL_PROCESSORS			BIT(0)
-#define HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES	BIT(1)
-#define HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY	BIT(2)
-#define HV_FLUSH_USE_EXTENDED_RANGE_FORMAT	BIT(3)
-
-enum HV_GENERIC_SET_FORMAT {
-	HV_GENERIC_SET_SPARCE_4K,
-	HV_GENERIC_SET_ALL,
-};
-
 /* hypercall status code */
 #define HV_STATUS_SUCCESS			0
 #define HV_STATUS_INVALID_HYPERCALL_CODE	2
@@ -290,18 +272,17 @@
 
 /* Define the number of synthetic interrupt sources. */
 #define HV_SYNIC_SINT_COUNT		(16)
-/* Define the expected SynIC version. */
-#define HV_SYNIC_VERSION_1		(0x1)
-
-#define HV_SYNIC_CONTROL_ENABLE		(1ULL << 0)
-#define HV_SYNIC_SIMP_ENABLE		(1ULL << 0)
-#define HV_SYNIC_SIEFP_ENABLE		(1ULL << 0)
-#define HV_SYNIC_SINT_MASKED		(1ULL << 16)
-#define HV_SYNIC_SINT_AUTO_EOI		(1ULL << 17)
-#define HV_SYNIC_SINT_VECTOR_MASK	(0xFF)
 
 #define HV_SYNIC_STIMER_COUNT		(4)
 
+/* Define timer message payload structure. */
+struct hv_timer_message_payload {
+	__u32 timer_index;
+	__u32 reserved;
+	__u64 expiration_time;	/* When the timer expired */
+	__u64 delivery_time;	/* When the message was delivered */
+};
+
 /* Define synthetic interrupt controller message constants. */
 #define HV_MESSAGE_SIZE			(256)
 #define HV_MESSAGE_PAYLOAD_BYTE_COUNT	(240)
@@ -378,12 +359,4 @@
 	struct hv_message sint_message[HV_SYNIC_SINT_COUNT];
 };
 
-/* Define timer message payload structure. */
-struct hv_timer_message_payload {
-	__u32 timer_index;
-	__u32 reserved;
-	__u64 expiration_time;	/* When the timer expired */
-	__u64 delivery_time;	/* When the message was delivered */
-};
-
 #endif
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/include/uapi/asm/kvm_para.h linux-3.10.0-957.el7.lis/arch/x86/include/uapi/asm/kvm_para.h
--- linux-3.10.0-957.el7.orig/arch/x86/include/uapi/asm/kvm_para.h	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/include/uapi/asm/kvm_para.h	2018-12-12 00:51:40.672785520 +0000
@@ -2,7 +2,7 @@
 #define _UAPI_ASM_X86_KVM_PARA_H
 
 #include <linux/types.h>
-#include <asm/hyperv.h>
+#include "../../../hyperv/include/asm/hyperv.h"
 
 /* This CPUID returns the signature 'KVMKVMKVM' in ebx, ecx, and edx.  It
  * should be used to determine that a VM is running under KVM.
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/kernel/cpu/hyperv.h linux-3.10.0-957.el7.lis/arch/x86/kernel/cpu/hyperv.h
--- linux-3.10.0-957.el7.orig/arch/x86/kernel/cpu/hyperv.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/kernel/cpu/hyperv.h	2018-12-12 00:51:40.619785857 +0000
@@ -0,0 +1,389 @@
+#ifndef _ASM_X86_HYPERV_H
+#define _ASM_X86_HYPERV_H
+
+#include <linux/types.h>
+
+/*
+ * The below CPUID leaves are present if VersionAndFeatures.HypervisorPresent
+ * is set by CPUID(HvCpuIdFunctionVersionAndFeatures).
+ */
+#define HYPERV_CPUID_VENDOR_AND_MAX_FUNCTIONS	0x40000000
+#define HYPERV_CPUID_INTERFACE			0x40000001
+#define HYPERV_CPUID_VERSION			0x40000002
+#define HYPERV_CPUID_FEATURES			0x40000003
+#define HYPERV_CPUID_ENLIGHTMENT_INFO		0x40000004
+#define HYPERV_CPUID_IMPLEMENT_LIMITS		0x40000005
+
+#define HYPERV_HYPERVISOR_PRESENT_BIT		0x80000000
+#define HYPERV_CPUID_MIN			0x40000005
+#define HYPERV_CPUID_MAX			0x4000ffff
+
+/*
+ * Feature identification. EAX indicates which features are available
+ * to the partition based upon the current partition privileges.
+ */
+
+/* VP Runtime (HV_X64_MSR_VP_RUNTIME) available */
+#define HV_X64_MSR_VP_RUNTIME_AVAILABLE		(1 << 0)
+/* Partition Reference Counter (HV_X64_MSR_TIME_REF_COUNT) available*/
+#define HV_X64_MSR_TIME_REF_COUNT_AVAILABLE	(1 << 1)
+/* Partition reference TSC MSR is available */
+#define HV_X64_MSR_REFERENCE_TSC_AVAILABLE              (1 << 9)
+
+/* A partition's reference time stamp counter (TSC) page */
+#define HV_X64_MSR_REFERENCE_TSC		0x40000021
+
+/*
+ * There is a single feature flag that signifies if the partition has access
+ * to MSRs with local APIC and TSC frequencies.
+ */
+#define HV_X64_ACCESS_FREQUENCY_MSRS		(1 << 11)
+
+/*
+ * Basic SynIC MSRs (HV_X64_MSR_SCONTROL through HV_X64_MSR_EOM
+ * and HV_X64_MSR_SINT0 through HV_X64_MSR_SINT15) available
+ */
+#define HV_X64_MSR_SYNIC_AVAILABLE		(1 << 2)
+/*
+ * Synthetic Timer MSRs (HV_X64_MSR_STIMER0_CONFIG through
+ * HV_X64_MSR_STIMER3_COUNT) available
+ */
+#define HV_X64_MSR_SYNTIMER_AVAILABLE		(1 << 3)
+/*
+ * APIC access MSRs (HV_X64_MSR_EOI, HV_X64_MSR_ICR and HV_X64_MSR_TPR)
+ * are available
+ */
+#define HV_X64_MSR_APIC_ACCESS_AVAILABLE	(1 << 4)
+/* Hypercall MSRs (HV_X64_MSR_GUEST_OS_ID and HV_X64_MSR_HYPERCALL) available*/
+#define HV_X64_MSR_HYPERCALL_AVAILABLE		(1 << 5)
+/* Access virtual processor index MSR (HV_X64_MSR_VP_INDEX) available*/
+#define HV_X64_MSR_VP_INDEX_AVAILABLE		(1 << 6)
+/* Virtual system reset MSR (HV_X64_MSR_RESET) is available*/
+#define HV_X64_MSR_RESET_AVAILABLE		(1 << 7)
+ /*
+  * Access statistics pages MSRs (HV_X64_MSR_STATS_PARTITION_RETAIL_PAGE,
+  * HV_X64_MSR_STATS_PARTITION_INTERNAL_PAGE, HV_X64_MSR_STATS_VP_RETAIL_PAGE,
+  * HV_X64_MSR_STATS_VP_INTERNAL_PAGE) available
+  */
+#define HV_X64_MSR_STAT_PAGES_AVAILABLE		(1 << 8)
+
+/* Frequency MSRs available */
+#define HV_FEATURE_FREQUENCY_MSRS_AVAILABLE	(1 << 8)
+
+/* Crash MSR available */
+#define HV_FEATURE_GUEST_CRASH_MSR_AVAILABLE (1 << 10)
+
+/*
+ * Feature identification: EBX indicates which flags were specified at
+ * partition creation. The format is the same as the partition creation
+ * flag structure defined in section Partition Creation Flags.
+ */
+#define HV_X64_CREATE_PARTITIONS		(1 << 0)
+#define HV_X64_ACCESS_PARTITION_ID		(1 << 1)
+#define HV_X64_ACCESS_MEMORY_POOL		(1 << 2)
+#define HV_X64_ADJUST_MESSAGE_BUFFERS		(1 << 3)
+#define HV_X64_POST_MESSAGES			(1 << 4)
+#define HV_X64_SIGNAL_EVENTS			(1 << 5)
+#define HV_X64_CREATE_PORT			(1 << 6)
+#define HV_X64_CONNECT_PORT			(1 << 7)
+#define HV_X64_ACCESS_STATS			(1 << 8)
+#define HV_X64_DEBUGGING			(1 << 11)
+#define HV_X64_CPU_POWER_MANAGEMENT		(1 << 12)
+#define HV_X64_CONFIGURE_PROFILER		(1 << 13)
+
+/*
+ * Feature identification. EDX indicates which miscellaneous features
+ * are available to the partition.
+ */
+/* The MWAIT instruction is available (per section MONITOR / MWAIT) */
+#define HV_X64_MWAIT_AVAILABLE				(1 << 0)
+/* Guest debugging support is available */
+#define HV_X64_GUEST_DEBUGGING_AVAILABLE		(1 << 1)
+/* Performance Monitor support is available*/
+#define HV_X64_PERF_MONITOR_AVAILABLE			(1 << 2)
+/* Support for physical CPU dynamic partitioning events is available*/
+#define HV_X64_CPU_DYNAMIC_PARTITIONING_AVAILABLE	(1 << 3)
+/*
+ * Support for passing hypercall input parameter block via XMM
+ * registers is available
+ */
+#define HV_X64_HYPERCALL_PARAMS_XMM_AVAILABLE		(1 << 4)
+/* Support for a virtual guest idle state is available */
+#define HV_X64_GUEST_IDLE_STATE_AVAILABLE		(1 << 5)
+
+/*
+ * Implementation recommendations. Indicates which behaviors the hypervisor
+ * recommends the OS implement for optimal performance.
+ */
+ /*
+  * Recommend using hypercall for address space switches rather
+  * than MOV to CR3 instruction
+  */
+#define HV_X64_AS_SWITCH_RECOMMENDED		(1 << 0)
+/* Recommend using hypercall for local TLB flushes rather
+ * than INVLPG or MOV to CR3 instructions */
+#define HV_X64_LOCAL_TLB_FLUSH_RECOMMENDED	(1 << 1)
+/*
+ * Recommend using hypercall for remote TLB flushes rather
+ * than inter-processor interrupts
+ */
+#define HV_X64_REMOTE_TLB_FLUSH_RECOMMENDED	(1 << 2)
+/*
+ * Recommend using MSRs for accessing APIC registers
+ * EOI, ICR and TPR rather than their memory-mapped counterparts
+ */
+#define HV_X64_APIC_ACCESS_RECOMMENDED		(1 << 3)
+/* Recommend using the hypervisor-provided MSR to initiate a system RESET */
+#define HV_X64_SYSTEM_RESET_RECOMMENDED		(1 << 4)
+/*
+ * Recommend using relaxed timing for this partition. If used,
+ * the VM should disable any watchdog timeouts that rely on the
+ * timely delivery of external interrupts
+ */
+#define HV_X64_RELAXED_TIMING_RECOMMENDED	(1 << 5)
+
+/*
+ * HV_VP_SET available
+ */
+#define HV_X64_EX_PROCESSOR_MASKS_RECOMMENDED	(1 << 11)
+/*
+ * Virtual APIC support
+ */
+#define HV_X64_DEPRECATING_AEOI_RECOMMENDED	(1 << 9)
+
+/* Recommend using the newer ExProcessorMasks interface */
+#define HV_X64_EX_PROCESSOR_MASKS_RECOMMENDED	(1 << 11)
+
+/*
+ * Crash notification flag.
+ */
+#define HV_CRASH_CTL_CRASH_NOTIFY (1ULL << 63)
+
+/* MSR used to identify the guest OS. */
+#define HV_X64_MSR_GUEST_OS_ID			0x40000000
+
+/* MSR used to setup pages used to communicate with the hypervisor. */
+#define HV_X64_MSR_HYPERCALL			0x40000001
+
+/* MSR used to provide vcpu index */
+#define HV_X64_MSR_VP_INDEX			0x40000002
+
+/* MSR used to reset the guest OS. */
+#define HV_X64_MSR_RESET			0x40000003
+
+/* MSR used to provide vcpu runtime in 100ns units */
+#define HV_X64_MSR_VP_RUNTIME			0x40000010
+
+/* MSR used to read the per-partition time reference counter */
+#define HV_X64_MSR_TIME_REF_COUNT		0x40000020
+
+/* MSR used to retrieve the TSC frequency */
+#define HV_X64_MSR_TSC_FREQUENCY		0x40000022
+
+/* MSR used to retrieve the local APIC timer frequency */
+#define HV_X64_MSR_APIC_FREQUENCY		0x40000023
+
+/* Define the virtual APIC registers */
+#define HV_X64_MSR_EOI				0x40000070
+#define HV_X64_MSR_ICR				0x40000071
+#define HV_X64_MSR_TPR				0x40000072
+#define HV_X64_MSR_APIC_ASSIST_PAGE		0x40000073
+
+/* Define synthetic interrupt controller model specific registers. */
+#define HV_X64_MSR_SCONTROL			0x40000080
+#define HV_X64_MSR_SVERSION			0x40000081
+#define HV_X64_MSR_SIEFP			0x40000082
+#define HV_X64_MSR_SIMP				0x40000083
+#define HV_X64_MSR_EOM				0x40000084
+#define HV_X64_MSR_SINT0			0x40000090
+#define HV_X64_MSR_SINT1			0x40000091
+#define HV_X64_MSR_SINT2			0x40000092
+#define HV_X64_MSR_SINT3			0x40000093
+#define HV_X64_MSR_SINT4			0x40000094
+#define HV_X64_MSR_SINT5			0x40000095
+#define HV_X64_MSR_SINT6			0x40000096
+#define HV_X64_MSR_SINT7			0x40000097
+#define HV_X64_MSR_SINT8			0x40000098
+#define HV_X64_MSR_SINT9			0x40000099
+#define HV_X64_MSR_SINT10			0x4000009A
+#define HV_X64_MSR_SINT11			0x4000009B
+#define HV_X64_MSR_SINT12			0x4000009C
+#define HV_X64_MSR_SINT13			0x4000009D
+#define HV_X64_MSR_SINT14			0x4000009E
+#define HV_X64_MSR_SINT15			0x4000009F
+
+/*
+ * Synthetic Timer MSRs. Four timers per vcpu.
+ */
+#define HV_X64_MSR_STIMER0_CONFIG		0x400000B0
+#define HV_X64_MSR_STIMER0_COUNT		0x400000B1
+#define HV_X64_MSR_STIMER1_CONFIG		0x400000B2
+#define HV_X64_MSR_STIMER1_COUNT		0x400000B3
+#define HV_X64_MSR_STIMER2_CONFIG		0x400000B4
+#define HV_X64_MSR_STIMER2_COUNT		0x400000B5
+#define HV_X64_MSR_STIMER3_CONFIG		0x400000B6
+#define HV_X64_MSR_STIMER3_COUNT		0x400000B7
+
+/* Hyper-V guest crash notification MSR's */
+#define HV_X64_MSR_CRASH_P0			0x40000100
+#define HV_X64_MSR_CRASH_P1			0x40000101
+#define HV_X64_MSR_CRASH_P2			0x40000102
+#define HV_X64_MSR_CRASH_P3			0x40000103
+#define HV_X64_MSR_CRASH_P4			0x40000104
+#define HV_X64_MSR_CRASH_CTL			0x40000105
+#define HV_X64_MSR_CRASH_CTL_NOTIFY		(1ULL << 63)
+#define HV_X64_MSR_CRASH_PARAMS		\
+		(1 + (HV_X64_MSR_CRASH_P4 - HV_X64_MSR_CRASH_P0))
+
+#define HV_X64_MSR_HYPERCALL_ENABLE		0x00000001
+#define HV_X64_MSR_HYPERCALL_PAGE_ADDRESS_SHIFT	12
+#define HV_X64_MSR_HYPERCALL_PAGE_ADDRESS_MASK	\
+		(~((1ull << HV_X64_MSR_HYPERCALL_PAGE_ADDRESS_SHIFT) - 1))
+
+/* Declare the various hypercall operations. */
+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE	0x0002
+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST	0x0003
+#define HVCALL_NOTIFY_LONG_SPIN_WAIT		0x0008
+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX  0x0013
+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST_EX   0x0014
+#define HVCALL_POST_MESSAGE			0x005c
+#define HVCALL_SIGNAL_EVENT			0x005d
+
+#define HV_X64_MSR_APIC_ASSIST_PAGE_ENABLE		0x00000001
+#define HV_X64_MSR_APIC_ASSIST_PAGE_ADDRESS_SHIFT	12
+#define HV_X64_MSR_APIC_ASSIST_PAGE_ADDRESS_MASK	\
+		(~((1ull << HV_X64_MSR_APIC_ASSIST_PAGE_ADDRESS_SHIFT) - 1))
+
+#define HV_X64_MSR_TSC_REFERENCE_ENABLE		0x00000001
+#define HV_X64_MSR_TSC_REFERENCE_ADDRESS_SHIFT	12
+
+#define HV_PROCESSOR_POWER_STATE_C0		0
+#define HV_PROCESSOR_POWER_STATE_C1		1
+#define HV_PROCESSOR_POWER_STATE_C2		2
+#define HV_PROCESSOR_POWER_STATE_C3		3
+
+#define HV_FLUSH_ALL_PROCESSORS			BIT(0)
+#define HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES	BIT(1)
+#define HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY	BIT(2)
+#define HV_FLUSH_USE_EXTENDED_RANGE_FORMAT	BIT(3)
+
+enum HV_GENERIC_SET_FORMAT {
+	HV_GENERIC_SET_SPARCE_4K,
+	HV_GENERIC_SET_ALL,
+};
+
+/* hypercall status code */
+#define HV_STATUS_SUCCESS			0
+#define HV_STATUS_INVALID_HYPERCALL_CODE	2
+#define HV_STATUS_INVALID_HYPERCALL_INPUT	3
+#define HV_STATUS_INVALID_ALIGNMENT		4
+#define HV_STATUS_INSUFFICIENT_MEMORY		11
+#define HV_STATUS_INVALID_CONNECTION_ID		18
+#define HV_STATUS_INSUFFICIENT_BUFFERS		19
+
+typedef struct _HV_REFERENCE_TSC_PAGE {
+	__u32 tsc_sequence;
+	__u32 res1;
+	__u64 tsc_scale;
+	__s64 tsc_offset;
+} HV_REFERENCE_TSC_PAGE, *PHV_REFERENCE_TSC_PAGE;
+
+/* Define the number of synthetic interrupt sources. */
+#define HV_SYNIC_SINT_COUNT		(16)
+/* Define the expected SynIC version. */
+#define HV_SYNIC_VERSION_1		(0x1)
+
+#define HV_SYNIC_CONTROL_ENABLE		(1ULL << 0)
+#define HV_SYNIC_SIMP_ENABLE		(1ULL << 0)
+#define HV_SYNIC_SIEFP_ENABLE		(1ULL << 0)
+#define HV_SYNIC_SINT_MASKED		(1ULL << 16)
+#define HV_SYNIC_SINT_AUTO_EOI		(1ULL << 17)
+#define HV_SYNIC_SINT_VECTOR_MASK	(0xFF)
+
+#define HV_SYNIC_STIMER_COUNT		(4)
+
+/* Define synthetic interrupt controller message constants. */
+#define HV_MESSAGE_SIZE			(256)
+#define HV_MESSAGE_PAYLOAD_BYTE_COUNT	(240)
+#define HV_MESSAGE_PAYLOAD_QWORD_COUNT	(30)
+
+/* Define hypervisor message types. */
+enum hv_message_type {
+	HVMSG_NONE			= 0x00000000,
+
+	/* Memory access messages. */
+	HVMSG_UNMAPPED_GPA		= 0x80000000,
+	HVMSG_GPA_INTERCEPT		= 0x80000001,
+
+	/* Timer notification messages. */
+	HVMSG_TIMER_EXPIRED			= 0x80000010,
+
+	/* Error messages. */
+	HVMSG_INVALID_VP_REGISTER_VALUE	= 0x80000020,
+	HVMSG_UNRECOVERABLE_EXCEPTION	= 0x80000021,
+	HVMSG_UNSUPPORTED_FEATURE		= 0x80000022,
+
+	/* Trace buffer complete messages. */
+	HVMSG_EVENTLOG_BUFFERCOMPLETE	= 0x80000040,
+
+	/* Platform-specific processor intercept messages. */
+	HVMSG_X64_IOPORT_INTERCEPT		= 0x80010000,
+	HVMSG_X64_MSR_INTERCEPT		= 0x80010001,
+	HVMSG_X64_CPUID_INTERCEPT		= 0x80010002,
+	HVMSG_X64_EXCEPTION_INTERCEPT	= 0x80010003,
+	HVMSG_X64_APIC_EOI			= 0x80010004,
+	HVMSG_X64_LEGACY_FP_ERROR		= 0x80010005
+};
+
+/* Define synthetic interrupt controller message flags. */
+union hv_message_flags {
+	__u8 asu8;
+	struct {
+		__u8 msg_pending:1;
+		__u8 reserved:7;
+	};
+};
+
+/* Define port identifier type. */
+union hv_port_id {
+	__u32 asu32;
+	struct {
+		__u32 id:24;
+		__u32 reserved:8;
+	} u;
+};
+
+/* Define synthetic interrupt controller message header. */
+struct hv_message_header {
+	__u32 message_type;
+	__u8 payload_size;
+	union hv_message_flags message_flags;
+	__u8 reserved[2];
+	union {
+		__u64 sender;
+		union hv_port_id port;
+	};
+};
+
+/* Define synthetic interrupt controller message format. */
+struct hv_message {
+	struct hv_message_header header;
+	union {
+		__u64 payload[HV_MESSAGE_PAYLOAD_QWORD_COUNT];
+	} u;
+};
+
+/* Define the synthetic interrupt message page layout. */
+struct hv_message_page {
+	struct hv_message sint_message[HV_SYNIC_SINT_COUNT];
+};
+
+/* Define timer message payload structure. */
+struct hv_timer_message_payload {
+	__u32 timer_index;
+	__u32 reserved;
+	__u64 expiration_time;	/* When the timer expired */
+	__u64 delivery_time;	/* When the message was delivered */
+};
+
+#endif
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/kernel/cpu/mshyperv.c linux-3.10.0-957.el7.lis/arch/x86/kernel/cpu/mshyperv.c
--- linux-3.10.0-957.el7.orig/arch/x86/kernel/cpu/mshyperv.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/kernel/cpu/mshyperv.c	2018-12-12 00:51:40.629785793 +0000
@@ -21,8 +21,8 @@
 #include <linux/kexec.h>
 #include <asm/processor.h>
 #include <asm/hypervisor.h>
-#include <asm/hyperv.h>
-#include <asm/mshyperv.h>
+#include "hyperv.h"
+#include "mshyperv.h"
 #include <asm/desc.h>
 #include <asm/idle.h>
 #include <asm/irq_regs.h>
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/kernel/cpu/mshyperv.h linux-3.10.0-957.el7.lis/arch/x86/kernel/cpu/mshyperv.h
--- linux-3.10.0-957.el7.orig/arch/x86/kernel/cpu/mshyperv.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/kernel/cpu/mshyperv.h	2018-12-12 00:51:40.624785825 +0000
@@ -0,0 +1,379 @@
+#ifndef _ASM_X86_MSHYPER_H
+#define _ASM_X86_MSHYPER_H
+
+#include <linux/types.h>
+#include <linux/atomic.h>
+#include <linux/nmi.h>
+#include <asm/io.h>
+#include "hyperv.h"
+#include <asm/nospec-branch.h>
+
+/*
+ * The below CPUID leaves are present if VersionAndFeatures.HypervisorPresent
+ * is set by CPUID(HVCPUID_VERSION_FEATURES).
+ */
+enum hv_cpuid_function {
+	HVCPUID_VERSION_FEATURES		= 0x00000001,
+	HVCPUID_VENDOR_MAXFUNCTION		= 0x40000000,
+	HVCPUID_INTERFACE			= 0x40000001,
+
+	/*
+	 * The remaining functions depend on the value of
+	 * HVCPUID_INTERFACE
+	 */
+	HVCPUID_VERSION				= 0x40000002,
+	HVCPUID_FEATURES			= 0x40000003,
+	HVCPUID_ENLIGHTENMENT_INFO		= 0x40000004,
+	HVCPUID_IMPLEMENTATION_LIMITS		= 0x40000005,
+};
+
+struct ms_hyperv_info {
+	u32 features;
+	u32 misc_features;
+	u32 hints;
+};
+
+extern struct ms_hyperv_info ms_hyperv;
+
+/*
+ * Declare the MSR used to setup pages used to communicate with the hypervisor.
+ */
+union hv_x64_msr_hypercall_contents {
+	u64 as_uint64;
+	struct {
+		u64 enable:1;
+		u64 reserved:11;
+		u64 guest_physical_address:52;
+	};
+};
+
+/*
+ * TSC page layout.
+ */
+
+struct ms_hyperv_tsc_page {
+	volatile u32 tsc_sequence;
+	u32 reserved1;
+	volatile u64 tsc_scale;
+	volatile s64 tsc_offset;
+	u64 reserved2[509];
+};
+
+/*
+ * The guest OS needs to register the guest ID with the hypervisor.
+ * The guest ID is a 64 bit entity and the structure of this ID is
+ * specified in the Hyper-V specification:
+ *
+ * msdn.microsoft.com/en-us/library/windows/hardware/ff542653%28v=vs.85%29.aspx
+ *
+ * While the current guideline does not specify how Linux guest ID(s)
+ * need to be generated, our plan is to publish the guidelines for
+ * Linux and other guest operating systems that currently are hosted
+ * on Hyper-V. The implementation here conforms to this yet
+ * unpublished guidelines.
+ *
+ *
+ * Bit(s)
+ * 63 - Indicates if the OS is Open Source or not; 1 is Open Source
+ * 62:56 - Os Type; Linux is 0x100
+ * 55:48 - Distro specific identification
+ * 47:16 - Linux kernel version number
+ * 15:0  - Distro specific identification
+ *
+ *
+ */
+
+#define HV_LINUX_VENDOR_ID              0x8100
+
+/*
+ * Generate the guest ID based on the guideline described above.
+ */
+
+static inline  __u64 generate_guest_id(__u64 d_info1, __u64 kernel_version,
+				       __u64 d_info2)
+{
+	__u64 guest_id = 0;
+
+	guest_id = (((__u64)HV_LINUX_VENDOR_ID) << 48);
+	guest_id |= (d_info1 << 48);
+	guest_id |= (kernel_version << 16);
+	guest_id |= d_info2;
+
+	return guest_id;
+}
+
+
+/* Free the message slot and signal end-of-message if required */
+static inline void vmbus_signal_eom(struct hv_message *msg, u32 old_msg_type)
+{
+	/*
+	 * On crash we're reading some other CPU's message page and we need
+	 * to be careful: this other CPU may already had cleared the header
+	 * and the host may already had delivered some other message there.
+	 * In case we blindly write msg->header.message_type we're going
+	 * to lose it. We can still lose a message of the same type but
+	 * we count on the fact that there can only be one
+	 * CHANNELMSG_UNLOAD_RESPONSE and we don't care about other messages
+	 * on crash.
+	 */
+	if (cmpxchg(&msg->header.message_type, old_msg_type,
+		    HVMSG_NONE) != old_msg_type)
+		return;
+
+	/*
+	 * Make sure the write to MessageType (ie set to
+	 * HVMSG_NONE) happens before we read the
+	 * MessagePending and EOMing. Otherwise, the EOMing
+	 * will not deliver any more messages since there is
+	 * no empty slot
+	 */
+	mb();
+
+	if (msg->header.message_flags.msg_pending) {
+		/*
+		 * This will cause message queue rescan to
+		 * possibly deliver another msg from the
+		 * hypervisor
+		 */
+		wrmsrl(HV_X64_MSR_EOM, 0);
+	}
+}
+
+#define hv_init_timer(timer, tick) wrmsrl(timer, tick)
+#define hv_init_timer_config(config, val) wrmsrl(config, val)
+
+#define hv_get_simp(val) rdmsrl(HV_X64_MSR_SIMP, val)
+#define hv_set_simp(val) wrmsrl(HV_X64_MSR_SIMP, val)
+
+#define hv_get_siefp(val) rdmsrl(HV_X64_MSR_SIEFP, val)
+#define hv_set_siefp(val) wrmsrl(HV_X64_MSR_SIEFP, val)
+
+#define hv_get_synic_state(val) rdmsrl(HV_X64_MSR_SCONTROL, val)
+#define hv_set_synic_state(val) wrmsrl(HV_X64_MSR_SCONTROL, val)
+
+#define hv_get_vp_index(index) rdmsrl(HV_X64_MSR_VP_INDEX, index)
+
+#define hv_get_synint_state(int_num, val) rdmsrl(int_num, val)
+#define hv_set_synint_state(int_num, val) wrmsrl(int_num, val)
+
+void hyperv_callback_vector(void);
+#ifdef CONFIG_TRACING
+#define trace_hyperv_callback_vector hyperv_callback_vector
+#endif
+void hyperv_vector_handler(struct pt_regs *regs);
+void hv_setup_vmbus_irq(void (*handler)(void));
+void hv_remove_vmbus_irq(void);
+
+void hv_setup_kexec_handler(void (*handler)(void));
+void hv_remove_kexec_handler(void);
+void hv_setup_crash_handler(void (*handler)(struct pt_regs *regs));
+void hv_remove_crash_handler(void);
+
+#if IS_ENABLED(CONFIG_HYPERV)
+extern struct clocksource *hyperv_cs;
+extern void *hv_hypercall_pg;
+
+static inline u64 hv_do_hypercall(u64 control, void *input, void *output)
+{
+	u64 input_address = input ? virt_to_phys(input) : 0;
+	u64 output_address = output ? virt_to_phys(output) : 0;
+	u64 hv_status;
+	register void *__sp asm(_ASM_SP);
+
+#ifdef CONFIG_X86_64
+	if (!hv_hypercall_pg)
+		return U64_MAX;
+
+	__asm__ __volatile__("mov %4, %%r8\n"
+			     CALL_NOSPEC
+			     : "=a" (hv_status), "+r" (__sp),
+			       "+c" (control), "+d" (input_address)
+			     :  "r" (output_address),
+				THUNK_TARGET(hv_hypercall_pg)
+			     : "cc", "memory", "r8", "r9", "r10", "r11");
+#else
+	u32 input_address_hi = upper_32_bits(input_address);
+	u32 input_address_lo = lower_32_bits(input_address);
+	u32 output_address_hi = upper_32_bits(output_address);
+	u32 output_address_lo = lower_32_bits(output_address);
+
+	if (!hv_hypercall_pg)
+		return U64_MAX;
+
+	__asm__ __volatile__(CALL_NOSPEC
+			     : "=A" (hv_status),
+			       "+c" (input_address_lo), "+r" (__sp)
+			     : "A" (control),
+			       "b" (input_address_hi),
+			       "D"(output_address_hi), "S"(output_address_lo),
+			       THUNK_TARGET(hv_hypercall_pg)
+			     : "cc", "memory");
+#endif /* !x86_64 */
+	return hv_status;
+}
+
+#define HV_HYPERCALL_RESULT_MASK	GENMASK_ULL(15, 0)
+#define HV_HYPERCALL_FAST_BIT		BIT(16)
+#define HV_HYPERCALL_VARHEAD_OFFSET	17
+#define HV_HYPERCALL_REP_COMP_OFFSET	32
+#define HV_HYPERCALL_REP_COMP_MASK	GENMASK_ULL(43, 32)
+#define HV_HYPERCALL_REP_START_OFFSET	48
+#define HV_HYPERCALL_REP_START_MASK	GENMASK_ULL(59, 48)
+
+/* Fast hypercall with 8 bytes of input and no output */
+static inline u64 hv_do_fast_hypercall8(u16 code, u64 input1)
+{
+	u64 hv_status, control = (u64)code | HV_HYPERCALL_FAST_BIT;
+	register void *__sp asm(_ASM_SP);
+
+#ifdef CONFIG_X86_64
+	{
+		__asm__ __volatile__(CALL_NOSPEC
+				     : "=a" (hv_status), "+r" (__sp),
+				       "+c" (control), "+d" (input1)
+				     : THUNK_TARGET(hv_hypercall_pg)
+				     : "cc", "r8", "r9", "r10", "r11");
+	}
+#else
+	{
+		u32 input1_hi = upper_32_bits(input1);
+		u32 input1_lo = lower_32_bits(input1);
+
+		__asm__ __volatile__ (CALL_NOSPEC
+				      : "=A"(hv_status),
+					"+c"(input1_lo),
+					"+r"(__sp)
+				      :	"A" (control),
+					"b" (input1_hi),
+					THUNK_TARGET(hv_hypercall_pg)
+				      : "cc", "edi", "esi");
+	}
+#endif
+		return hv_status;
+}
+
+/*
+ * Rep hypercalls. Callers of this functions are supposed to ensure that
+ * rep_count and varhead_size comply with Hyper-V hypercall definition.
+ */
+static inline u64 hv_do_rep_hypercall(u16 code, u16 rep_count, u16 varhead_size,
+				      void *input, void *output)
+{
+	u64 control = code;
+	u64 status;
+	u16 rep_comp;
+
+	control |= (u64)varhead_size << HV_HYPERCALL_VARHEAD_OFFSET;
+	control |= (u64)rep_count << HV_HYPERCALL_REP_COMP_OFFSET;
+
+	do {
+		status = hv_do_hypercall(control, input, output);
+		if ((status & HV_HYPERCALL_RESULT_MASK) != HV_STATUS_SUCCESS)
+			return status;
+
+		/* Bits 32-43 of status have 'Reps completed' data. */
+		rep_comp = (status & HV_HYPERCALL_REP_COMP_MASK) >>
+			HV_HYPERCALL_REP_COMP_OFFSET;
+
+		control &= ~HV_HYPERCALL_REP_START_MASK;
+		control |= (u64)rep_comp << HV_HYPERCALL_REP_START_OFFSET;
+
+		touch_nmi_watchdog();
+	} while (rep_comp < rep_count);
+
+	return status;
+}
+
+/*
+ * Hypervisor's notion of virtual processor ID is different from
+ * Linux' notion of CPU ID. This information can only be retrieved
+ * in the context of the calling CPU. Setup a map for easy access
+ * to this information.
+ */
+extern u32 *hv_vp_index;
+extern u32 hv_max_vp_index;
+
+/**
+ * hv_cpu_number_to_vp_number() - Map CPU to VP.
+ * @cpu_number: CPU number in Linux terms
+ *
+ * This function returns the mapping between the Linux processor
+ * number and the hypervisor's virtual processor number, useful
+ * in making hypercalls and such that talk about specific
+ * processors.
+ *
+ * Return: Virtual processor number in Hyper-V terms
+ */
+static inline int hv_cpu_number_to_vp_number(int cpu_number)
+{
+	return hv_vp_index[cpu_number];
+}
+
+void hyperv_init(void);
+void hyperv_setup_mmu_ops(void);
+void hyper_alloc_mmu(void);
+void hyperv_report_panic(struct pt_regs *regs, long err);
+bool hv_is_hyperv_initialized(void);
+void hyperv_cleanup(void);
+#else /* CONFIG_HYPERV */
+static inline void hyperv_init(void) {}
+static inline bool hv_is_hyperv_initialized(void) { return false; }
+static inline void hyperv_cleanup(void) {}
+static inline void hyperv_setup_mmu_ops(void) {}
+#endif /* CONFIG_HYPERV */
+
+#ifdef CONFIG_HYPERV_TSCPAGE
+struct ms_hyperv_tsc_page *hv_get_tsc_page(void);
+static inline u64 hv_read_tsc_page(const struct ms_hyperv_tsc_page *tsc_pg)
+{
+	u64 scale, offset, cur_tsc;
+	u32 sequence;
+
+	/*
+	 * The protocol for reading Hyper-V TSC page is specified in Hypervisor
+	 * Top-Level Functional Specification ver. 3.0 and above. To get the
+	 * reference time we must do the following:
+	 * - READ ReferenceTscSequence
+	 *   A special '0' value indicates the time source is unreliable and we
+	 *   need to use something else. The currently published specification
+	 *   versions (up to 4.0b) contain a mistake and wrongly claim '-1'
+	 *   instead of '0' as the special value, see commit c35b82ef0294.
+	 * - ReferenceTime =
+	 *        ((RDTSC() * ReferenceTscScale) >> 64) + ReferenceTscOffset
+	 * - READ ReferenceTscSequence again. In case its value has changed
+	 *   since our first reading we need to discard ReferenceTime and repeat
+	 *   the whole sequence as the hypervisor was updating the page in
+	 *   between.
+	 */
+	do {
+		sequence = READ_ONCE(tsc_pg->tsc_sequence);
+		if (!sequence)
+			return U64_MAX;
+		/*
+		 * Make sure we read sequence before we read other values from
+		 * TSC page.
+		 */
+		smp_rmb();
+
+		scale = READ_ONCE(tsc_pg->tsc_scale);
+		offset = READ_ONCE(tsc_pg->tsc_offset);
+		cur_tsc = rdtsc_ordered();
+
+		/*
+		 * Make sure we read sequence after we read all other values
+		 * from TSC page.
+		 */
+		smp_rmb();
+
+	} while (READ_ONCE(tsc_pg->tsc_sequence) != sequence);
+
+	return mul_u64_u64_shr(cur_tsc, scale, 64) + offset;
+}
+
+#else
+static inline struct ms_hyperv_tsc_page *hv_get_tsc_page(void)
+{
+	return NULL;
+}
+#endif
+#endif
diff -Naur linux-3.10.0-957.el7.orig/arch/x86/vdso/vclock_gettime.c linux-3.10.0-957.el7.lis/arch/x86/vdso/vclock_gettime.c
--- linux-3.10.0-957.el7.orig/arch/x86/vdso/vclock_gettime.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/arch/x86/vdso/vclock_gettime.c	2018-12-12 00:51:40.670785533 +0000
@@ -23,7 +23,7 @@
 #include <asm/unistd.h>
 #include <asm/io.h>
 #include <asm/pvclock.h>
-#include <asm/mshyperv.h>
+#include "../hyperv/include/asm/mshyperv.h"
 #include <asm/msr.h>
 
 #define gtod (&VVAR(vsyscall_gtod_data))
diff -Naur linux-3.10.0-957.el7.orig/drivers/hid/hid-hyperv.c linux-3.10.0-957.el7.lis/drivers/hid/hid-hyperv.c
--- linux-3.10.0-957.el7.orig/drivers/hid/hid-hyperv.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hid/hid-hyperv.c	2018-12-12 00:51:40.873784242 +0000
@@ -463,6 +463,7 @@
 {
 }
 
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
 static int mousevsc_hid_raw_request(struct hid_device *hid,
 				    unsigned char report_num,
 				    __u8 *buf, size_t len,
@@ -471,6 +472,7 @@
 {
 	return 0;
 }
+#endif
 
 static struct hid_ll_driver mousevsc_ll_driver = {
 	.parse = mousevsc_hid_parse,
@@ -478,7 +480,9 @@
 	.close = mousevsc_hid_close,
 	.start = mousevsc_hid_start,
 	.stop = mousevsc_hid_stop,
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
 	.raw_request = mousevsc_hid_raw_request,
+#endif
 };
 
 static struct hid_driver mousevsc_hid_driver;
@@ -611,5 +615,7 @@
 }
 
 MODULE_LICENSE("GPL");
+MODULE_VERSION(HV_DRV_VERSION);
+
 module_init(mousevsc_init);
 module_exit(mousevsc_exit);
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/channel.c linux-3.10.0-957.el7.lis/drivers/hv/channel.c
--- linux-3.10.0-957.el7.orig/drivers/hv/channel.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/channel.c	2018-12-12 00:51:40.750785024 +0000
@@ -96,11 +96,11 @@
 
 	/* Allocate the ring buffer */
 	page = alloc_pages_node(cpu_to_node(newchannel->target_cpu),
-				GFP_KERNEL|__GFP_ZERO,
-				get_order(send_ringbuffer_size +
-				recv_ringbuffer_size));
+                               GFP_KERNEL|__GFP_ZERO,
+                               get_order(send_ringbuffer_size +
+                               recv_ringbuffer_size));
 
-	if (!page)
+        if (!page)
 		page = alloc_pages(GFP_KERNEL|__GFP_ZERO,
 				   get_order(send_ringbuffer_size +
 					     recv_ringbuffer_size));
@@ -196,6 +196,7 @@
 
 	wait_for_completion(&open_info->waitevent);
 
+
 	spin_lock_irqsave(&vmbus_connection.channelmsg_lock, flags);
 	list_del(&open_info->msglistentry);
 	spin_unlock_irqrestore(&vmbus_connection.channelmsg_lock, flags);
@@ -209,7 +210,7 @@
 		err = -EAGAIN;
 		goto error_free_gpadl;
 	}
-
+ 
 	newchannel->state = CHANNEL_OPENED_STATE;
 	kfree(open_info);
 	return 0;
@@ -246,10 +247,9 @@
 	conn_msg.host_service_id = *shv_host_servie_id;
 
 	ret = vmbus_post_msg(&conn_msg, sizeof(conn_msg), true);
-
 	trace_vmbus_send_tl_connect_request(&conn_msg, ret);
-
 	return ret;
+
 }
 EXPORT_SYMBOL_GPL(vmbus_send_tl_connect_request);
 
@@ -443,7 +443,7 @@
 	ret = vmbus_post_msg(gpadlmsg, msginfo->msgsize -
 			     sizeof(*msginfo), true);
 
-	trace_vmbus_establish_gpadl_header(gpadlmsg, ret);
+	trace_vmbus_establish_gpadl_header(gpadlmsg, ret);	
 
 	if (ret != 0)
 		goto cleanup;
@@ -460,15 +460,20 @@
 		ret = vmbus_post_msg(gpadl_body,
 				     submsginfo->msgsize - sizeof(*submsginfo),
 				     true);
-
-		trace_vmbus_establish_gpadl_body(gpadl_body, ret);
-
 		if (ret != 0)
 			goto cleanup;
 
 	}
 	wait_for_completion(&msginfo->waitevent);
 
+	if (msginfo->response.gpadl_created.creation_status != 0) {
+		pr_err("Failed to establish GPADL: err = 0x%x\n",
+		       msginfo->response.gpadl_created.creation_status);
+
+		ret = -EDQUOT;
+		goto cleanup;
+	}
+
 	if (channel->rescind) {
 		ret = -ENODEV;
 		goto cleanup;
@@ -534,6 +539,7 @@
 	wait_for_completion(&info->waitevent);
 
 post_msg_err:
+
 	/*
 	 * If the channel has been rescinded;
 	 * we will be awakened by the rescind
@@ -570,12 +576,11 @@
 	 * could be freeing the ring_buffer pages, so here we must stop it
 	 * first.
 	 */
-	tasklet_disable(&channel->callback_event);
 
 	/*
 	 * In case a device driver's probe() fails (e.g.,
 	 * util_probe() -> vmbus_open() returns -ENOMEM) and the device is
-	 * rescinded later (e.g., we dynamically disble an Integrated Service
+	 * rescinded later (e.g., we dynamically disable Integrated Service
 	 * in Hyper-V Manager), the driver's remove() invokes vmbus_close():
 	 * here we should skip most of the below cleanup work.
 	 */
@@ -605,7 +610,7 @@
 
 	ret = vmbus_post_msg(msg, sizeof(struct vmbus_channel_close_channel),
 			     true);
-
+	
 	trace_vmbus_close_internal(msg, ret);
 
 	if (ret) {
@@ -621,7 +626,7 @@
 	if (channel->ringbuffer_gpadlhandle) {
 		ret = vmbus_teardown_gpadl(channel,
 					   channel->ringbuffer_gpadlhandle);
-		if (ret) {
+                if (ret && !channel->rescind){
 			pr_err("Close failed: teardown gpadl return %d\n", ret);
 			/*
 			 * If we failed to teardown gpadl,
@@ -676,7 +681,7 @@
 			vmbus_close_internal(cur_channel);
 		}
 		mutex_unlock(&vmbus_connection.channel_mutex);
-	}
+ 	}
 	/*
 	 * Now close the primary.
 	 */
@@ -709,8 +714,6 @@
 	u32 packetlen_aligned = ALIGN(packetlen, sizeof(u64));
 	struct kvec bufferlist[3];
 	u64 aligned_data = 0;
-	int num_vecs = ((bufferlen != 0) ? 3 : 1);
-
 
 	/* Setup the descriptor */
 	desc.type = type; /* VmbusPacketTypeDataInBand; */
@@ -727,7 +730,7 @@
 	bufferlist[2].iov_base = &aligned_data;
 	bufferlist[2].iov_len = (packetlen_aligned - packetlen);
 
-	return hv_ringbuffer_write(channel, bufferlist, num_vecs);
+	return hv_ringbuffer_write(channel, bufferlist, 3);
 }
 EXPORT_SYMBOL(vmbus_sendpacket);
 
@@ -790,6 +793,7 @@
 }
 EXPORT_SYMBOL_GPL(vmbus_sendpacket_pagebuffer);
 
+
 /*
  * vmbus_sendpacket_multipagebuffer - Send a multi-page buffer packet
  * using a GPADL Direct packet type.
@@ -848,7 +852,6 @@
 {
 	return hv_ringbuffer_read(channel, buffer, bufferlen,
 				  buffer_actual_len, requestid, raw);
-
 }
 
 int vmbus_recvpacket(struct vmbus_channel *channel, void *buffer,
@@ -871,3 +874,4 @@
 				  buffer_actual_len, requestid, true);
 }
 EXPORT_SYMBOL_GPL(vmbus_recvpacket_raw);
+
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/channel_mgmt.c linux-3.10.0-957.el7.lis/drivers/hv/channel_mgmt.c
--- linux-3.10.0-957.el7.orig/drivers/hv/channel_mgmt.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/channel_mgmt.c	2018-12-12 00:51:40.755784992 +0000
@@ -29,6 +29,7 @@
 #include <linux/list.h>
 #include <linux/module.h>
 #include <linux/completion.h>
+#include <linux/topology.h>
 #include <linux/delay.h>
 #include <linux/hyperv.h>
 #include <asm/mshyperv.h>
@@ -71,7 +72,7 @@
 	/* PCIE */
 	{ .dev_type = HV_PCIE,
 	  HV_PCIE_GUID,
-	  .perf_device = false,
+	  .perf_device = true,
 	},
 
 	/* Synthetic Frame Buffer */
@@ -190,13 +191,19 @@
 		return HV_UNKNOWN;
 
 	for (i = HV_IDE; i < HV_UNKNOWN; i++) {
-		if (!uuid_le_cmp(*guid, vmbus_devs[i].guid))
+		/* deviation from upstream - NHM */
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+		if (!memcmp(guid->b, vmbus_devs[i].guid, sizeof(uuid_le)))
+#else
+	if (!uuid_le_cmp(*guid, vmbus_devs[i].guid))
+#endif
 			return i;
 	}
 	pr_info("Unknown GUID: %pUl\n", guid);
 	return i;
 }
 
+
 /**
  * vmbus_prep_negotiate_resp() - Create default response for Hyper-V Negotiate message
  * @icmsghdrp: Pointer to msg header structure
@@ -321,6 +328,7 @@
 
 EXPORT_SYMBOL_GPL(vmbus_prep_negotiate_resp);
 
+
 /*
  * alloc_channel - Allocate and initialize a vmbus channel object
  */
@@ -370,7 +378,6 @@
 	list_del_rcu(&channel->percpu_list);
 }
 
-
 static void vmbus_release_relid(u32 relid)
 {
 	struct vmbus_channel_relid_released msg;
@@ -379,6 +386,7 @@
 	memset(&msg, 0, sizeof(struct vmbus_channel_relid_released));
 	msg.child_relid = relid;
 	msg.header.msgtype = CHANNELMSG_RELID_RELEASED;
+	
 	ret = vmbus_post_msg(&msg, sizeof(struct vmbus_channel_relid_released),
 			     true);
 
@@ -409,6 +417,7 @@
 		put_cpu();
 	}
 
+
 	if (channel->primary_channel == NULL) {
 		list_del(&channel->listentry);
 
@@ -417,7 +426,6 @@
 		primary_channel = channel->primary_channel;
 		spin_lock_irqsave(&primary_channel->lock, flags);
 		list_del(&channel->sc_list);
-		primary_channel->num_sc--;
 		spin_unlock_irqrestore(&primary_channel->lock, flags);
 	}
 
@@ -447,61 +455,17 @@
 	}
 }
 
-/*
- * vmbus_process_offer - Process the offer by creating a channel/device
- * associated with this offer
- */
-static void vmbus_process_offer(struct vmbus_channel *newchannel)
+
+/* Note: the function can run concurrently for primary/sub channels. */
+static void vmbus_add_channel_work(struct work_struct *work)
 {
-	struct vmbus_channel *channel;
-	bool fnew = true;
+	struct vmbus_channel *newchannel =
+		container_of(work, struct vmbus_channel, add_channel_work);
+	struct vmbus_channel *primary_channel = newchannel->primary_channel;
 	unsigned long flags;
 	u16 dev_type;
 	int ret;
 
-	/* Make sure this is a new offer */
-	mutex_lock(&vmbus_connection.channel_mutex);
-
-	/*
-	 * Now that we have acquired the channel_mutex,
-	 * we can release the potentially racing rescind thread.
-	 */
-	atomic_dec(&vmbus_connection.offer_in_progress);
-
-	list_for_each_entry(channel, &vmbus_connection.chn_list, listentry) {
-		if (!uuid_le_cmp(channel->offermsg.offer.if_type,
-			newchannel->offermsg.offer.if_type) &&
-			!uuid_le_cmp(channel->offermsg.offer.if_instance,
-				newchannel->offermsg.offer.if_instance)) {
-			fnew = false;
-			break;
-		}
-	}
-
-	if (fnew)
-		list_add_tail(&newchannel->listentry,
-			      &vmbus_connection.chn_list);
-
-	mutex_unlock(&vmbus_connection.channel_mutex);
-
-	if (!fnew) {
-		/*
-		 * Check to see if this is a sub-channel.
-		 */
-		if (newchannel->offermsg.offer.sub_channel_index != 0) {
-			/*
-			 * Process the sub-channel.
-			 */
-			newchannel->primary_channel = channel;
-			spin_lock_irqsave(&channel->lock, flags);
-			list_add_tail(&newchannel->sc_list, &channel->sc_list);
-			channel->num_sc++;
-			spin_unlock_irqrestore(&channel->lock, flags);
-		} else {
-			goto err_free_chan;
-		}
-	}
-
 	dev_type = hv_get_dev_type(newchannel);
 
 	init_vp_index(newchannel, dev_type);
@@ -514,34 +478,32 @@
 	} else {
 		percpu_channel_enq(newchannel);
 		put_cpu();
+
 	}
 
 	/*
 	 * This state is used to indicate a successful open
 	 * so that when we do close the channel normally, we
-	 * can cleanup properly
+	 * can cleanup properly.
 	 */
 	newchannel->state = CHANNEL_OPEN_STATE;
 
-	if (!fnew) {
-		struct hv_device *dev
-			= newchannel->primary_channel->device_obj;
-
-		if (vmbus_add_channel_kobj(dev, newchannel)) {
-			atomic_dec(&vmbus_connection.offer_in_progress);
-			goto err_free_chan;
-		}
+	if (primary_channel != NULL) {
+		/* newchannel is a sub-channel. */
+		struct hv_device *dev = primary_channel->device_obj;
+
+		if (vmbus_add_channel_kobj(dev, newchannel))
+			goto err_deq_chan;
+
+		if (primary_channel->sc_creation_callback != NULL)
+			primary_channel->sc_creation_callback(newchannel);
 
-		if (channel->sc_creation_callback != NULL)
-			channel->sc_creation_callback(newchannel);
 		newchannel->probe_done = true;
 		return;
 	}
 
 	/*
-	 * Start the process of binding this offer to the driver
-	 * We need to set the DeviceObject field before calling
-	 * vmbus_child_dev_add()
+	 * Start the process of binding the primary channel to the driver
 	 */
 	newchannel->device_obj = vmbus_device_create(
 		&newchannel->offermsg.offer.if_type,
@@ -570,13 +532,28 @@
 
 err_deq_chan:
 	mutex_lock(&vmbus_connection.channel_mutex);
-	list_del(&newchannel->listentry);
+
+	/*
+	 * We need to set the flag, otherwise
+	 * vmbus_onoffer_rescind() can be blocked.
+	 */
+	newchannel->probe_done = true;
+
+	if (primary_channel == NULL) {
+		list_del(&newchannel->listentry);
+	} else {
+		spin_lock_irqsave(&primary_channel->lock, flags);
+		list_del(&newchannel->sc_list);
+		spin_unlock_irqrestore(&primary_channel->lock, flags);
+	}
+
 	mutex_unlock(&vmbus_connection.channel_mutex);
 
 	if (newchannel->target_cpu != get_cpu()) {
 		put_cpu();
 		smp_call_function_single(newchannel->target_cpu,
-					 percpu_channel_deq, newchannel, true);
+					 percpu_channel_deq,
+					 newchannel, true);
 	} else {
 		percpu_channel_deq(newchannel);
 		put_cpu();
@@ -584,14 +561,104 @@
 
 	vmbus_release_relid(newchannel->offermsg.child_relid);
 
-err_free_chan:
 	free_channel(newchannel);
 }
 
 /*
+ * vmbus_process_offer - Process the offer by creating a channel/device
+ * associated with this offer
+ */
+static void vmbus_process_offer(struct vmbus_channel *newchannel)
+{
+	struct vmbus_channel *channel;
+	struct workqueue_struct *wq;
+	unsigned long flags;
+	bool fnew = true;
+
+	mutex_lock(&vmbus_connection.channel_mutex);
+
+	/*
+	 * Now that we have acquired the channel_mutex,
+	 * we can release the potentially racing rescind thread.
+	 */
+	atomic_dec(&vmbus_connection.offer_in_progress);
+
+	list_for_each_entry(channel, &vmbus_connection.chn_list, listentry) {
+		if (!uuid_le_cmp(channel->offermsg.offer.if_type,
+				 newchannel->offermsg.offer.if_type) &&
+		    !uuid_le_cmp(channel->offermsg.offer.if_instance,
+				 newchannel->offermsg.offer.if_instance)) {
+			fnew = false;
+			break;
+		}
+	}
+
+	if (fnew)
+		list_add_tail(&newchannel->listentry,
+			      &vmbus_connection.chn_list);
+	else {
+		/*
+		 * Check to see if this is a valid sub-channel.
+		 */
+		if (newchannel->offermsg.offer.sub_channel_index == 0) {
+			mutex_unlock(&vmbus_connection.channel_mutex);
+			/*
+			 * Don't call free_channel(), because newchannel->kobj
+			 * is not initialized yet.
+			 */
+			kfree(newchannel);
+			WARN_ON_ONCE(1);
+			return;
+		}
+		/*
+		 * Process the sub-channel.
+		 */
+		newchannel->primary_channel = channel;
+		spin_lock_irqsave(&channel->lock, flags);
+		list_add_tail(&newchannel->sc_list, &channel->sc_list);
+		spin_unlock_irqrestore(&channel->lock, flags);
+	}
+
+	mutex_unlock(&vmbus_connection.channel_mutex);
+
+	/*
+	 * vmbus_process_offer() mustn't call channel->sc_creation_callback()
+	 * directly for sub-channels, because sc_creation_callback() ->
+	 * vmbus_open() may never get the host's response to the
+	 * OPEN_CHANNEL message (the host may rescind a channel at any time,
+	 * e.g. in the case of hot removing a NIC), and vmbus_onoffer_rescind()
+	 * may not wake up the vmbus_open() as it's blocked due to a non-zero
+	 * vmbus_connection.offer_in_progress, and finally we have a deadlock.
+	 *
+	 * The above is also true for primary channels, if the related device
+	 * drivers use sync probing mode by default.
+	 *
+	 * And, usually the handling of primary channels and sub-channels can
+	 * depend on each other, so we should offload them to different
+	 * workqueues to avoid possible deadlock, e.g. in sync-probing mode,
+	 * NIC1's netvsc_subchan_work() can race with NIC2's netvsc_probe() ->
+	 * rtnl_lock(), and causes deadlock: the former gets the rtnl_lock
+	 * and waits for all the sub-channels to appear, but the latter
+	 * can't get the rtnl_lock and this blocks the handling of
+	 * sub-channels.
+	 */
+	INIT_WORK(&newchannel->add_channel_work, vmbus_add_channel_work);
+	wq = fnew ? vmbus_connection.handle_primary_chan_wq :
+		    vmbus_connection.handle_sub_chan_wq;
+	queue_work(wq, &newchannel->add_channel_work);
+}
+
+/*
  * We use this state to statically distribute the channel interrupt load.
  */
 static int next_numa_node_id;
+/*
+ * init_vp_index() accesses global variables like next_numa_node_id, and
+ * it can run concurrently for primary channels and sub-channels: see
+ * vmbus_process_offer(), so we need the lock to protect the global
+ * variables.
+ */
+static DEFINE_SPINLOCK(bind_channel_to_cpu_lock);
 
 /*
  * Starting with Win8, we can statically distribute the incoming
@@ -611,6 +678,11 @@
 	struct cpumask available_mask;
 	struct cpumask *alloced_mask;
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+	struct cpumask *cpu_sibling_mask;
+	struct cpumask *cpu_thread_tmp_mask;
+#endif
+
 	if ((vmbus_proto_version == VERSION_WS2008) ||
 	    (vmbus_proto_version == VERSION_WIN7) || (!perf_chn)) {
 		/*
@@ -625,38 +697,42 @@
 		return;
 	}
 
+	spin_lock(&bind_channel_to_cpu_lock);
+
 	/*
 	 * Based on the channel affinity policy, we will assign the NUMA
 	 * nodes.
 	 */
 
 	if ((channel->affinity_policy == HV_BALANCED) || (!primary)) {
-		while (true) {
-			next_node = next_numa_node_id++;
+               while (true) {
+                       next_node = next_numa_node_id++;
 			if (next_node == nr_node_ids) {
-				next_node = next_numa_node_id = 0;
+                               next_node = next_numa_node_id = 0;
 				continue;
 			}
-			if (cpumask_empty(cpumask_of_node(next_node)))
-				continue;
-			break;
-		}
-		channel->numa_node = next_node;
-		primary = channel;
-	}
+                       if (cpumask_empty(cpumask_of_node(next_node)))
+                               continue;
+                       break;
+               }
+               channel->numa_node = next_node;
+               primary = channel;
+        }
+
 	alloced_mask = &hv_context.hv_numa_map[primary->numa_node];
 
 	if (cpumask_weight(alloced_mask) ==
-	    cpumask_weight(cpumask_of_node(primary->numa_node))) {
-		/*
+           cpumask_weight(cpumask_of_node(primary->numa_node))) {
+
+		/* 
 		 * We have cycled through all the CPUs in the node;
 		 * reset the alloced map.
 		 */
 		cpumask_clear(alloced_mask);
-	}
+        }
 
 	cpumask_xor(&available_mask, alloced_mask,
-		    cpumask_of_node(primary->numa_node));
+                   cpumask_of_node(primary->numa_node));
 
 	cur_cpu = -1;
 
@@ -672,6 +748,18 @@
 			cpumask_clear(&primary->alloced_cpus_in_node);
 	}
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+	cpu_thread_tmp_mask = kzalloc(cpumask_size(), GFP_KERNEL);
+	if (!cpu_thread_tmp_mask) {
+		channel->numa_node = 0;
+		channel->target_cpu = 0;
+		channel->target_vp = hv_cpu_number_to_vp_number(0);
+
+		spin_unlock(&bind_channel_to_cpu_lock);
+		return;
+	}
+#endif
+
 	while (true) {
 		cur_cpu = cpumask_next(cur_cpu, &available_mask);
 		if (cur_cpu >= nr_cpu_ids) {
@@ -681,6 +769,27 @@
 			continue;
 		}
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+		if (affinity_mode == HV_SKIP_HT_CPU) {
+			cpu_sibling_mask = topology_sibling_cpumask(cur_cpu);
+			cpumask_and(cpu_thread_tmp_mask,
+				    cpu_sibling_mask,
+				    &available_mask);
+			if (!cpumask_equal(cpu_thread_tmp_mask, cpu_sibling_mask)) {
+				/*
+				 * NOTE: The thread sibling of this CPU has been
+				 * assigned to a channel.
+				 * We do not assign both Hyper-Threading CPUs of
+				 * the same physical core to vmbus channels.
+				 * So, mark this CPU as occupied too then move to
+				 * next one and try.
+				 */
+				cpumask_set_cpu(cur_cpu, alloced_mask);
+				continue;
+			}
+		}
+#endif
+
 		if (primary->affinity_policy == HV_LOCALIZED) {
 			/*
 			 * NOTE: in the case of sub-channel, we clear the
@@ -706,6 +815,12 @@
 
 	channel->target_cpu = cur_cpu;
 	channel->target_vp = hv_cpu_number_to_vp_number(cur_cpu);
+
+	spin_unlock(&bind_channel_to_cpu_lock);
+
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+	kfree(cpu_thread_tmp_mask);
+#endif
 }
 
 static void vmbus_wait_for_unload(void)
@@ -783,7 +898,7 @@
 void vmbus_initiate_unload(bool crash)
 {
 	struct vmbus_channel_message_header hdr;
-
+	
 	/* Pre-Win2012R2 hosts don't support reconnect */
 	if (vmbus_proto_version < VERSION_WIN8_1)
 		return;
@@ -841,7 +956,6 @@
 	       sizeof(struct vmbus_channel_offer_channel));
 	newchannel->monitor_grp = (u8)offer->monitorid / 32;
 	newchannel->monitor_bit = (u8)offer->monitorid % 32;
-
 	vmbus_process_offer(newchannel);
 }
 
@@ -893,7 +1007,7 @@
 		 */
 		return;
 	}
-
+	
 	/*
 	 * Now wait for offer handling to complete.
 	 */
@@ -915,6 +1029,7 @@
 			channel->chn_rescind_callback(channel);
 			return;
 		}
+
 		/*
 		 * We will have to unregister this device from the
 		 * driver core.
@@ -947,6 +1062,7 @@
 	}
 }
 
+
 void vmbus_hvsock_device_unregister(struct vmbus_channel *channel)
 {
 	BUG_ON(!is_hvsock_channel(channel));
@@ -959,7 +1075,6 @@
 }
 EXPORT_SYMBOL_GPL(vmbus_hvsock_device_unregister);
 
-
 /*
  * vmbus_onoffers_delivered -
  * This is invoked when all offers have been delivered.
@@ -1130,7 +1245,7 @@
 
 	version_response = (struct vmbus_channel_version_response *)hdr;
 
-	trace_vmbus_onversion_response(version_response);
+	trace_vmbus_onversion_response(version_response);	
 
 	spin_lock_irqsave(&vmbus_connection.channelmsg_lock, flags);
 
@@ -1226,6 +1341,7 @@
 
 	msg->msgtype = CHANNELMSG_REQUESTOFFERS;
 
+
 	ret = vmbus_post_msg(msg, sizeof(struct vmbus_channel_message_header),
 			     true);
 
@@ -1243,49 +1359,6 @@
 	return ret;
 }
 
-/*
- * Retrieve the (sub) channel on which to send an outgoing request.
- * When a primary channel has multiple sub-channels, we try to
- * distribute the load equally amongst all available channels.
- */
-struct vmbus_channel *vmbus_get_outgoing_channel(struct vmbus_channel *primary)
-{
-	struct list_head *cur, *tmp;
-	int cur_cpu;
-	struct vmbus_channel *cur_channel;
-	struct vmbus_channel *outgoing_channel = primary;
-	int next_channel;
-	int i = 1;
-
-	if (list_empty(&primary->sc_list))
-		return outgoing_channel;
-
-	next_channel = primary->next_oc++;
-
-	if (next_channel > (primary->num_sc)) {
-		primary->next_oc = 0;
-		return outgoing_channel;
-	}
-
-	cur_cpu = hv_cpu_number_to_vp_number(smp_processor_id());
-	list_for_each_safe(cur, tmp, &primary->sc_list) {
-		cur_channel = list_entry(cur, struct vmbus_channel, sc_list);
-		if (cur_channel->state != CHANNEL_OPENED_STATE)
-			continue;
-
-		if (cur_channel->target_vp == cur_cpu)
-			return cur_channel;
-
-		if (i == next_channel)
-			return cur_channel;
-
-		i++;
-	}
-
-	return outgoing_channel;
-}
-EXPORT_SYMBOL_GPL(vmbus_get_outgoing_channel);
-
 static void invoke_sc_cb(struct vmbus_channel *primary_channel)
 {
 	struct list_head *cur, *tmp;
@@ -1328,8 +1401,8 @@
 EXPORT_SYMBOL_GPL(vmbus_are_subchannels_present);
 
 void vmbus_set_chn_rescind_callback(struct vmbus_channel *channel,
-		void (*chn_rescind_cb)(struct vmbus_channel *))
+		void (*chn_rescind_cb) (struct vmbus_channel *))
 {
-	channel->chn_rescind_callback = chn_rescind_cb;
+	channel->chn_rescind_callback =  chn_rescind_cb;
 }
 EXPORT_SYMBOL_GPL(vmbus_set_chn_rescind_callback);
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/connection.c linux-3.10.0-957.el7.lis/drivers/hv/connection.c
--- linux-3.10.0-957.el7.orig/drivers/hv/connection.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/connection.c	2018-12-12 00:51:40.758784973 +0000
@@ -33,7 +33,6 @@
 #include <linux/export.h>
 #include <asm/hyperv.h>
 #include <asm/mshyperv.h>
-
 #include "hyperv_vmbus.h"
 
 
@@ -41,6 +40,7 @@
 	.conn_state		= DISCONNECTED,
 	.next_gpadl_handle	= ATOMIC_INIT(0xE1E10),
 };
+
 EXPORT_SYMBOL_GPL(vmbus_connection);
 
 /*
@@ -86,23 +86,23 @@
 	msg->interrupt_page = virt_to_phys(vmbus_connection.int_page);
 	msg->monitor_page1 = virt_to_phys(vmbus_connection.monitor_pages[0]);
 	msg->monitor_page2 = virt_to_phys(vmbus_connection.monitor_pages[1]);
-	/*
-	 * We want all channel messages to be delivered on CPU 0.
-	 * This has been the behavior pre-win8. This is not
-	 * perf issue and having all channel messages delivered on CPU 0
-	 * would be ok.
-	 * For post win8 hosts, we support receiving channel messagges on
-	 * all the CPUs. This is needed for kexec to work correctly where
-	 * the CPU attempting to connect may not be CPU 0.
-	 */
-	if (version >= VERSION_WIN8_1) {
+
+        /*
+         * For Win8 and below, we want all channel messages to be delivered
+         * on CPU 0. This is not a perf issue and having all channel messages
+         * delivered on CPU 0 would be OK.
+         * For post win8 hosts, we support receiving channel messages on
+         * all the CPUs. This is needed for kexec to work correctly where
+         * the CPU attempting to connect may not be CPU 0.
+         */
+        if (version >= VERSION_WIN8_1) {
 		msg->target_vcpu =
 			hv_cpu_number_to_vp_number(smp_processor_id());
 		vmbus_connection.connect_cpu = smp_processor_id();
-	} else {
-		msg->target_vcpu = 0;
-		vmbus_connection.connect_cpu = 0;
-	}
+		} else {
+                msg->target_vcpu = 0;
+				vmbus_connection.connect_cpu = 0;
+		}
 
 	/*
 	 * Add to list before we send the request since we may
@@ -117,7 +117,7 @@
 	ret = vmbus_post_msg(msg,
 			     sizeof(struct vmbus_channel_initiate_contact),
 			     true);
-
+	
 	trace_vmbus_negotiate_version(msg, ret);
 
 	if (ret != 0) {
@@ -162,6 +162,20 @@
 		goto cleanup;
 	}
 
+	vmbus_connection.handle_primary_chan_wq =
+		create_workqueue("hv_pri_chan");
+	if (!vmbus_connection.handle_primary_chan_wq) {
+		ret = -ENOMEM;
+		goto cleanup;
+	}
+
+	vmbus_connection.handle_sub_chan_wq =
+		create_workqueue("hv_sub_chan");
+	if (!vmbus_connection.handle_sub_chan_wq) {
+		ret = -ENOMEM;
+		goto cleanup;
+	}
+
 	INIT_LIST_HEAD(&vmbus_connection.chn_msg_list);
 	spin_lock_init(&vmbus_connection.channelmsg_lock);
 
@@ -230,13 +244,14 @@
 	vmbus_proto_version = version;
 	pr_info("Vmbus version:%d.%d\n",
 		version >> 16, version & 0xFFFF);
+	pr_info("Vmbus LIS version: %s\n",
+                HV_DRV_VERSION);
 
 	kfree(msginfo);
 	return 0;
 
 cleanup:
 	pr_err("Unable to connect to host\n");
-
 	vmbus_connection.conn_state = DISCONNECTED;
 	vmbus_disconnect();
 
@@ -252,10 +267,14 @@
 	 */
 	vmbus_initiate_unload(false);
 
-	if (vmbus_connection.work_queue) {
-		drain_workqueue(vmbus_connection.work_queue);
+	if (vmbus_connection.handle_sub_chan_wq)
+		destroy_workqueue(vmbus_connection.handle_sub_chan_wq);
+
+	if (vmbus_connection.handle_primary_chan_wq)
+		destroy_workqueue(vmbus_connection.handle_primary_chan_wq);
+
+	if (vmbus_connection.work_queue)
 		destroy_workqueue(vmbus_connection.work_queue);
-	}
 
 	if (vmbus_connection.int_page) {
 		free_pages((unsigned long)vmbus_connection.int_page, 0);
@@ -330,7 +349,8 @@
 		/* A channel once created is persistent even when
 		 * there is no driver handling the device. An
 		 * unloading driver sets the onchannel_callback to NULL.
-		 */
+ 		 */
+
 		callback_fn = READ_ONCE(channel->onchannel_callback);
 		if (unlikely(callback_fn == NULL))
 			return;
@@ -365,8 +385,8 @@
 
 	/*
 	 * hv_post_message() can have transient failures because of
-	 * insufficient resources. Retry the operation a couple of
-	 * times before giving up.
+	 * insufficient resources. Host guarantees it will eventually
+	 * succeed.
 	 */
 	while (retries < 100) {
 		ret = hv_post_message(conn_id, 1, buffer, buflen);
@@ -414,8 +434,6 @@
 	if (!channel->is_dedicated_interrupt)
 		vmbus_send_interrupt(child_relid);
 
-	++channel->sig_events;
-
 	hv_do_fast_hypercall8(HVCALL_SIGNAL_EVENT, channel->sig_event);
 }
 EXPORT_SYMBOL_GPL(vmbus_set_event);
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hv_balloon.c linux-3.10.0-957.el7.lis/drivers/hv/hv_balloon.c
--- linux-3.10.0-957.el7.orig/drivers/hv/hv_balloon.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hv_balloon.c	2018-12-12 00:51:40.765784929 +0000
@@ -34,9 +34,6 @@
 
 #include <linux/hyperv.h>
 
-#define CREATE_TRACE_POINTS
-#include "hv_trace_balloon.h"
-
 /*
  * We begin with definitions supporting the Dynamic Memory protocol
  * with the host.
@@ -732,7 +729,7 @@
 		spin_unlock_irqrestore(&dm_device.ha_lock, flags);
 
 		init_completion(&dm_device.ol_waitevent);
-		dm_device.ha_waiting = true; /* memhp_auto_online is missing in RHEL */
+		dm_device.ha_waiting = !memhp_auto_online;
 
 		nid = memory_add_physaddr_to_nid(PFN_PHYS(start_pfn));
 		ret = add_memory(nid, PFN_PHYS((start_pfn)),
@@ -1162,9 +1159,6 @@
 		 dm->num_pages_added - dm->num_pages_onlined : 0) +
 		compute_balloon_floor();
 
-	trace_balloon_status(status.num_avail, status.num_committed,
-			     vm_memory_committed(), dm->num_pages_ballooned,
-			     dm->num_pages_added, dm->num_pages_onlined);
 	/*
 	 * If our transaction ID is no longer current, just don't
 	 * send the status. This can happen if we were interrupted
@@ -1239,8 +1233,10 @@
 		 * can free them in any order we get.
 		 */
 
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,4))
 		if (alloc_unit != 1)
 			split_page(pg, get_order(alloc_unit << PAGE_SHIFT));
+#endif
 
 		bl_resp->range_count++;
 		bl_resp->range_array[i].finfo.start_page =
@@ -1272,7 +1268,11 @@
 	 * We will attempt 2M allocations. However, if we fail to
 	 * allocate 2M chunks, we will go back to 4k allocations.
 	 */
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,4))
 	alloc_unit = 512;
+#else
+	alloc_unit = 1;
+#endif
 
 	avail_pages = si_mem_available();
 	floor = compute_balloon_floor();
@@ -1575,6 +1575,11 @@
 #endif
 
 	/*
+	 * This is missing from upstream.
+	 */
+	last_post_time = jiffies;
+
+	/*
 	 * First allocate a send buffer.
 	 */
 
@@ -1777,3 +1782,5 @@
 
 MODULE_DESCRIPTION("Hyper-V Balloon");
 MODULE_LICENSE("GPL");
+MODULE_VERSION(HV_DRV_VERSION);
+MODULE_ALIAS("vmbus:dc7450528589e2468057a307dc18a502");
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hv.c linux-3.10.0-957.el7.lis/drivers/hv/hv.c
--- linux-3.10.0-957.el7.orig/drivers/hv/hv.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hv.c	2018-12-12 00:51:40.769784904 +0000
@@ -32,6 +32,8 @@
 #include <asm/hyperv.h>
 #include <asm/mshyperv.h>
 #include "hyperv_vmbus.h"
+#include <asm/msr.h>
+
 
 /* The one and only */
 struct hv_context hv_context = {
@@ -49,6 +51,22 @@
  */
 int hv_init(void)
 {
+	/*
+	 * This initialization is normally done at
+	 * early boot time in the upstream kernel.
+	 *
+	 * Since we can't change the kernel bootup behavior,
+	 * we do this at module load time.
+	 */
+	hyperv_init();
+
+	memset(hv_context.clk_evt, 0, sizeof(void *) * NR_CPUS);
+
+	hv_print_host_info();
+
+	if (!hv_is_hypercall_page_setup())
+		return -ENOTSUPP;
+
 	hv_context.cpu_context = alloc_percpu(struct hv_per_cpu_context);
 	if (!hv_context.cpu_context)
 		return -ENOMEM;
@@ -81,7 +99,7 @@
 	memcpy((void *)aligned_msg->payload, payload, payload_size);
 
 	status = hv_do_hypercall(HVCALL_POST_MESSAGE, aligned_msg, NULL);
-
+	
 	/* Preemption must remain disabled until after the hypercall
 	 * so some other thread can't get scheduled onto this cpu and
 	 * corrupt the per-cpu post_msg_page
@@ -137,6 +155,7 @@
 	dev->features = CLOCK_EVT_FEAT_ONESHOT;
 	dev->cpumask = cpumask_of(cpu);
 	dev->rating = 1000;
+
 	/*
 	 * Avoid settint dev->owner = THIS_MODULE deliberately as doing so will
 	 * result in clockevents_config_and_register() taking additional
@@ -167,16 +186,17 @@
 		tasklet_init(&hv_cpu->msg_dpc,
 			     vmbus_on_msg_dpc, (unsigned long) hv_cpu);
 
-		hv_cpu->clk_evt = kzalloc(sizeof(struct clock_event_device),
-					  GFP_KERNEL);
-		if (hv_cpu->clk_evt == NULL) {
+		hv_context.clk_evt[cpu] = kzalloc(sizeof(struct clock_event_device),
+						  GFP_ATOMIC);
+		if (hv_context.clk_evt[cpu] == NULL) {
 			pr_err("Unable to allocate clock event device\n");
 			goto err;
 		}
-		hv_init_clockevent_device(hv_cpu->clk_evt, cpu);
+		hv_init_clockevent_device(hv_context.clk_evt[cpu], cpu);
 
 		hv_cpu->synic_message_page =
 			(void *)get_zeroed_page(GFP_ATOMIC);
+
 		if (hv_cpu->synic_message_page == NULL) {
 			pr_err("Unable to allocate SYNIC message page\n");
 			goto err;
@@ -202,6 +222,7 @@
 	return -ENOMEM;
 }
 
+
 void hv_synic_free(void)
 {
 	int cpu;
@@ -216,41 +237,92 @@
 			free_page((unsigned long)hv_cpu->synic_message_page);
 		if (hv_cpu->post_msg_page)
 			free_page((unsigned long)hv_cpu->post_msg_page);
+		if (hv_context.clk_evt[cpu])
+			kfree(hv_context.clk_evt[cpu]);
 	}
 
 	kfree(hv_context.hv_numa_map);
 }
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))  
 void hv_clockevents_bind(int cpu)
 {
-	struct hv_per_cpu_context *hv_cpu
-		= per_cpu_ptr(hv_context.cpu_context, cpu);
-
-	if (ms_hyperv.features & HV_X64_MSR_SYNTIMER_AVAILABLE)
-		clockevents_config_and_register(hv_cpu->clk_evt,
+	if (ms_hyperv_ext.features & HV_X64_MSR_SYNTIMER_AVAILABLE)
+		clockevents_config_and_register(hv_context.clk_evt[cpu],
 						HV_TIMER_FREQUENCY,
 						HV_MIN_DELTA_TICKS,
 						HV_MAX_MAX_DELTA_TICKS);
 }
 
+void hv_clockevents_unbind(int cpu)
+{
+	if (ms_hyperv_ext.features & HV_X64_MSR_SYNTIMER_AVAILABLE)
+		clockevents_unbind_device(hv_context.clk_evt[cpu], cpu);
+}
+
+int hv_synic_cpu_used(unsigned int cpu)
+{
+	struct vmbus_channel *channel, *sc;
+	bool channel_found = false;
+	unsigned long flags;
+
+	/*
+	 * Search for channels which are bound to the CPU we're about to
+	 * cleanup. In case we find one and vmbus is still connected we need to
+	 * fail, this will effectively prevent CPU offlining. There is no way
+	 * we can re-bind channels to different CPUs for now.
+	 */
+	mutex_lock(&vmbus_connection.channel_mutex);
+	list_for_each_entry(channel, &vmbus_connection.chn_list, listentry) {
+		if (channel->target_cpu == cpu) {
+			channel_found = true;
+			break;
+		}
+		spin_lock_irqsave(&channel->lock, flags);
+		list_for_each_entry(sc, &channel->sc_list, sc_list) {
+			if (sc->target_cpu == cpu) {
+				channel_found = true;
+				break;
+			}
+		}
+		spin_unlock_irqrestore(&channel->lock, flags);
+		if (channel_found)
+			break;
+	}
+	mutex_unlock(&vmbus_connection.channel_mutex);
+
+	if (channel_found && vmbus_connection.conn_state == CONNECTED)
+		return 1;
+
+	return 0;
+}
+#endif 
+
 /*
- * hv_synic_init - Initialize the Synthetic Interrupt Controller.
+ * hv_synic_init - Initialize the Synthethic Interrupt Controller.
  *
  * If it is already initialized by another entity (ie x2v shim), we need to
  * retrieve the initialized message and event pages.  Otherwise, we create and
  * initialize the message and event pages.
  */
-int hv_synic_init(unsigned int cpu)
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+void hv_synic_init(unsigned int arg)
+#else
+void hv_synic_init(void *arg)
+#endif
 {
-	struct hv_per_cpu_context *hv_cpu
-		= per_cpu_ptr(hv_context.cpu_context, cpu);
 	union hv_synic_simp simp;
 	union hv_synic_siefp siefp;
 	union hv_synic_sint shared_sint;
 	union hv_synic_scontrol sctrl;
 
+	int cpu = smp_processor_id();
+	struct hv_per_cpu_context *hv_cpu
+		= per_cpu_ptr(hv_context.cpu_context, cpu);
+
 	/* Setup the Synic's message page */
 	hv_get_simp(simp.as_uint64);
+
 	simp.simp_enabled = 1;
 	simp.base_simp_gpa = virt_to_phys(hv_cpu->synic_message_page)
 		>> PAGE_SHIFT;
@@ -269,12 +341,24 @@
 	hv_get_synint_state(HV_X64_MSR_SINT0 + VMBUS_MESSAGE_SINT,
 			    shared_sint.as_uint64);
 
+	shared_sint.as_uint64 = 0;
 	shared_sint.vector = HYPERVISOR_CALLBACK_VECTOR;
 	shared_sint.masked = false;
-	if (ms_hyperv.hints & HV_X64_DEPRECATING_AEOI_RECOMMENDED)
+
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,4))
+	/*
+	 * RHEL 7.4 and older's hyperv_vector_handler() doesn't have the
+	 * patch: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=a33fd4c27b3ad11c66bdadc5fe6075297ca87a6d,
+	 * so we must set shared_sint.auto_eoi to true, otherwise the VM
+	 * hangs when booting up.
+	 */
+	shared_sint.auto_eoi = true;
+#else
+	if (ms_hyperv_ext.hints & HV_X64_DEPRECATING_AEOI_RECOMMENDED)
 		shared_sint.auto_eoi = false;
 	else
 		shared_sint.auto_eoi = true;
+#endif
 
 	hv_set_synint_state(HV_X64_MSR_SINT0 + VMBUS_MESSAGE_SINT,
 			    shared_sint.as_uint64);
@@ -287,93 +371,65 @@
 
 	hv_context.synic_initialized = true;
 
+	hv_cpu_init(cpu);
+
+#ifdef NOTYET
 	/*
 	 * Register the per-cpu clockevent source.
 	 */
-	hv_clockevents_bind(cpu);
+	if (ms_hyperv_ext.features & HV_X64_MSR_SYNTIMER_AVAILABLE) 
+		clockevents_config_and_register(hv_cpu->clk_evt, 
+			HV_TIMER_FREQUENCY, 
+			HV_MIN_DELTA_TICKS, 
+			HV_MAX_MAX_DELTA_TICKS); 
+#endif
 
-	return 0;
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+	hv_clockevents_bind(cpu);
+#endif
+	return;
 }
 
 /*
  * hv_synic_clockevents_cleanup - Cleanup clockevent devices
  */
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
 void hv_synic_clockevents_cleanup(void)
 {
 	int cpu;
 
-	if (!(ms_hyperv.features & HV_X64_MSR_SYNTIMER_AVAILABLE))
+	if (!(ms_hyperv_ext.features & HV_X64_MSR_SYNTIMER_AVAILABLE))
 		return;
 
-	for_each_present_cpu(cpu) {
-		struct hv_per_cpu_context *hv_cpu
-			= per_cpu_ptr(hv_context.cpu_context, cpu);
-
-		clockevents_unbind_device(hv_cpu->clk_evt, cpu);
-	}
-}
-
-void hv_clockevents_unbind(int cpu)
-{
-	struct hv_per_cpu_context *hv_cpu
-		= per_cpu_ptr(hv_context.cpu_context, cpu);
-
-	if (ms_hyperv.features & HV_X64_MSR_SYNTIMER_AVAILABLE)
-		clockevents_unbind_device(hv_cpu->clk_evt, cpu);
-}
-
-int hv_synic_cpu_used(unsigned int cpu)
-{
-	struct vmbus_channel *channel, *sc;
-	bool channel_found = false;
-	unsigned long flags;
-
-	/*
-	 * Search for channels which are bound to the CPU we're about to
-	 * cleanup. In case we find one and vmbus is still connected we need to
-	 * fail, this will effectively prevent CPU offlining. There is no way
-	 * we can re-bind channels to different CPUs for now.
-	 */
-	mutex_lock(&vmbus_connection.channel_mutex);
-	list_for_each_entry(channel, &vmbus_connection.chn_list, listentry) {
-		if (channel->target_cpu == cpu) {
-			channel_found = true;
-			break;
-		}
-		spin_lock_irqsave(&channel->lock, flags);
-		list_for_each_entry(sc, &channel->sc_list, sc_list) {
-			if (sc->target_cpu == cpu) {
-				channel_found = true;
-				break;
-			}
-		}
-		spin_unlock_irqrestore(&channel->lock, flags);
-		if (channel_found)
-			break;
-	}
-	mutex_unlock(&vmbus_connection.channel_mutex);
-
-	if (channel_found && vmbus_connection.conn_state == CONNECTED)
-		return 1;
-
-	return 0;
+	for_each_present_cpu(cpu)
+		clockevents_unbind_device(hv_context.clk_evt[cpu], cpu);
 }
-
+#endif
 /*
  * hv_synic_cleanup - Cleanup routine for hv_synic_init().
  */
-int hv_synic_cleanup(unsigned int cpu)
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+void hv_synic_cleanup(unsigned int cpu)
+#else
+void hv_synic_cleanup(void *arg)
+#endif
 {
 	union hv_synic_sint shared_sint;
 	union hv_synic_simp simp;
 	union hv_synic_siefp siefp;
 	union hv_synic_scontrol sctrl;
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+	unsigned int cpu = smp_processor_id();
+#endif
 
 	if (!hv_context.synic_initialized)
-		return -EFAULT;
+		return;
 
-	/* Turn off clockevent device */
-	if (ms_hyperv.features & HV_X64_MSR_SYNTIMER_AVAILABLE) {
+/*	Upstream referecen: 6ffc4b85358f6b7d252420cfa5862312cf5f83d8
+	Code locks on 7.3 with no reboot during kdump
+
+	#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
+	if (ms_hyperv_ext.features & HV_X64_MSR_SYNTIMER_AVAILABLE) {
 		struct hv_per_cpu_context *hv_cpu
 			= this_cpu_ptr(hv_context.cpu_context);
 
@@ -381,6 +437,12 @@
 		hv_ce_setmode(CLOCK_EVT_MODE_SHUTDOWN, hv_cpu->clk_evt);
 		put_cpu_ptr(hv_cpu);
 	}
+#else
+*/
+	/* Turn off clockevent device */
+	if (ms_hyperv_ext.features & HV_X64_MSR_SYNTIMER_AVAILABLE)
+		hv_ce_setmode(CLOCK_EVT_MODE_SHUTDOWN,
+			      hv_context.clk_evt[cpu]);
 
 	hv_get_synint_state(HV_X64_MSR_SINT0 + VMBUS_MESSAGE_SINT,
 			    shared_sint.as_uint64);
@@ -392,22 +454,19 @@
 	hv_set_synint_state(HV_X64_MSR_SINT0 + VMBUS_MESSAGE_SINT,
 			    shared_sint.as_uint64);
 
-	hv_get_simp(simp.as_uint64);
+        hv_get_simp(simp.as_uint64);
 	simp.simp_enabled = 0;
 	simp.base_simp_gpa = 0;
 
-	hv_set_simp(simp.as_uint64);
+        hv_set_simp(simp.as_uint64);
 
 	hv_get_siefp(siefp.as_uint64);
 	siefp.siefp_enabled = 0;
 	siefp.base_siefp_gpa = 0;
 
 	hv_set_siefp(siefp.as_uint64);
-
 	/* Disable the global synic bit */
 	hv_get_synic_state(sctrl.as_uint64);
 	sctrl.enable = 0;
 	hv_set_synic_state(sctrl.as_uint64);
-
-	return 0;
 }
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hv_fcopy.c linux-3.10.0-957.el7.lis/drivers/hv/hv_fcopy.c
--- linux-3.10.0-957.el7.orig/drivers/hv/hv_fcopy.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hv_fcopy.c	2018-12-12 00:51:40.771784891 +0000
@@ -61,6 +61,7 @@
 	struct hv_fcopy_hdr  *fcopy_msg; /* current message */
 	struct vmbus_channel *recv_channel; /* chn we got the request */
 	u64 recv_req_id; /* request ID. */
+	void *fcopy_context; /* for the channel callback */
 } fcopy_transaction;
 
 static void fcopy_respond_to_host(int error);
@@ -71,6 +72,7 @@
 static const char fcopy_devname[] = "vmbus/hv_fcopy";
 static u8 *recv_buffer;
 static struct hvutil_transport *hvt;
+
 /*
  * This state maintains the version number registered by the daemon.
  */
@@ -90,6 +92,7 @@
 	 * process the pending transaction.
 	 */
 	fcopy_respond_to_host(HV_E_FAIL);
+
 	hv_poll_channel(fcopy_transaction.recv_channel, fcopy_poll_wrapper);
 }
 
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hv_init.c linux-3.10.0-957.el7.lis/drivers/hv/hv_init.c
--- linux-3.10.0-957.el7.orig/drivers/hv/hv_init.c	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hv_init.c	2018-12-12 00:51:40.774784872 +0000
@@ -0,0 +1,328 @@
+/*
+ * X86 specific Hyper-V initialization code.
+ *
+ * Copyright (C) 2016, Microsoft, Inc.
+ *
+ * Author : K. Y. Srinivasan <kys@microsoft.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
+ * NON INFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ */
+
+#include <linux/types.h>
+#include <asm/hyperv.h>
+#include <asm/mshyperv.h>
+#include <asm/hypervisor.h>
+#include <linux/version.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include <linux/clockchips.h>
+#include <linux/slab.h>
+
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5))
+#ifdef CONFIG_X86_64
+
+static struct ms_hyperv_tsc_page *tsc_pg;
+
+static u64 read_hv_clock_tsc(struct clocksource *arg)
+{
+	u64 current_tick;
+
+	if (tsc_pg->tsc_sequence != 0) {
+		/*
+		 * Use the tsc page to compute the value.
+		 */
+
+		while (1) {
+			u64 tmp;
+			u32 sequence = tsc_pg->tsc_sequence;
+			u64 cur_tsc;
+			u64 scale = tsc_pg->tsc_scale;
+			s64 offset = tsc_pg->tsc_offset;
+
+			rdtscll(cur_tsc);
+			/* current_tick = ((cur_tsc *scale) >> 64) + offset */
+			asm("mulq %3"
+				: "=d" (current_tick), "=a" (tmp)
+				: "a" (cur_tsc), "r" (scale));
+
+			current_tick += offset;
+			if (tsc_pg->tsc_sequence == sequence)
+				return current_tick;
+
+			if (tsc_pg->tsc_sequence != 0)
+				continue;
+			/*
+			 * Fallback using MSR method.
+			 */
+			break;
+		}
+	}
+	rdmsrl(HV_X64_MSR_TIME_REF_COUNT, current_tick);
+	return current_tick;
+}
+
+static struct clocksource hyperv_cs_tsc = {
+	.name		= "lis_hyperv_clocksource_tsc_page",
+	.rating		= 425,
+	.read		= read_hv_clock_tsc,
+	.mask		= CLOCKSOURCE_MASK(64),
+	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
+};
+#endif
+
+static u64 read_hv_clock_msr(struct clocksource *arg)
+{
+	u64 current_tick;
+	/*
+	 * Read the partition counter to get the current tick count. This count
+	 * is set to 0 when the partition is created and is incremented in
+	 * 100 nanosecond units.
+	 */
+	rdmsrl(HV_X64_MSR_TIME_REF_COUNT, current_tick);
+	return current_tick;
+}
+
+static struct clocksource hyperv_cs_msr = {
+	.name		= "lis_hyperv_clocksource_msr",
+	.rating		= 425,
+	.read		= read_hv_clock_msr,
+	.mask		= CLOCKSOURCE_MASK(64),
+	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
+};
+
+
+void *hv_hypercall_pg;
+EXPORT_SYMBOL_GPL(hv_hypercall_pg);
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,4))
+/*
+ *  The clocksource is exposed by kernel in RH7.4 and higher.
+ *  Avoid redefining it for RH 7.4 and higher.
+ */
+struct clocksource *hyperv_cs;
+EXPORT_SYMBOL_GPL(hyperv_cs);
+#endif
+
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,5))
+int hv_cpu_init(unsigned int cpu)
+{
+	/*
+	 * In RHEL 7.5+, the kernel initializes the hv_vp_index[]. So
+	 * in our LIS drivers, we don't need do anything.
+	 */
+	return 0;
+}
+
+void hyperv_init(void)
+{
+	/*
+	 * In RHEL 7.5+, hyperv_init() runs in the kernel: see
+	 * <RHEL 7.5+'s src code>/arch/x86/hyperv/hv_init.c: hyperv_init().
+	 * So, in our LIS drivers we don't need do anything.
+	 */
+}
+#else
+
+u32 *hv_vp_index;
+EXPORT_SYMBOL_GPL(hv_vp_index);
+
+int hv_cpu_init(unsigned int cpu)
+{
+	u64 msr_vp_index;
+
+	hv_get_vp_index(msr_vp_index);
+
+	hv_vp_index[smp_processor_id()] = msr_vp_index;
+
+	return 0;
+}
+
+/*
+ * This function is to be invoked early in the boot sequence after the
+ * hypervisor has been detected.
+ *
+ * 1. Setup the hypercall page.
+ * 2. Register Hyper-V specific clocksource.
+ */
+void hyperv_init(void)
+{
+	u64 guest_id, required_msrs;
+	union hv_x64_msr_hypercall_contents hypercall_msr;
+
+	if (x86_hyper != &x86_hyper_ms_hyperv)
+		return;
+
+	/* Absolutely required MSRs */
+	required_msrs = HV_X64_MSR_HYPERCALL_AVAILABLE |
+		HV_X64_MSR_VP_INDEX_AVAILABLE;
+
+	if ((ms_hyperv_ext.features & required_msrs) != required_msrs)
+		return;
+
+	/* Allocate percpu VP index */
+	hv_vp_index = kmalloc_array(num_possible_cpus(), sizeof(*hv_vp_index),
+				    GFP_KERNEL);
+	if (!hv_vp_index)
+		return;
+	/* We'll initialize hv_vp_index[] later in hv_synic_init. */
+
+	/*
+	 * Setup the hypercall page and enable hypercalls.
+	 * 1. Register the guest ID
+	 * 2. Enable the hypercall and register the hypercall page
+	 */
+	guest_id = generate_guest_id(0, LINUX_VERSION_CODE, 0);
+	wrmsrl(HV_X64_MSR_GUEST_OS_ID, guest_id);
+
+	hv_hypercall_pg  = __vmalloc(PAGE_SIZE, GFP_KERNEL, PAGE_KERNEL_RX);
+	if (hv_hypercall_pg == NULL) {
+		wrmsrl(HV_X64_MSR_GUEST_OS_ID, 0);
+		goto free_vp_index;
+	}
+
+	rdmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);
+	hypercall_msr.enable = 1;
+	hypercall_msr.guest_physical_address = vmalloc_to_pfn(hv_hypercall_pg);
+	wrmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);
+
+	/*
+	 * Register Hyper-V specific clocksource.
+	 */
+#ifdef CONFIG_X86_64
+	if (ms_hyperv_ext.features & HV_X64_MSR_REFERENCE_TSC_AVAILABLE) {
+		union hv_x64_msr_hypercall_contents tsc_msr;
+
+		tsc_pg = __vmalloc(PAGE_SIZE, GFP_KERNEL, PAGE_KERNEL);
+		if (!tsc_pg)
+			goto register_msr_cs;
+
+		hyperv_cs = &hyperv_cs_tsc;
+
+		rdmsrl(HV_X64_MSR_REFERENCE_TSC, tsc_msr.as_uint64);
+
+		tsc_msr.enable = 1;
+		tsc_msr.guest_physical_address = vmalloc_to_pfn(tsc_pg);
+
+		wrmsrl(HV_X64_MSR_REFERENCE_TSC, tsc_msr.as_uint64);
+		clocksource_register_hz(&hyperv_cs_tsc, NSEC_PER_SEC/100);
+		return;
+	}
+#endif
+	/*
+	 * For 32 bit guests just use the MSR based mechanism for reading
+	 * the partition counter.
+	 */
+
+register_msr_cs:
+	hyperv_cs = &hyperv_cs_msr;
+	if (ms_hyperv_ext.features & HV_X64_MSR_TIME_REF_COUNT_AVAILABLE)
+		clocksource_register_hz(&hyperv_cs_msr, NSEC_PER_SEC/100);
+
+	return;
+
+free_vp_index:
+	kfree(hv_vp_index);
+	hv_vp_index = NULL;
+}
+
+/*
+ * This routine is called before kexec/kdump, it does the required cleanup.
+ */
+
+void hyperv_cleanup(void)
+{
+	union hv_x64_msr_hypercall_contents hypercall_msr;
+
+	/* Reset our OS id */
+	wrmsrl(HV_X64_MSR_GUEST_OS_ID, 0);
+
+	/* Reset the hypercall page */
+	hypercall_msr.as_uint64 = 0;
+	wrmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);
+
+	/* Reset the TSC page */
+	hypercall_msr.as_uint64 = 0;
+	wrmsrl(HV_X64_MSR_REFERENCE_TSC, hypercall_msr.as_uint64);
+}
+EXPORT_SYMBOL_GPL(hyperv_cleanup);
+
+void hyperv_report_panic(struct pt_regs *regs, long err)
+{
+	static bool panic_reported;
+	u64 guest_id;
+
+	/*
+	 * We prefer to report panic on 'die' chain as we have proper
+	 * registers to report, but if we miss it (e.g. on BUG()) we need
+	 * to report it on 'panic'.
+	 */
+	if (panic_reported)
+		return;
+	panic_reported = true;
+
+	rdmsrl(HV_X64_MSR_GUEST_OS_ID, guest_id);
+
+	wrmsrl(HV_X64_MSR_CRASH_P0, err);
+	wrmsrl(HV_X64_MSR_CRASH_P1, guest_id);
+	wrmsrl(HV_X64_MSR_CRASH_P2, regs->ip);
+	wrmsrl(HV_X64_MSR_CRASH_P3, _HV_DRV_VERSION);
+	wrmsrl(HV_X64_MSR_CRASH_P4, regs->sp);
+
+	/*
+	 * Let Hyper-V know there is crash data available
+	 */
+	wrmsrl(HV_X64_MSR_CRASH_CTL, HV_CRASH_CTL_CRASH_NOTIFY);
+}
+EXPORT_SYMBOL_GPL(hyperv_report_panic);
+
+
+bool hv_is_hypercall_page_setup(void)
+{
+	union hv_x64_msr_hypercall_contents hypercall_msr;
+
+	/* Check if the hypercall page is setup */
+	hypercall_msr.as_uint64 = 0;
+	rdmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);
+
+	if (!hypercall_msr.enable)
+		return false;
+
+	return true;
+}
+
+EXPORT_SYMBOL_GPL(hv_is_hypercall_page_setup);
+#endif
+
+void hv_print_host_info(void) {
+	int hv_host_info_eax;
+	int hv_host_info_ebx;
+	int hv_host_info_ecx;
+	int hv_host_info_edx;
+
+	/*
+	 * Extract host information.
+	 */
+	if (cpuid_eax(HVCPUID_VENDOR_MAXFUNCTION) >= HVCPUID_VERSION) {
+		hv_host_info_eax = cpuid_eax(HVCPUID_VERSION);
+		hv_host_info_ebx = cpuid_ebx(HVCPUID_VERSION);
+		hv_host_info_ecx = cpuid_ecx(HVCPUID_VERSION);
+		hv_host_info_edx = cpuid_edx(HVCPUID_VERSION);
+
+		pr_info("Hyper-V Host Build:%d-%d.%d-%d-%d.%d\n",
+			hv_host_info_eax, hv_host_info_ebx >> 16,
+			hv_host_info_ebx & 0xFFFF, hv_host_info_ecx,
+			hv_host_info_edx >> 24, hv_host_info_edx & 0xFFFFFF);
+	}
+}
+EXPORT_SYMBOL_GPL(hv_print_host_info);
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hv_kvp.c linux-3.10.0-957.el7.lis/drivers/hv/hv_kvp.c
--- linux-3.10.0-957.el7.orig/drivers/hv/hv_kvp.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hv_kvp.c	2018-12-12 00:51:40.777784853 +0000
@@ -101,12 +101,13 @@
 static const char kvp_devname[] = "vmbus/hv_kvp";
 static u8 *recv_buffer;
 static struct hvutil_transport *hvt;
+
 /*
  * Register the kernel component with the user-level daemon.
  * As part of this registration, pass the LIC version number.
  * This number has no meaning, it satisfies the registration protocol.
  */
-#define HV_DRV_VERSION           "3.1"
+//#define HV_DRV_VERSION           "3.1"
 
 static void kvp_poll_wrapper(void *channel)
 {
@@ -634,7 +635,7 @@
 		if (host_negotiatied == NEGO_NOT_STARTED) {
 			host_negotiatied = NEGO_IN_PROGRESS;
 			schedule_delayed_work(&kvp_host_handshake_work,
-				      HV_UTIL_NEGO_TIMEOUT * HZ);
+					      HV_UTIL_NEGO_TIMEOUT * HZ);
 		}
 		return;
 	}
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hv_snapshot.c linux-3.10.0-957.el7.lis/drivers/hv/hv_snapshot.c
--- linux-3.10.0-957.el7.orig/drivers/hv/hv_snapshot.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hv_snapshot.c	2018-12-12 00:51:40.780784834 +0000
@@ -368,6 +368,7 @@
 			" not supported on this host version.\n");
 		return -ENOTSUPP;
 	}
+
 	recv_buffer = srv->recv_buffer;
 	vss_transaction.recv_channel = srv->channel;
 
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hv_trace.c linux-3.10.0-957.el7.lis/drivers/hv/hv_trace.c
--- linux-3.10.0-957.el7.orig/drivers/hv/hv_trace.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hv_trace.c	2018-12-12 00:51:40.784784808 +0000
@@ -1,5 +1,3 @@
-// SPDX-License-Identifier: GPL-2.0
-
 #include "hyperv_vmbus.h"
 
 #define CREATE_TRACE_POINTS
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hv_trace.h linux-3.10.0-957.el7.lis/drivers/hv/hv_trace.h
--- linux-3.10.0-957.el7.orig/drivers/hv/hv_trace.h	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hv_trace.h	2018-12-12 00:51:40.786784795 +0000
@@ -1,5 +1,3 @@
-// SPDX-License-Identifier: GPL-2.0
-
 #undef TRACE_SYSTEM
 #define TRACE_SYSTEM hyperv
 
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hv_util.c linux-3.10.0-957.el7.lis/drivers/hv/hv_util.c
--- linux-3.10.0-957.el7.orig/drivers/hv/hv_util.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hv_util.c	2018-12-12 00:51:40.789784776 +0000
@@ -232,9 +232,9 @@
 
 static void hv_set_host_time(struct work_struct *work)
 {
-	struct timespec ts = timespec64_to_timespec(hv_get_adj_host_time());
+	struct timespec64 ts = hv_get_adj_host_time();
 
-	do_settimeofday(&ts);
+	do_settimeofday64(&ts);
 }
 
 /*
@@ -356,11 +356,11 @@
 
 	while (1) {
 
-		vmbus_recvpacket(channel, hbeat_txf_buf,
-				 PAGE_SIZE, &recvlen, &requestid);
+               vmbus_recvpacket(channel, hbeat_txf_buf,
+                                PAGE_SIZE, &recvlen, &requestid);
 
-		if (!recvlen)
-			break;
+               if (!recvlen)
+                       break;
 
 		icmsghdrp = (struct icmsg_hdr *)&hbeat_txf_buf[
 				sizeof(struct vmbuspipe_hdr)];
@@ -521,8 +521,8 @@
 	.enable         = hv_ptp_enable,
 	.adjtime        = hv_ptp_adjtime,
 	.adjfreq        = hv_ptp_adjfreq,
-	.gettime64      = hv_ptp_gettime,
-	.settime64      = hv_ptp_settime,
+	.gettime        = hv_ptp_gettime,
+	.settime        = hv_ptp_settime,
 	.owner		= THIS_MODULE,
 };
 
@@ -533,7 +533,6 @@
 	/* TimeSync requires Hyper-V clocksource. */
 	if (!hyperv_cs)
 		return -ENODEV;
-
 	spin_lock_init(&host_ts.lock);
 
 	INIT_WORK(&adj_time_work, hv_set_host_time);
@@ -579,3 +578,4 @@
 
 MODULE_DESCRIPTION("Hyper-V Utilities");
 MODULE_LICENSE("GPL");
+MODULE_VERSION(HV_DRV_VERSION);
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/hyperv_vmbus.h linux-3.10.0-957.el7.lis/drivers/hv/hyperv_vmbus.h
--- linux-3.10.0-957.el7.orig/drivers/hv/hyperv_vmbus.h	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/hyperv_vmbus.h	2018-12-12 00:51:40.796784732 +0000
@@ -28,9 +28,8 @@
 #include <linux/list.h>
 #include <asm/sync_bitops.h>
 #include <linux/atomic.h>
-#include <linux/hyperv.h>
 #include <linux/interrupt.h>
-
+#include <linux/hyperv.h>
 #include "hv_trace.h"
 
 /*
@@ -43,6 +42,9 @@
  */
 #define HV_UTIL_NEGO_TIMEOUT 55
 
+
+#define HV_SYNIC_VERSION_1		(0x1)
+
 /* Define synthetic interrupt controller flag constants. */
 #define HV_EVENT_FLAGS_COUNT		(256 * 8)
 #define HV_EVENT_FLAGS_LONG_COUNT	(256 / sizeof(unsigned long))
@@ -63,6 +65,7 @@
 	};
 };
 
+
 /* Define the synthetic interrupt controller event flags format. */
 union hv_synic_event_flags {
 	unsigned long flags[HV_EVENT_FLAGS_LONG_COUNT];
@@ -180,7 +183,6 @@
 	u64 payload[HV_MESSAGE_PAYLOAD_QWORD_COUNT];
 };
 
-
 enum {
 	VMBUS_MESSAGE_CONNECTION_ID	= 1,
 	VMBUS_MESSAGE_PORT_ID		= 1,
@@ -191,6 +193,7 @@
 	VMBUS_MESSAGE_SINT		= 2,
 };
 
+
 /*
  * Per cpu state for channel handling
  */
@@ -214,7 +217,6 @@
 	 * per-cpu list of the channels based on their CPU affinity.
 	 */
 	struct list_head chan_list;
-	struct clock_event_device *clk_evt;
 };
 
 struct hv_context {
@@ -229,15 +231,19 @@
 
 	struct hv_per_cpu_context __percpu *cpu_context;
 
+	struct clock_event_device *clk_evt[NR_CPUS];
+
 	/*
-	 * To manage allocations in a NUMA node.
-	 * Array indexed by numa node ID.
-	 */
-	struct cpumask *hv_numa_map;
+         * To manage allocations in a NUMA node.
+         * Array indexed by numa node ID.
+         */
+        struct cpumask *hv_numa_map;
 };
 
 extern struct hv_context hv_context;
 
+extern int affinity_mode;
+
 /* Hv Interface */
 
 extern int hv_init(void);
@@ -250,9 +256,15 @@
 
 extern void hv_synic_free(void);
 
-extern int hv_synic_init(unsigned int cpu);
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+extern void hv_synic_init(unsigned int cpu);
 
-extern int hv_synic_cleanup(unsigned int cpu);
+extern void hv_synic_cleanup(unsigned int cpu);
+#else
+extern void hv_synic_init(void *cpu);
+
+extern void hv_synic_cleanup(void *cpu);
+#endif
 
 extern void hv_synic_clockevents_cleanup(void);
 
@@ -273,9 +285,13 @@
 int hv_ringbuffer_write(struct vmbus_channel *channel,
 			const struct kvec *kv_list, u32 kv_count);
 
+void hv_get_ringbuffer_available_space(struct hv_ring_buffer_info *inring_info,
+				       u32 *bytes_avail_toread,
+				       u32 *bytes_avail_towrite);
+
 int hv_ringbuffer_read(struct vmbus_channel *channel,
-		       void *buffer, u32 buflen, u32 *buffer_actual_len,
-		       u64 *requestid, bool raw);
+		   void *buffer, u32 buflen, u32 *buffer_actual_len,
+		   u64 *requestid, bool raw);
 
 /*
  * Maximum channels is determined by the size of the interrupt page
@@ -334,7 +350,14 @@
 	struct list_head chn_list;
 	struct mutex channel_mutex;
 
+	/*
+	 * An offer message is handled first on the work_queue, and then
+	 * is further handled on handle_primary_chan_wq or
+	 * handle_sub_chan_wq.
+	 */
 	struct workqueue_struct *work_queue;
+	struct workqueue_struct *handle_primary_chan_wq;
+	struct workqueue_struct *handle_sub_chan_wq;
 };
 
 
@@ -408,6 +431,7 @@
 int hv_fcopy_init(struct hv_util_service *srv);
 void hv_fcopy_deinit(void);
 void hv_fcopy_onchannelcallback(void *context);
+
 void vmbus_initiate_unload(bool crash);
 
 static inline void hv_poll_channel(struct vmbus_channel *channel,
@@ -416,10 +440,12 @@
 	if (!channel)
 		return;
 
-	if (in_interrupt() && (channel->target_cpu == smp_processor_id())) {
+	if ((irqs_disabled() || in_interrupt()) &&
+	    (channel->target_cpu == smp_processor_id())) {
 		cb(channel);
 		return;
 	}
+
 	smp_call_function_single(channel->target_cpu, cb, channel, true);
 }
 
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/Makefile linux-3.10.0-957.el7.lis/drivers/hv/Makefile
--- linux-3.10.0-957.el7.orig/drivers/hv/Makefile	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/Makefile	2018-12-12 00:51:40.932783867 +0000
@@ -3,9 +3,10 @@
 obj-$(CONFIG_HYPERV_BALLOON)	+= hv_balloon.o
 
 CFLAGS_hv_trace.o = -I$(src)
-CFLAGS_hv_balloon.o = -I$(src)
 
 hv_vmbus-y := vmbus_drv.o \
-		 hv.o connection.o channel.o \
-		 channel_mgmt.o ring_buffer.o hv_trace.o
+		 hv.o connection.o channel.o  hv_trace.o \
+		 channel_mgmt.o ring_buffer.o \
+		 hv_init.o \
+		 ms_hyperv_ext.o
 hv_utils-y := hv_util.o hv_kvp.o hv_snapshot.o hv_fcopy.o hv_utils_transport.o
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/ms_hyperv_ext.c linux-3.10.0-957.el7.lis/drivers/hv/ms_hyperv_ext.c
--- linux-3.10.0-957.el7.orig/drivers/hv/ms_hyperv_ext.c	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/ms_hyperv_ext.c	2018-12-12 00:51:40.801784700 +0000
@@ -0,0 +1,38 @@
+/*
+ *
+ * Copyright (C) 2017, Microsoft, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
+ * NON INFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ */
+
+#include <linux/types.h>
+#include <asm/hyperv.h>
+#include <asm/mshyperv.h>
+
+/* See the comment near the struct definition to know why we need this */
+struct ms_hyperv_info_external ms_hyperv_ext;
+
+
+/* Copied from the upstream's ms_hyperv_init_platform() */
+void init_ms_hyperv_ext(void)
+{
+	/*
+	 * Extract the features and hints
+	 */
+
+	ms_hyperv_ext.features = cpuid_eax(HYPERV_CPUID_FEATURES);
+	ms_hyperv_ext.misc_features = cpuid_edx(HYPERV_CPUID_FEATURES);
+	ms_hyperv_ext.hints = cpuid_eax(HYPERV_CPUID_ENLIGHTMENT_INFO);
+
+	pr_info("Hyper-V: detected features 0x%x, hints 0x%x\n",
+		ms_hyperv_ext.features, ms_hyperv_ext.hints);
+}
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/ring_buffer.c linux-3.10.0-957.el7.lis/drivers/hv/ring_buffer.c
--- linux-3.10.0-957.el7.orig/drivers/hv/ring_buffer.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/ring_buffer.c	2018-12-12 00:51:40.804784681 +0000
@@ -49,21 +49,18 @@
  *	   once the ring buffer is empty, it will clear the
  *	   interrupt_mask and re-check to see if new data has
  *	   arrived.
- *
  * KYS: Oct. 30, 2016:
  * It looks like Windows hosts have logic to deal with DOS attacks that
  * can be triggered if it receives interrupts when it is not expecting
  * the interrupt. The host expects interrupts only when the ring
- * transitions from empty to non-empty (or full to non full on the guest
+ * transitions from empty to non-empty (or full to non full on the  guest
  * to host ring).
  * So, base the signaling decision solely on the ring state until the
  * host logic is fixed.
  */
-
 static void hv_signal_on_write(u32 old_write, struct vmbus_channel *channel)
 {
 	struct hv_ring_buffer_info *rbi = &channel->outbound;
-
 	mb();
 	if (READ_ONCE(rbi->ring_buffer->interrupt_mask))
 		return;
@@ -78,7 +75,7 @@
 		vmbus_setevent(channel);
 }
 
-/* Get the next write location for the specified ring buffer. */
+/* Get the next write location for the specified ring buffer */
 static inline u32
 hv_get_next_write_location(struct hv_ring_buffer_info *ring_info)
 {
@@ -87,7 +84,7 @@
 	return next;
 }
 
-/* Set the next write location for the specified ring buffer. */
+/* Set the next write location for the specified ring buffer */
 static inline void
 hv_set_next_write_location(struct hv_ring_buffer_info *ring_info,
 		     u32 next_write_location)
@@ -95,7 +92,7 @@
 	ring_info->ring_buffer->write_index = next_write_location;
 }
 
-/* Set the next read location for the specified ring buffer. */
+/* Set the next read location for the specified ring buffer */
 static inline void
 hv_set_next_read_location(struct hv_ring_buffer_info *ring_info,
 		    u32 next_read_location)
@@ -104,14 +101,14 @@
 	ring_info->priv_read_index = next_read_location;
 }
 
-/* Get the size of the ring buffer. */
+/* Get the size of the ring buffer */
 static inline u32
 hv_get_ring_buffersize(const struct hv_ring_buffer_info *ring_info)
 {
 	return ring_info->ring_datasize;
 }
 
-/* Get the read and write indices as u64 of the specified ring buffer. */
+/* Get the read and write indices as u64 of the specified ring buffer */
 static inline u64
 hv_get_ring_bufferindices(struct hv_ring_buffer_info *ring_info)
 {
@@ -136,6 +133,7 @@
 	start_write_offset += srclen;
 	if (start_write_offset >= ring_buffer_size)
 		start_write_offset -= ring_buffer_size;
+ 
 
 	return start_write_offset;
 }
@@ -147,15 +145,15 @@
  * Get number of bytes available to read and to write to
  * for the specified ring buffer
  */
-static void
+static inline void
 hv_get_ringbuffer_availbytes(const struct hv_ring_buffer_info *rbi,
 			     u32 *read, u32 *write)
 {
 	u32 read_loc, write_loc, dsize;
 
 	/* Capture the read/write indices before they changed */
-	read_loc = READ_ONCE(rbi->ring_buffer->read_index);
-	write_loc = READ_ONCE(rbi->ring_buffer->write_index);
+	read_loc = rbi->ring_buffer->read_index;
+	write_loc = rbi->ring_buffer->write_index;
 	dsize = rbi->ring_datasize;
 
 	*write = write_loc >= read_loc ? dsize - (write_loc - read_loc) :
@@ -163,9 +161,9 @@
 	*read = dsize - *write;
 }
 
-/* Get various debug metrics for the specified ring buffer. */
+/* Get various debug metrics for the specified ring buffer */
 void hv_ringbuffer_get_debuginfo(const struct hv_ring_buffer_info *ring_info,
-				 struct hv_ring_buffer_debug_info *debug_info)
+			    struct hv_ring_buffer_debug_info *debug_info)
 {
 	u32 bytes_avail_towrite;
 	u32 bytes_avail_toread;
@@ -187,6 +185,7 @@
 }
 EXPORT_SYMBOL_GPL(hv_ringbuffer_get_debuginfo);
 
+
 /* Initialize the ring buffer. */
 int hv_ringbuffer_init(struct hv_ring_buffer_info *ring_info,
 		       struct page *pages, u32 page_cnt)
@@ -227,8 +226,6 @@
 	ring_info->ring_buffer->feature_bits.value = 1;
 
 	ring_info->ring_size = page_cnt << PAGE_SHIFT;
-	ring_info->ring_size_div10_reciprocal =
-		reciprocal_value(ring_info->ring_size / 10);
 	ring_info->ring_datasize = ring_info->ring_size -
 		sizeof(struct hv_ring_buffer);
 
@@ -237,23 +234,24 @@
 	return 0;
 }
 
-/* Cleanup the ring buffer. */
+/* Cleanup the ring buffer */
 void hv_ringbuffer_cleanup(struct hv_ring_buffer_info *ring_info)
 {
 	vunmap(ring_info->ring_buffer);
 }
 
-/* Write to the ring buffer. */
+/* Write to the ring buffer */
 int hv_ringbuffer_write(struct vmbus_channel *channel,
 			const struct kvec *kv_list, u32 kv_count)
 {
-	int i;
+	int i = 0;
 	u32 bytes_avail_towrite;
-	u32 totalbytes_towrite = sizeof(u64);
+	u32 totalbytes_towrite = 0;
+
 	u32 next_write_location;
 	u32 old_write;
-	u64 prev_indices;
-	unsigned long flags;
+	u64 prev_indices = 0;
+	unsigned long flags = 0;
 	struct hv_ring_buffer_info *outring_info = &channel->outbound;
 
 	if (channel->rescind)
@@ -262,6 +260,8 @@
 	for (i = 0; i < kv_count; i++)
 		totalbytes_towrite += kv_list[i].iov_len;
 
+	totalbytes_towrite += sizeof(u64);
+
 	spin_lock_irqsave(&outring_info->ring_lock, flags);
 
 	bytes_avail_towrite = hv_get_bytes_to_write(outring_info);
@@ -269,7 +269,7 @@
 	/*
 	 * If there is only room for the packet, assume it is full.
 	 * Otherwise, the next time around, we think the ring buffer
-	 * is empty since the read index == write index.
+	 * is empty since the read index == write index
 	 */
 	if (bytes_avail_towrite <= totalbytes_towrite) {
 		spin_unlock_irqrestore(&outring_info->ring_lock, flags);
@@ -302,7 +302,6 @@
 	/* Now, update the write location */
 	hv_set_next_write_location(outring_info, next_write_location);
 
-
 	spin_unlock_irqrestore(&outring_info->ring_lock, flags);
 
 	hv_signal_on_write(old_write, channel);
@@ -313,10 +312,26 @@
 	return 0;
 }
 
+void hv_get_ringbuffer_available_space(struct hv_ring_buffer_info *inring_info,
+				       u32 *bytes_avail_toread,
+				       u32 *bytes_avail_towrite)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&inring_info->ring_lock, flags);
+
+	hv_get_ringbuffer_availbytes(inring_info,
+				     bytes_avail_toread,
+				     bytes_avail_towrite);
+
+	spin_unlock_irqrestore(&inring_info->ring_lock, flags);
+}
+
 int hv_ringbuffer_read(struct vmbus_channel *channel,
-		       void *buffer, u32 buflen, u32 *buffer_actual_len,
-		       u64 *requestid, bool raw)
+			void *buffer, u32 buflen, u32 *buffer_actual_len,
+			u64 *requestid, bool raw)
 {
+	int ret = 0;
 	struct vmpacket_descriptor *desc;
 	u32 packetlen, offset;
 
@@ -333,14 +348,14 @@
 		 * No error is set when there is even no header, drivers are
 		 * supposed to analyze buffer_actual_len.
 		 */
-		return 0;
+		return ret;
 	}
 
 	offset = raw ? 0 : (desc->offset8 << 3);
 	packetlen = (desc->len8 << 3) - offset;
 	*buffer_actual_len = packetlen;
 	*requestid = desc->trans_id;
-
+	
 	if (unlikely(packetlen > buflen))
 		return -ENOBUFS;
 
@@ -353,7 +368,7 @@
 	/* Notify host of update */
 	hv_pkt_iter_close(channel);
 
-	return 0;
+	return ret;
 }
 
 /*
@@ -416,77 +431,123 @@
 
 	/* more data? */
 	return hv_pkt_iter_first(channel);
-}
-EXPORT_SYMBOL_GPL(__hv_pkt_iter_next);
 
-/* How many bytes were read in this iterator cycle */
-static u32 hv_pkt_iter_bytes_read(const struct hv_ring_buffer_info *rbi,
-					u32 start_read_index)
-{
-	if (rbi->priv_read_index >= start_read_index)
-		return rbi->priv_read_index - start_read_index;
-	else
-		return rbi->ring_datasize - start_read_index +
-			rbi->priv_read_index;
 }
+EXPORT_SYMBOL_GPL(__hv_pkt_iter_next);
 
 /*
- * Update host ring buffer after iterating over packets.
+ * Update host ring buffer after iterating over packets. If the ring buffer
+ * is blocked on the host side due to being full, and sufficient space is
+ * being freed up, signal the host. But be careful to only signal the host
+ * when necesary, both for performance reasons and because Hyper-V protects
+ * itself by throttling guests that signal inappropriately.
+ *
+ * Determing when to signal is tricky. There are three key data inputs that
+ * must be handled in this order to avoid race conditions:
+ *
+ * 1. Update the read_index
+ * 2. Read the pending_send_sz
+ * 3. Read the current write_index
+ *
+ * Note that the interrupt_mask is not used to determine when to signal.
+ * The interrupt_mask is used only on the guest->host ring buffer when
+ * sending requests to the host. The host does not use it on the host->
+ * guest ring buffer to indicate whether it should be signaled.
+ *
  */
 void hv_pkt_iter_close(struct vmbus_channel *channel)
 {
 	struct hv_ring_buffer_info *rbi = &channel->inbound;
-	u32 curr_write_sz, pending_sz, bytes_read, start_read_index;
+	u32 orig_read_index, read_index, write_index, pending_sz;
+	u32 orig_free_space, free_space;
 
 	/*
-	 * Make sure all reads are done before we update the read index since
+	 * Make sure all reads are done before updating the read index since
 	 * the writer may start writing to the read area once the read index
 	 * is updated.
 	 */
-	virt_rmb();
-	start_read_index = rbi->ring_buffer->read_index;
+	rmb();
+	orig_read_index = rbi->ring_buffer->read_index;
 	rbi->ring_buffer->read_index = rbi->priv_read_index;
 
+	/*
+	 * Older versions of Hyper-V (before WS2012 and Win8) do not
+	 * implement pending_send_sz and simply poll if the host->guest
+	 * ring buffer is full. No signaling is needed or expected.
+	 */
 	if (!rbi->ring_buffer->feature_bits.feat_pending_send_sz)
 		return;
 
 	/*
 	 * Issue a full memory barrier before making the signaling decision.
-	 * Here is the reason for having this barrier:
-	 * If the reading of the pend_sz (in this function)
-	 * were to be reordered and read before we commit the new read
-	 * index (in the calling function)  we could
-	 * have a problem. If the host were to set the pending_sz after we
-	 * have sampled pending_sz and go to sleep before we commit the
-	 * read index, we could miss sending the interrupt. Issue a full
+	 * If the reading of pending_send_sz were to be reordered and happen
+	 * before we commit the new read_index, a race could occur.  If the
+	 * host were to set the pending_send_sz after we have sampled
+	 * pending_send_sz, and the ring buffer blocks before we commit the
+	 * read index, we could miss signaling the host.  Issue a full
 	 * memory barrier to address this.
 	 */
 	mb();
 
+	/*
+	 * If the pending_send_sz is zero, then the ring buffer is not
+	 * blocked and there is no need to signal. This is by far the
+	 * most common case, so exit quickly for best performance.
+	 */
 	pending_sz = READ_ONCE(rbi->ring_buffer->pending_send_sz);
 	if (!pending_sz)
 		return;
 
 	/*
-	 * Ensure the read of write_index in hv_get_bytes_to_write()
-	 * happens after the read of pending_send_sz.
+	 * Since pending_send_sz is non-zero, this ring buffer is probably
+	 * blocked on the host, though we don't know for sure because the
+	 * host may check the ring buffer at any time. In any case, see
+	 * if we're freeing enough space in the ring buffer to warrant
+	 * signaling the host. To avoid duplicates, signal the host only if
+	 * transitioning from a "not enough free space" state to a "enough
+	 * free space" state. For example, it's possible that this function
+	 * could run and free up enough space to signal the host, and then
+	 * run again and free up additional space before the host has a
+	 * chance to clear the pending_send_sz. The 2nd invocation would be
+	 * a null transition from "enough free space" to "enough free space",
+	 * which doesn't warrant a signal.
+	 * 
+	 * To do this, calculate the amount of free space that was available
+	 * before updating the read_index and the amount of free space
+	 * available after updating the read_index. Base the calculation
+	 * on the current write_index, protected by READ_ONCE() because
+	 * the host could be changing the value. rmb() ensures the
+	 * value is read after pending_send_sz is read.
 	 */
-	virt_rmb();
-	curr_write_sz = hv_get_bytes_to_write(rbi);
-	bytes_read = hv_pkt_iter_bytes_read(rbi, start_read_index);
+	rmb();
+	write_index = READ_ONCE(rbi->ring_buffer->write_index);
 
 	/*
-	 * If there was space before we began iteration,
-	 * then host was not blocked.
+	 * If the state was "enough free space" prior to updating
+	 * the read_index, then there's no need to signal.
 	 */
-
-	if (curr_write_sz - bytes_read > pending_sz)
+	orig_free_space = (write_index >= orig_read_index)
+			? rbi->ring_datasize - (write_index - orig_read_index)
+			: orig_read_index - write_index;
+	if (orig_free_space > pending_sz)
 		return;
 
-	/* If pending write will not fit, don't give false hope. */
-	if (curr_write_sz <= pending_sz)
+	/* 
+	 * If still in a "not enough space" situation after updating the
+	 * read_index, there's no need to signal. A later invocation of
+	 * this routine will free up enough space and signal the host.
+	 */
+	read_index = rbi->ring_buffer->read_index;
+	free_space = (write_index >= read_index)
+			? rbi->ring_datasize - (write_index - read_index)
+			: read_index - write_index;
+	if (free_space <= pending_sz)
 		return;
 
+	/*
+	 * We're transitioning from "not enough free space" to
+	 * "enough free space", so signal the host.
+	 */
 	vmbus_setevent(channel);
 }
 EXPORT_SYMBOL_GPL(hv_pkt_iter_close);
diff -Naur linux-3.10.0-957.el7.orig/drivers/hv/vmbus_drv.c linux-3.10.0-957.el7.lis/drivers/hv/vmbus_drv.c
--- linux-3.10.0-957.el7.orig/drivers/hv/vmbus_drv.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/hv/vmbus_drv.c	2018-12-12 00:51:40.811784637 +0000
@@ -23,37 +23,39 @@
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
 #include <linux/init.h>
+#include <linux/version.h>
 #include <linux/module.h>
 #include <linux/device.h>
 #include <linux/interrupt.h>
 #include <linux/sysctl.h>
 #include <linux/slab.h>
 #include <linux/acpi.h>
-#include <acpi/acpi_bus.h>
 #include <linux/completion.h>
 #include <linux/hyperv.h>
+#include <asm/mshyperv.h>
 #include <linux/kernel_stat.h>
 #include <linux/clockchips.h>
 #include <linux/cpu.h>
 #include <asm/hyperv.h>
-#include <asm/mshyperv.h>
-#include <linux/notifier.h>
-#include <linux/ptrace.h>
+#include <asm/hypervisor.h>
 #include <linux/screen_info.h>
-#include <linux/kdebug.h>
 #include <linux/efi.h>
 #include <linux/random.h>
+#include <linux/notifier.h>
+#include <linux/ptrace.h>
+#include <linux/kdebug.h>
 #include "hyperv_vmbus.h"
 
-struct vmbus_dynid {
-	struct list_head node;
-	struct hv_vmbus_device_id id;
-};
-
 static struct acpi_device  *hv_acpi_dev;
 
-static struct completion probe_event;
+int affinity_mode = HV_KEEP_HT_CPU;
+module_param(affinity_mode, int, S_IRUGO);
+MODULE_PARM_DESC(affinity_mode, "vmbus channel cpu affinity mode: 0, 1");
 
+static struct completion probe_event;
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+static int irq;
+#endif
 
 static int hyperv_panic_event(struct notifier_block *nb, unsigned long val,
 			      void *args)
@@ -79,8 +81,9 @@
 static struct notifier_block hyperv_die_block = {
 	.notifier_call = hyperv_die_event,
 };
+
 static struct notifier_block hyperv_panic_block = {
-	.notifier_call = hyperv_panic_event,
+        .notifier_call = hyperv_panic_event,
 };
 
 static const char *fb_mmio_name = "fb_range";
@@ -88,11 +91,6 @@
 static struct resource *hyperv_mmio;
 static DEFINE_SEMAPHORE(hyperv_mmio_lock);
 
-struct hv_device_info {
-	struct hv_ring_buffer_debug_info inbound;
-	struct hv_ring_buffer_debug_info outbound;
-};
-
 static int vmbus_exists(void)
 {
 	if (hv_acpi_dev == NULL)
@@ -468,9 +466,13 @@
 }
 static DEVICE_ATTR_RO(channel_vp_mapping);
 
+/*
+ * Divergence from upstream.
+ * Vendor and device attributes needed for RDMA.
+ */
 static ssize_t vendor_show(struct device *dev,
-			   struct device_attribute *dev_attr,
-			   char *buf)
+			  struct device_attribute *dev_attr,
+			  char *buf)
 {
 	struct hv_device *hv_dev = device_to_hv_device(dev);
 	return sprintf(buf, "0x%x\n", hv_dev->vendor_id);
@@ -478,8 +480,8 @@
 static DEVICE_ATTR_RO(vendor);
 
 static ssize_t device_show(struct device *dev,
-			   struct device_attribute *dev_attr,
-			   char *buf)
+			  struct device_attribute *dev_attr,
+			  char *buf)
 {
 	struct hv_device *hv_dev = device_to_hv_device(dev);
 	return sprintf(buf, "0x%x\n", hv_dev->device_id);
@@ -487,7 +489,7 @@
 static DEVICE_ATTR_RO(device);
 
 /* Set up per device attributes in /sys/bus/vmbus/devices/<bus device> */
-static struct attribute *vmbus_dev_attrs[] = {
+static struct attribute *vmbus_attrs[] = {
 	&dev_attr_id.attr,
 	&dev_attr_state.attr,
 	&dev_attr_monitor_id.attr,
@@ -515,7 +517,7 @@
 	&dev_attr_device.attr,
 	NULL,
 };
-ATTRIBUTE_GROUPS(vmbus_dev);
+ATTRIBUTE_GROUPS(vmbus);
 
 /*
  * vmbus_uevent - add uevent for our device
@@ -541,145 +543,46 @@
 
 static const uuid_le null_guid;
 
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,2))
+static inline bool is_null_guid(const __u8 *guid)
+#else
 static inline bool is_null_guid(const uuid_le *guid)
+#endif
 {
-	if (uuid_le_cmp(*guid, null_guid))
+	if (memcmp(guid, &null_guid, sizeof(uuid_le)))
 		return false;
 	return true;
 }
 
+
 /*
  * Return a matching hv_vmbus_device_id pointer.
  * If there is no match, return NULL.
  */
-static const struct hv_vmbus_device_id *hv_vmbus_get_id(struct hv_driver *drv,
-					const uuid_le *guid)
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,2))
+static const struct hv_vmbus_device_id *hv_vmbus_get_id(
+					const struct hv_vmbus_device_id *id,
+					const __u8 *guid)
 {
-	const struct hv_vmbus_device_id *id = NULL;
-	struct vmbus_dynid *dynid;
-
-	/* Look at the dynamic ids first, before the static ones */
-	spin_lock(&drv->dynids.lock);
-	list_for_each_entry(dynid, &drv->dynids.list, node) {
-		if (!uuid_le_cmp(dynid->id.guid, *guid)) {
-			id = &dynid->id;
-			break;
-		}
-	}
-	spin_unlock(&drv->dynids.lock);
-
-	if (id)
-		return id;
-
-	id = drv->id_table;
-	if (id == NULL)
-		return NULL; /* empty device table */
-
-	for (; !is_null_guid(&id->guid); id++)
-		if (!uuid_le_cmp(id->guid, *guid))
+	for (; !is_null_guid(id->guid); id++)
+		if (!memcmp(&id->guid, guid, sizeof(uuid_le)))
 			return id;
 
 	return NULL;
 }
-
-/* vmbus_add_dynid - add a new device ID to this driver and re-probe devices */
-static int vmbus_add_dynid(struct hv_driver *drv, uuid_le *guid)
+#else
+static const struct hv_vmbus_device_id *hv_vmbus_get_id(
+                                        const struct hv_vmbus_device_id *id,
+                                        const uuid_le *guid)
 {
-	struct vmbus_dynid *dynid;
+        for (; !is_null_guid(&id->guid); id++)
+                if (!memcmp(&id->guid, guid, sizeof(uuid_le)))
+                        return id;
 
-	dynid = kzalloc(sizeof(*dynid), GFP_KERNEL);
-	if (!dynid)
-		return -ENOMEM;
-
-	dynid->id.guid = *guid;
-
-	spin_lock(&drv->dynids.lock);
-	list_add_tail(&dynid->node, &drv->dynids.list);
-	spin_unlock(&drv->dynids.lock);
-
-	return driver_attach(&drv->driver);
+        return NULL;
 }
 
-static void vmbus_free_dynids(struct hv_driver *drv)
-{
-	struct vmbus_dynid *dynid, *n;
-
-	spin_lock(&drv->dynids.lock);
-	list_for_each_entry_safe(dynid, n, &drv->dynids.list, node) {
-		list_del(&dynid->node);
-		kfree(dynid);
-	}
-	spin_unlock(&drv->dynids.lock);
-}
-
-/*
- * store_new_id - sysfs frontend to vmbus_add_dynid()
- *
- * Allow GUIDs to be added to an existing driver via sysfs.
- */
-static ssize_t new_id_store(struct device_driver *driver, const char *buf,
-			    size_t count)
-{
-	struct hv_driver *drv = drv_to_hv_drv(driver);
-	uuid_le guid;
-	ssize_t retval;
-
-	retval = uuid_le_to_bin(buf, &guid);
-	if (retval)
-		return retval;
-
-	if (hv_vmbus_get_id(drv, &guid))
-		return -EEXIST;
-
-	retval = vmbus_add_dynid(drv, &guid);
-	if (retval)
-		return retval;
-	return count;
-}
-static DRIVER_ATTR_WO(new_id);
-
-/*
- * store_remove_id - remove a PCI device ID from this driver
- *
- * Removes a dynamic pci device ID to this driver.
- */
-static ssize_t remove_id_store(struct device_driver *driver, const char *buf,
-			       size_t count)
-{
-	struct hv_driver *drv = drv_to_hv_drv(driver);
-	struct vmbus_dynid *dynid, *n;
-	uuid_le guid;
-	ssize_t retval;
-
-	retval = uuid_le_to_bin(buf, &guid);
-	if (retval)
-		return retval;
-
-	retval = -ENODEV;
-	spin_lock(&drv->dynids.lock);
-	list_for_each_entry_safe(dynid, n, &drv->dynids.list, node) {
-		struct hv_vmbus_device_id *id = &dynid->id;
-
-		if (!uuid_le_cmp(id->guid, guid)) {
-			list_del(&dynid->node);
-			kfree(dynid);
-			retval = count;
-			break;
-		}
-	}
-	spin_unlock(&drv->dynids.lock);
-
-	return retval;
-}
-static DRIVER_ATTR_WO(remove_id);
-
-static struct attribute *vmbus_drv_attrs[] = {
-	&driver_attr_new_id.attr,
-	&driver_attr_remove_id.attr,
-	NULL,
-};
-ATTRIBUTE_GROUPS(vmbus_drv);
-
+#endif
 
 /*
  * vmbus_match - Attempt to match the specified device to the specified driver
@@ -692,8 +595,12 @@
 	/* The hv_sock driver handles all hv_sock offers. */
 	if (is_hvsock_channel(hv_dev->channel))
 		return drv->hvsock;
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,2))
+	if (hv_vmbus_get_id(drv->id_table, hv_dev->dev_type.b))
+#else
+	if (hv_vmbus_get_id(drv->id_table, &hv_dev->dev_type))
+#endif
 
-	if (hv_vmbus_get_id(drv, &hv_dev->dev_type))
 		return 1;
 
 	return 0;
@@ -709,8 +616,11 @@
 			drv_to_hv_drv(child_device->driver);
 	struct hv_device *dev = device_to_hv_device(child_device);
 	const struct hv_vmbus_device_id *dev_id;
-
-	dev_id = hv_vmbus_get_id(drv, &dev->dev_type);
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,2))
+	dev_id = hv_vmbus_get_id(drv->id_table, dev->dev_type.b);
+#else
+	dev_id = hv_vmbus_get_id(drv->id_table, &dev->dev_type);
+#endif
 	if (drv->probe) {
 		ret = drv->probe(dev, dev_id);
 		if (ret != 0)
@@ -734,9 +644,9 @@
 	struct hv_device *dev = device_to_hv_device(child_device);
 
 	if (child_device->driver) {
-		drv = drv_to_hv_drv(child_device->driver);
-		if (drv->remove)
-			drv->remove(dev);
+ 		drv = drv_to_hv_drv(child_device->driver);
+ 		if (drv->remove)
+ 			drv->remove(dev);
 	}
 
 	return 0;
@@ -780,16 +690,19 @@
 
 /* The one and only one */
 static struct bus_type  hv_bus = {
-	.name =		"vmbus",
+	.name =			"vmbus",
 	.match =		vmbus_match,
 	.shutdown =		vmbus_shutdown,
 	.remove =		vmbus_remove,
 	.probe =		vmbus_probe,
 	.uevent =		vmbus_uevent,
-	.dev_groups =		vmbus_dev_groups,
-	.drv_groups =		vmbus_drv_groups,
+	.dev_groups =		vmbus_groups,
 };
 
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+static const char *driver_name = "hyperv";
+#endif
+
 struct onmessage_work_context {
 	struct work_struct work;
 	struct hv_message msg;
@@ -798,7 +711,6 @@
 static void vmbus_onmessage_work(struct work_struct *work)
 {
 	struct onmessage_work_context *ctx;
-
 	/* Do not process messages if we're in DISCONNECTED state */
 	if (vmbus_connection.conn_state == DISCONNECTED)
 		return;
@@ -810,9 +722,9 @@
 }
 
 static void hv_process_timer_expiration(struct hv_message *msg,
-					struct hv_per_cpu_context *hv_cpu)
+					int cpu)
 {
-	struct clock_event_device *dev = hv_cpu->clk_evt;
+	struct clock_event_device *dev = hv_context.clk_evt[cpu];
 
 	if (dev->event_handler)
 		dev->event_handler(dev);
@@ -845,11 +757,12 @@
 	}
 
 	entry = &channel_message_table[hdr->msgtype];
-	if (entry->handler_type	== VMHT_BLOCKING) {
+	if (entry->handler_type == VMHT_BLOCKING) {
 		ctx = kmalloc(sizeof(*ctx), GFP_ATOMIC);
 		if (ctx == NULL)
 			return;
 
+
 		INIT_WORK(&ctx->work, vmbus_onmessage_work);
 		memcpy(&ctx->msg, msg, sizeof(*msg));
 
@@ -886,7 +799,6 @@
 	vmbus_signal_eom(msg, message_type);
 }
 
-
 /*
  * Direct callback for channels using other deferred processing
  */
@@ -950,8 +862,6 @@
 
 			trace_vmbus_chan_sched(channel);
 
-			++channel->interrupts;
-
 			switch (channel->callback_mode) {
 			case HV_CALL_ISR:
 				vmbus_channel_isr(channel);
@@ -969,8 +879,13 @@
 	}
 }
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)) /* KYS; we may have to tweak this */
 static void vmbus_isr(void)
+#else
+static irqreturn_t vmbus_isr(int irq, void *dev_id)
+#endif
 {
+	int cpu = smp_processor_id();
 	struct hv_per_cpu_context *hv_cpu
 		= this_cpu_ptr(hv_context.cpu_context);
 	void *page_addr = hv_cpu->synic_event_page;
@@ -979,7 +894,11 @@
 	bool handled = false;
 
 	if (unlikely(page_addr == NULL))
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)) /* KYS; we may have to tweak this */
 		return;
+#else
+		return IRQ_NONE;
+#endif
 
 	event = (union hv_synic_event_flags *)page_addr +
 					 VMBUS_MESSAGE_SINT;
@@ -1014,14 +933,26 @@
 	/* Check if there are actual msgs to be processed */
 	if (msg->header.message_type != HVMSG_NONE) {
 		if (msg->header.message_type == HVMSG_TIMER_EXPIRED)
-			hv_process_timer_expiration(msg, hv_cpu);
+			hv_process_timer_expiration(msg, cpu);
 		else
 			tasklet_schedule(&hv_cpu->msg_dpc);
 	}
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4)) /* we dont have add_interrupt_randomness symbol in kernel yet in 7.3 */
+        add_interrupt_randomness(HYPERVISOR_CALLBACK_VECTOR, 0);
+#endif
+
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)) /* KYS; we may have to tweak this */
+	return;
+#else
+	if (handled)
+		return IRQ_HANDLED;
+	else
+		return IRQ_NONE;
 
-	add_interrupt_randomness(HYPERVISOR_CALLBACK_VECTOR, 0);
+#endif
 }
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
 static void hv_synic_init_oncpu(void *arg)
 {
 	int cpu = get_cpu();
@@ -1039,32 +970,79 @@
 static int hv_cpuhp_callback(struct notifier_block *nfb,
 			     unsigned long action, void *hcpu)
 {
-	unsigned int cpu = (unsigned long) hcpu;
+	unsigned long cpu = (unsigned long) hcpu;
 
 	switch (action & ~CPU_TASKS_FROZEN) {
-	case CPU_STARTING:
-		hv_synic_init(cpu);
-		break;
-	case CPU_DYING:
-		hv_synic_cleanup(cpu);
-		break;
-	case CPU_DOWN_PREPARE:
-		if (hv_synic_cpu_used(cpu))
-			return NOTIFY_BAD;
-		hv_clockevents_unbind(cpu);
-		break;
-	case CPU_DOWN_FAILED:
-		hv_clockevents_bind(cpu);
-		break;
-	}
-
+		case CPU_STARTING:
+			hv_synic_init(cpu);
+			break;
+		case CPU_DYING:
+			hv_synic_cleanup(cpu);
+			break;
+		case CPU_DOWN_PREPARE:		
+			if (hv_synic_cpu_used(cpu))
+				return NOTIFY_BAD;
+			hv_clockevents_unbind(cpu);
+			break;
+		case CPU_DOWN_FAILED:
+			hv_clockevents_bind(cpu);
+			break;
+		}
 	return NOTIFY_OK;
 }
 
 static struct notifier_block hv_cpuhp_notifier __refdata = {
-       .notifier_call = hv_cpuhp_callback,
-       .priority = INT_MAX - 1, /* Run after hv_cpu_init() */
+	.notifier_call = hv_cpuhp_callback,
+	.priority = INT_MAX,
 };
+#else
+#ifdef CONFIG_HOTPLUG_CPU
+static int hyperv_cpu_disable(void)
+{
+	return -ENOSYS;
+}
+
+static void hv_cpu_hotplug_quirk(bool vmbus_loaded)
+{
+	static void *previous_cpu_disable;
+
+	/*
+	 * Offlining a CPU when running on newer hypervisors (WS2012R2, Win8,
+	 * ...) is not supported at this moment as channel interrupts are
+	 * distributed across all of them.
+	 */
+
+	if ((vmbus_proto_version == VERSION_WS2008) ||
+	    (vmbus_proto_version == VERSION_WIN7))
+		return;
+
+	if (vmbus_loaded) {
+		previous_cpu_disable = smp_ops.cpu_disable;
+		smp_ops.cpu_disable = hyperv_cpu_disable;
+		pr_notice("CPU offlining is not supported by hypervisor\n");
+	} else if (previous_cpu_disable)
+		smp_ops.cpu_disable = previous_cpu_disable;
+}
+#else
+static void hv_cpu_hotplug_quirk(bool vmbus_loaded)
+{
+}
+#endif
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+/*
+ * vmbus interrupt flow handler:
+ * vmbus interrupts can concurrently occur on multiple CPUs and
+ * can be handled concurrently.
+ */
+static void vmbus_flow_handler(unsigned int irq, struct irq_desc *desc)
+{
+	kstat_incr_irqs_this_cpu(irq, desc);
+
+	desc->action->handler(irq, desc->action->dev_id);
+}
+#endif
 
 /*
  * vmbus_bus_init -Main vmbus driver initialization routine.
@@ -1072,11 +1050,20 @@
  * Here, we
  *	- initialize the vmbus driver context
  *	- invoke the vmbus hv main init routine
+ *	- get the irq resource
  *	- retrieve the channel offers
  */
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2))
 static int vmbus_bus_init(void)
+#else
+static int vmbus_bus_init(int irq)
+#endif
 {
-	int ret, cpu;
+	int ret;
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+	int cpu;
+#endif
+
 
 	/* Hypervisor initialization...setup hypercall page..etc */
 	ret = hv_init();
@@ -1089,8 +1076,26 @@
 	if (ret)
 		return ret;
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)) /* KYS; we may have to tweak this */
 	hv_setup_vmbus_irq(vmbus_isr);
 
+#else
+	ret = request_irq(irq, vmbus_isr, 0, driver_name, hv_acpi_dev);
+	if (ret != 0) {
+		pr_err("Unable to request IRQ %d\n", irq);
+		goto err_unregister;
+	}
+
+	/*
+	 * Vmbus interrupts can be handled concurrently on
+	 * different CPUs. Establish an appropriate interrupt flow
+	 * handler that can support this model.
+	 */
+	irq_set_handler(irq, vmbus_flow_handler);
+
+	hv_register_vmbus_handler(irq, vmbus_isr);
+#endif
+
 	ret = hv_synic_alloc();
 	if (ret)
 		goto err_alloc;
@@ -1098,29 +1103,37 @@
 	 * Initialize the per-cpu interrupt state and
 	 * connect to the host.
 	 */
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3)) 
 	cpu_notifier_register_begin();
 	on_each_cpu(hv_synic_init_oncpu, NULL, 1);
 	__register_hotcpu_notifier(&hv_cpuhp_notifier);
 	cpu_notifier_register_done();
+#else
+	on_each_cpu(hv_synic_init, NULL, 1);
+#endif
 
 	ret = vmbus_connect();
 	if (ret)
 		goto err_connect;
 
 	/*
-	 * Only register if the crash MSRs are available
-	 */
-	if (ms_hyperv.misc_features & HV_FEATURE_GUEST_CRASH_MSR_AVAILABLE) {
+         * Only register if the crash MSRs are available
+         */
+	if (ms_hyperv_ext.misc_features & HV_FEATURE_GUEST_CRASH_MSR_AVAILABLE) {
 		register_die_notifier(&hyperv_die_block);
-		atomic_notifier_chain_register(&panic_notifier_list,
-					       &hyperv_panic_block);
-	}
-
+                atomic_notifier_chain_register(&panic_notifier_list,
+                                               &hyperv_panic_block);
+        }
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+	hv_cpu_hotplug_quirk(true);
+#endif
 	vmbus_request_offers();
 
 	return 0;
 
 err_connect:
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
 	cpu_notifier_register_begin();
 	__unregister_hotcpu_notifier(&hv_cpuhp_notifier);
 	for_each_online_cpu(cpu) {
@@ -1128,10 +1141,21 @@
 		smp_call_function_single(cpu, hv_synic_cleanup_oncpu, NULL, 1);
 	}
 	cpu_notifier_register_done();
+#else
+	on_each_cpu(hv_synic_cleanup, NULL, 1);
+#endif
+
 err_alloc:
 	hv_synic_free();
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,2)) /* KYS; we may have to tweak this */
 	hv_remove_vmbus_irq();
-
+#else
+	free_irq(irq, hv_acpi_dev);
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+err_unregister:
+#endif
 	bus_unregister(&hv_bus);
 
 	return ret;
@@ -1163,9 +1187,6 @@
 	hv_driver->driver.mod_name = mod_name;
 	hv_driver->driver.bus = &hv_bus;
 
-	spin_lock_init(&hv_driver->dynids.lock);
-	INIT_LIST_HEAD(&hv_driver->dynids.list);
-
 	ret = driver_register(&hv_driver->driver);
 
 	return ret;
@@ -1184,10 +1205,8 @@
 {
 	pr_info("unregistering driver %s\n", hv_driver->name);
 
-	if (!vmbus_exists()) {
+	if (!vmbus_exists())
 		driver_unregister(&hv_driver->driver);
-		vmbus_free_dynids(hv_driver);
-	}
 }
 EXPORT_SYMBOL_GPL(vmbus_driver_unregister);
 
@@ -1230,9 +1249,6 @@
 	if (!attribute->show)
 		return -EIO;
 
-	if (chan->state != CHANNEL_OPENED_STATE)
-		return -EINVAL;
-
 	return attribute->show(chan, buf);
 }
 
@@ -1296,18 +1312,6 @@
 }
 static VMBUS_CHAN_ATTR(latency, S_IRUGO, channel_latency_show, NULL);
 
-static ssize_t channel_interrupts_show(const struct vmbus_channel *channel, char *buf)
-{
-	return sprintf(buf, "%llu\n", channel->interrupts);
-}
-static VMBUS_CHAN_ATTR(interrupts, S_IRUGO, channel_interrupts_show, NULL);
-
-static ssize_t channel_events_show(const struct vmbus_channel *channel, char *buf)
-{
-	return sprintf(buf, "%llu\n", channel->sig_events);
-}
-static VMBUS_CHAN_ATTR(events, S_IRUGO, channel_events_show, NULL);
-
 static ssize_t subchannel_monitor_id_show(const struct vmbus_channel *channel,
 					  char *buf)
 {
@@ -1331,8 +1335,6 @@
 	&chan_attr_cpu.attr,
 	&chan_attr_pending.attr,
 	&chan_attr_latency.attr,
-	&chan_attr_interrupts.attr,
-	&chan_attr_events.attr,
 	&chan_attr_monitor_id.attr,
 	&chan_attr_subchannel_id.attr,
 	NULL
@@ -1386,7 +1388,6 @@
 	       sizeof(uuid_le));
 	child_device_obj->vendor_id = 0x1414; /* MSFT vendor ID */
 
-
 	return child_device_obj;
 }
 
@@ -1400,7 +1401,6 @@
 
 	dev_set_name(&child_device_obj->device, "%pUl",
 		     child_device_obj->channel->offermsg.offer.if_instance.b);
-
 	child_device_obj->device.bus = &hv_bus;
 	child_device_obj->device.parent = &hv_acpi_dev->dev;
 	child_device_obj->device.release = vmbus_device_release;
@@ -1448,6 +1448,16 @@
 	pr_debug("child device %s unregistered\n",
 		dev_name(&device_obj->device));
 
+/*
+ * kset_unregister() in RHEL 7.4 and older lacks this patch:
+ * https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=35a5fe695b07ae899510ad76fdf0aeaef85fe951
+ * So we have to manually add the kobject_del() to properly decrease the
+ * device refcnt, otherwise the device can't be thoroughly destroyed.
+ */
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,4))
+	kobject_del(&device_obj->channels_kset->kobj);
+#endif
+
 	kset_unregister(device_obj->channels_kset);
 
 	/*
@@ -1472,20 +1482,40 @@
 	struct resource **prev_res = NULL;
 
 	switch (res->type) {
-
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+	case ACPI_RESOURCE_TYPE_IRQ:
+		irq = res->data.irq.interrupts[0];
+		return AE_OK;
+#endif
 	/*
 	 * "Address" descriptors are for bus windows. Ignore
 	 * "memory" descriptors, which are for registers on
 	 * devices.
 	 */
 	case ACPI_RESOURCE_TYPE_ADDRESS32:
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+		start = res->data.address32.minimum;
+#else
 		start = res->data.address32.address.minimum;
+#endif
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+		end = res->data.address32.maximum;
+#else
 		end = res->data.address32.address.maximum;
+#endif
 		break;
 
 	case ACPI_RESOURCE_TYPE_ADDRESS64:
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+		start = res->data.address64.minimum;
+#else
 		start = res->data.address64.address.minimum;
+#endif
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+		end = res->data.address64.maximum;
+#else
 		end = res->data.address64.address.maximum;
+#endif
 		break;
 
 	default:
@@ -1514,7 +1544,7 @@
 	new_res->end = end;
 
 	/*
-	 * If two ranges are adjacent, merge them.
+	 * If two ranges are adjacent, merget them.
 	 */
 	do {
 		if (!*old_res) {
@@ -1657,6 +1687,7 @@
 
 		range_min = iter->start;
 		range_max = iter->end;
+
 		start = (range_min + align - 1) & ~(align - 1);
 		for (; start + size - 1 <= range_max; start += align) {
 			shadow = __request_region(iter, start, size, NULL,
@@ -1670,7 +1701,6 @@
 				retval = 0;
 				goto exit;
 			}
-
 			__release_region(iter, start, size);
 		}
 	}
@@ -1702,7 +1732,6 @@
 	}
 	release_mem_region(start, size);
 	up(&hyperv_mmio_lock);
-
 }
 EXPORT_SYMBOL_GPL(vmbus_free_mmio);
 
@@ -1759,47 +1788,46 @@
 	},
 };
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
 static void hv_kexec_handler(void)
 {
 	int cpu;
 
 	hv_synic_clockevents_cleanup();
 	vmbus_initiate_unload(false);
-	vmbus_connection.conn_state = DISCONNECTED;
-	/* Make sure conn_state is set as hv_synic_cleanup checks for it */
-	mb();
 	for_each_online_cpu(cpu)
 		smp_call_function_single(cpu, hv_synic_cleanup_oncpu, NULL, 1);
 	hyperv_cleanup();
 };
+#endif
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
 static void hv_crash_handler(struct pt_regs *regs)
 {
-	int cpu = smp_processor_id();
-
 	vmbus_initiate_unload(true);
 	/*
 	 * In crash handler we can't schedule synic cleanup for all CPUs,
 	 * doing the cleanup for current CPU only. This should be sufficient
 	 * for kdump.
 	 */
-	vmbus_connection.conn_state = DISCONNECTED;
-	hv_clockevents_unbind(cpu);
-	hv_synic_cleanup(cpu);
+	hv_synic_cleanup(smp_processor_id());
 	hyperv_cleanup();
 };
+#endif
 
 static int __init hv_acpi_init(void)
 {
 	int ret, t;
 
-	if (!hv_is_hyperv_initialized())
+	if (x86_hyper != &x86_hyper_ms_hyperv)
 		return -ENODEV;
 
+	init_ms_hyperv_ext();
+
 	init_completion(&probe_event);
 
 	/*
-	 * Get ACPI resources first.
+	 * Get ACPI/irq resources first.
 	 */
 	ret = acpi_bus_register_driver(&vmbus_acpi_driver);
 
@@ -1812,13 +1840,24 @@
 		goto cleanup;
 	}
 
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+	if (irq <= 0) {
+		ret = -ENODEV;
+		goto cleanup;
+	}
+
+	ret = vmbus_bus_init(irq);
+#else
 	ret = vmbus_bus_init();
+#endif
 	if (ret)
 		goto cleanup;
-
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
 	hv_setup_kexec_handler(hv_kexec_handler);
 	hv_setup_crash_handler(hv_crash_handler);
+#endif
 
+	pr_info("vmbus channel cpu affinity mode: %d\n", affinity_mode);
 	return 0;
 
 cleanup:
@@ -1830,13 +1869,16 @@
 static void __exit vmbus_exit(void)
 {
 	int cpu;
-
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
 	hv_remove_kexec_handler();
 	hv_remove_crash_handler();
+#endif
 	vmbus_connection.conn_state = DISCONNECTED;
-	hv_synic_clockevents_cleanup();
+//	hv_synic_clockevents_cleanup();  will comment this for time being till clockevents_unbind showed up in distro code
 	vmbus_disconnect();
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(8,0)) /* KYS; we may have to tweak this */
 	hv_remove_vmbus_irq();
+#endif
 	for_each_online_cpu(cpu) {
 		struct hv_per_cpu_context *hv_cpu
 			= per_cpu_ptr(hv_context.cpu_context, cpu);
@@ -1845,25 +1887,31 @@
 	}
 	vmbus_free_channels();
 
-	if (ms_hyperv.misc_features & HV_FEATURE_GUEST_CRASH_MSR_AVAILABLE) {
+        if (ms_hyperv_ext.misc_features & HV_FEATURE_GUEST_CRASH_MSR_AVAILABLE) {
 		unregister_die_notifier(&hyperv_die_block);
 		atomic_notifier_chain_unregister(&panic_notifier_list,
 						 &hyperv_panic_block);
 	}
 	bus_unregister(&hv_bus);
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
 	cpu_notifier_register_begin();
 	__unregister_hotcpu_notifier(&hv_cpuhp_notifier);
-
-	for_each_online_cpu(cpu)
+	for_each_online_cpu(cpu) {
 		smp_call_function_single(cpu, hv_synic_cleanup_oncpu, NULL, 1);
-
+	}
 	cpu_notifier_register_done();
+#else
+	for_each_online_cpu(cpu)
+		smp_call_function_single(cpu, hv_synic_cleanup, NULL, 1);
+	hv_cpu_hotplug_quirk(false);
+#endif
 	hv_synic_free();
 	acpi_bus_unregister_driver(&vmbus_acpi_driver);
 }
 
 
 MODULE_LICENSE("GPL");
+MODULE_VERSION(HV_DRV_VERSION);
 
 subsys_initcall(hv_acpi_init);
 module_exit(vmbus_exit);
diff -Naur linux-3.10.0-957.el7.orig/drivers/input/serio/hyperv-keyboard.c linux-3.10.0-957.el7.lis/drivers/input/serio/hyperv-keyboard.c
--- linux-3.10.0-957.el7.orig/drivers/input/serio/hyperv-keyboard.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/input/serio/hyperv-keyboard.c	2018-12-12 00:51:40.902784058 +0000
@@ -438,5 +438,7 @@
 }
 
 MODULE_LICENSE("GPL");
+MODULE_VERSION(HV_DRV_VERSION);
+
 module_init(hv_kbd_init);
 module_exit(hv_kbd_exit);
diff -Naur linux-3.10.0-957.el7.orig/drivers/net/hyperv/hyperv_net.h linux-3.10.0-957.el7.lis/drivers/net/hyperv/hyperv_net.h
--- linux-3.10.0-957.el7.orig/drivers/net/hyperv/hyperv_net.h	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/net/hyperv/hyperv_net.h	2018-12-12 00:51:40.830784516 +0000
@@ -12,8 +12,7 @@
  * more details.
  *
  * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
- * Place - Suite 330, Boston, MA 02111-1307 USA.
+ * this program; if not, see <http://www.gnu.org/licenses/>.
  *
  * Authors:
  *   Haiyang Zhang <haiyangz@microsoft.com>
@@ -29,6 +28,8 @@
 #include <linux/hyperv.h>
 #include <linux/rndis.h>
 
+#include "netvsc_compat.h"
+
 /* RSS related */
 #define OID_GEN_RECEIVE_SCALE_CAPABILITIES 0x00010203  /* query only */
 #define OID_GEN_RECEIVE_SCALE_PARAMETERS 0x00010204  /* query and set */
@@ -86,6 +87,8 @@
 #define NDIS_RSS_HASH_SECRET_KEY_MAX_SIZE_REVISION_2   40
 
 #define ITAB_NUM 128
+#define HASH_KEYLEN NDIS_RSS_HASH_SECRET_KEY_MAX_SIZE_REVISION_2
+extern u8 netvsc_hash_key[];
 
 struct ndis_recv_scale_param { /* NDIS_RECEIVE_SCALE_PARAMETERS */
 	struct ndis_obj_header hdr;
@@ -131,23 +134,24 @@
  */
 struct hv_netvsc_packet {
 	/* Bookkeeping stuff */
-	u8 cp_partial; /* partial copy into send buffer */
 
+	u8 xmit_more; /* from skb */
+	u8 cp_partial; /* partial copy into send buffer */	
 	u8 rmsg_size; /* RNDIS header and PPI size */
 	u8 rmsg_pgcnt; /* page count of RNDIS header and PPI */
 	u8 page_buf_cnt;
 
-	u16 q_idx;
+ 	u16 q_idx;
 	u16 total_packets;
-
 	u32 total_bytes;
 	u32 send_buf_index;
 	u32 total_data_buflen;
+	void *send_completion_ctx;
 };
 
 struct netvsc_device_info {
 	unsigned char mac_adr[ETH_ALEN];
-	u32  num_chn;
+	u32 num_chn;
 	u32  send_sections;
 	u32  recv_sections;
 	u32  send_section_size;
@@ -167,7 +171,6 @@
 	struct net_device *ndev;
 
 	enum rndis_device_state state;
-
 	atomic_t new_req_id;
 
 	spinlock_t request_lock;
@@ -191,6 +194,12 @@
 
 extern u32 netvsc_ring_bytes;
 
+#if (RHEL_RELEASE_CODE == RHEL_RELEASE_VERSION(7,0))
+extern u32 netvsc_ring_reciprocal;
+#else
+extern struct reciprocal_value netvsc_ring_reciprocal;
+#endif
+
 struct netvsc_device *netvsc_device_add(struct hv_device *device,
 					const struct netvsc_device_info *info);
 int netvsc_alloc_recv_comp_ring(struct netvsc_device *net_device, u32 q_idx);
@@ -211,7 +220,7 @@
 void netvsc_channel_cb(void *context);
 int netvsc_poll(struct napi_struct *napi, int budget);
 
-void rndis_set_subchannel(struct work_struct *w);
+int rndis_set_subchannel(struct net_device *ndev, struct netvsc_device *nvdev);
 int rndis_filter_open(struct netvsc_device *nvdev);
 int rndis_filter_close(struct netvsc_device *nvdev);
 struct netvsc_device *rndis_filter_device_add(struct hv_device *dev,
@@ -634,8 +643,8 @@
 } __packed;
 
 
-#define NETVSC_MTU 65535
-#define NETVSC_MTU_MIN ETH_MIN_MTU
+#define NETVSC_MTU 65536
+#define NETVSC_MTU_MIN 68
 
 /* Max buffer sizes allowed by a host */
 #define NETVSC_RECEIVE_BUFFER_SIZE		(1024 * 1024 * 31) /* 31MB */
@@ -724,20 +733,14 @@
 	u32 event;
 };
 
-/* L4 hash bits for different protocols */
-#define HV_TCP4_L4HASH 1
-#define HV_TCP6_L4HASH 2
-#define HV_UDP4_L4HASH 4
-#define HV_UDP6_L4HASH 8
-#define HV_DEFAULT_L4HASH (HV_TCP4_L4HASH | HV_TCP6_L4HASH | HV_UDP4_L4HASH | \
-			   HV_UDP6_L4HASH)
-
-/* The context of the netvsc device  */
+/* The context of the netvsc device */
 struct net_device_context {
 	/* point back to our device context */
 	struct hv_device *device_ctx;
 	/* netvsc_device */
 	struct netvsc_device __rcu *nvdev;
+	/* list of netvsc net_devices */
+	struct list_head list;
 	/* reconfigure work */
 	struct delayed_work dwork;
 	/* last reconfig time */
@@ -754,9 +757,10 @@
 	u32 tx_table[VRSS_SEND_TAB_SIZE];
 
 	/* Ethtool settings */
+	bool udp4_l4_hash;
+	bool udp6_l4_hash;
 	u8 duplex;
 	u32 speed;
-	u32 l4_hash; /* L4 hash settings */
 	struct netvsc_ethtool_stats eth_stats;
 
 	/* State to manage the associated VF interface. */
@@ -779,7 +783,6 @@
 	struct multi_send_data msd;
 	struct multi_recv_comp mrc;
 	atomic_t queue_sends;
-
 	struct netvsc_stats tx_stats;
 	struct netvsc_stats rx_stats;
 };
@@ -793,7 +796,6 @@
 
 	/* Receive buffer allocated by us but manages by NetVSP */
 	void *recv_buf;
-	u32 recv_buf_size; /* allocated bytes */
 	u32 recv_buf_gpadl_handle;
 	u32 recv_section_cnt;
 	u32 recv_section_size;
@@ -813,7 +815,7 @@
 	struct nvsp_message revoke_packet;
 
 	u32 max_chn;
-	u32 num_chn;
+	u32 num_chn;	
 
 	atomic_t open_chn;
 	struct work_struct subchan_work;
@@ -825,7 +827,6 @@
 	u32 pkt_align; /* alignment bytes, e.g. 8 */
 
 	struct netvsc_channel chan_table[VRSS_CHANNEL_MAX];
-
 	struct rcu_head rcu;
 };
 
@@ -1256,7 +1257,7 @@
 
 /* Total size of all PPI data */
 #define NDIS_ALL_PPI_SIZE (NDIS_VLAN_PPI_SIZE + NDIS_CSUM_PPI_SIZE + \
-			   NDIS_LSO_PPI_SIZE + NDIS_HASH_PPI_SIZE)
+		NDIS_LSO_PPI_SIZE + NDIS_HASH_PPI_SIZE)
 
 /* Format of Information buffer passed in a SetRequest for the OID */
 /* OID_GEN_RNDIS_CONFIG_PARAMETER. */
diff -Naur linux-3.10.0-957.el7.orig/drivers/net/hyperv/netvsc.c linux-3.10.0-957.el7.lis/drivers/net/hyperv/netvsc.c
--- linux-3.10.0-957.el7.orig/drivers/net/hyperv/netvsc.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/net/hyperv/netvsc.c	2018-12-12 00:51:40.839784459 +0000
@@ -11,8 +11,7 @@
  * more details.
  *
  * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
- * Place - Suite 330, Boston, MA 02111-1307 USA.
+ * this program; if not, see <http://www.gnu.org/licenses/>.
  *
  * Authors:
  *   Haiyang Zhang <haiyangz@microsoft.com>
@@ -29,11 +28,10 @@
 #include <linux/slab.h>
 #include <linux/netdevice.h>
 #include <linux/if_ether.h>
-#include <linux/vmalloc.h>
+#include <asm/sync_bitops.h>
 #include <linux/rtnetlink.h>
 #include <linux/prefetch.h>
-
-#include <asm/sync_bitops.h>
+#include <linux/reciprocal_div.h>
 
 #include "hyperv_net.h"
 
@@ -63,6 +61,41 @@
 			       VM_PKT_DATA_INBAND, 0);
 }
 
+/* Worker to setup sub channels on initial setup
+ * Initial hotplug event occurs in softirq context
+ * and can't wait for channels.
+ */
+static void netvsc_subchan_work(struct work_struct *w)
+{
+	struct netvsc_device *nvdev =
+		container_of(w, struct netvsc_device, subchan_work);
+	struct rndis_device *rdev;
+	int i, ret;
+
+	/* Avoid deadlock with device removal already under RTNL */
+	if (!rtnl_trylock()) {
+		schedule_work(w);
+		return;
+	}
+
+	rdev = nvdev->extension;
+	if (rdev) {
+		ret = rndis_set_subchannel(rdev->ndev, nvdev);
+		if (ret == 0) {
+			netif_device_attach(rdev->ndev);
+		} else {
+			/* fallback to only primary channel */
+			for (i = 1; i < nvdev->num_chn; i++)
+				netif_napi_del(&nvdev->chan_table[i].napi);
+
+			nvdev->max_chn = 1;
+			nvdev->num_chn = 1;
+		}
+	}
+
+	rtnl_unlock();
+}
+
 static struct netvsc_device *alloc_net_device(void)
 {
 	struct netvsc_device *net_device;
@@ -79,7 +112,7 @@
 
 	init_completion(&net_device->channel_init_wait);
 	init_waitqueue_head(&net_device->subchan_open);
-	INIT_WORK(&net_device->subchan_work, rndis_set_subchannel);
+	INIT_WORK(&net_device->subchan_work, netvsc_subchan_work);
 
 	return net_device;
 }
@@ -90,11 +123,6 @@
 		= container_of(head, struct netvsc_device, rcu);
 	int i;
 
-	kfree(nvdev->extension);
-	vfree(nvdev->recv_buf);
-	vfree(nvdev->send_buf);
-	kfree(nvdev->send_section_map);
-
 	for (i = 0; i < VRSS_CHANNEL_MAX; i++)
 		vfree(nvdev->chan_table[i].mrc.slots);
 
@@ -106,11 +134,11 @@
 	call_rcu(&nvdev->rcu, free_netvsc_device);
 }
 
-static void netvsc_revoke_recv_buf(struct hv_device *device,
-				   struct netvsc_device *net_device,
-				   struct net_device *ndev)
+static void netvsc_revoke_buf(struct hv_device *device,
+			      struct netvsc_device *net_device)
 {
 	struct nvsp_message *revoke_packet;
+	struct net_device *ndev = hv_get_drvdata(device);
 	int ret;
 
 	/*
@@ -134,6 +162,7 @@
 				       sizeof(struct nvsp_message),
 				       (unsigned long)revoke_packet,
 				       VM_PKT_DATA_INBAND, 0);
+
 		/* If the failure is because the channel is rescinded;
 		 * ignore the failure since we cannot send on a rescinded
 		 * channel. This would allow us to properly cleanup
@@ -141,6 +170,7 @@
 		 */
 		if (device->channel->rescind)
 			ret = 0;
+
 		/*
 		 * If we failed here, we might as well return and
 		 * have a leak rather than continue and a bugchk
@@ -152,14 +182,6 @@
 		}
 		net_device->recv_section_cnt = 0;
 	}
-}
-
-static void netvsc_revoke_send_buf(struct hv_device *device,
-				   struct netvsc_device *net_device,
-				   struct net_device *ndev)
-{
-	struct nvsp_message *revoke_packet;
-	int ret;
 
 	/* Deal with the send buffer we may have setup.
 	 * If we got a  send section size, it means we received a
@@ -203,10 +225,10 @@
 	}
 }
 
-static void netvsc_teardown_recv_gpadl(struct hv_device *device,
-				       struct netvsc_device *net_device,
-				       struct net_device *ndev)
+static void netvsc_teardown_gpadl(struct hv_device *device,
+				  struct netvsc_device *net_device)
 {
+	struct net_device *ndev = hv_get_drvdata(device);
 	int ret;
 
 	if (net_device->recv_buf_gpadl_handle) {
@@ -223,13 +245,12 @@
 		}
 		net_device->recv_buf_gpadl_handle = 0;
 	}
-}
 
-static void netvsc_teardown_send_gpadl(struct hv_device *device,
-				       struct netvsc_device *net_device,
-				       struct net_device *ndev)
-{
-	int ret;
+	if (net_device->recv_buf) {
+		/* Free up the receive buffer */
+		vfree(net_device->recv_buf);
+		net_device->recv_buf = NULL;
+	}
 
 	if (net_device->send_buf_gpadl_handle) {
 		ret = vmbus_teardown_gpadl(device->channel,
@@ -245,6 +266,12 @@
 		}
 		net_device->send_buf_gpadl_handle = 0;
 	}
+	if (net_device->send_buf) {
+		/* Free up the send buffer */
+		vfree(net_device->send_buf);
+		net_device->send_buf = NULL;
+	}
+	kfree(net_device->send_section_map);
 }
 
 int netvsc_alloc_recv_comp_ring(struct netvsc_device *net_device, u32 q_idx)
@@ -290,8 +317,6 @@
 		goto cleanup;
 	}
 
-	net_device->recv_buf_size = buf_size;
-
 	/*
 	 * Establish the gpadl handle for this buffer on this
 	 * channel.  Note: This call uses the vmbus connection rather
@@ -441,10 +466,8 @@
 	goto exit;
 
 cleanup:
-	netvsc_revoke_recv_buf(device, net_device, ndev);
-	netvsc_revoke_send_buf(device, net_device, ndev);
-	netvsc_teardown_recv_gpadl(device, net_device, ndev);
-	netvsc_teardown_send_gpadl(device, net_device, ndev);
+	netvsc_revoke_buf(device, net_device);
+	netvsc_teardown_gpadl(device, net_device);
 
 exit:
 	return ret;
@@ -574,24 +597,10 @@
 		= rtnl_dereference(net_device_ctx->nvdev);
 	int i;
 
-	/*
-	 * Revoke receive buffer. If host is pre-Win2016 then tear down
-	 * receive buffer GPADL. Do the same for send buffer.
-	 */
-	netvsc_revoke_recv_buf(device, net_device, ndev);
-	if (vmbus_proto_version < VERSION_WIN10)
-		netvsc_teardown_recv_gpadl(device, net_device, ndev);
-
-	netvsc_revoke_send_buf(device, net_device, ndev);
-	if (vmbus_proto_version < VERSION_WIN10)
-		netvsc_teardown_send_gpadl(device, net_device, ndev);
+	netvsc_revoke_buf(device, net_device);
 
 	RCU_INIT_POINTER(net_device_ctx->nvdev, NULL);
 
-	/* And disassociate NAPI context from device */
-	for (i = 0; i < net_device->num_chn; i++)
-		netif_napi_del(&net_device->chan_table[i].napi);
-
 	/*
 	 * At this point, no one should be accessing net_device
 	 * except in here
@@ -601,14 +610,11 @@
 	/* Now, we can close the channel safely */
 	vmbus_close(device->channel);
 
-	/*
-	 * If host is Win2016 or higher then we do the GPADL tear down
-	 * here after VMBus is closed.
-	*/
-	if (vmbus_proto_version >= VERSION_WIN10) {
-		netvsc_teardown_recv_gpadl(device, net_device, ndev);
-		netvsc_teardown_send_gpadl(device, net_device, ndev);
-	}
+	netvsc_teardown_gpadl(device, net_device);
+
+	/* And dissassociate NAPI context from device */
+	for (i = 0; i < net_device->num_chn; i++)
+		netif_napi_del(&net_device->chan_table[i].napi);
 
 	/* Release all resources */
 	free_netvsc_device_rcu(net_device);
@@ -617,6 +623,16 @@
 #define RING_AVAIL_PERCENT_HIWATER 20
 #define RING_AVAIL_PERCENT_LOWATER 10
 
+/*
+ * Get the percentage of available bytes to write in the ring.
+ * The return value is in range from 0 to 100.
+ */
+static u32 hv_ringbuf_avail_percent(const struct hv_ring_buffer_info *ring_info)
+{
+    u32 avail_write = hv_get_bytes_to_write(ring_info);
+    return reciprocal_divide(avail_write * 100, netvsc_ring_reciprocal);
+}
+
 static inline void netvsc_free_send_slot(struct netvsc_device *net_device,
 					 u32 index)
 {
@@ -648,12 +664,17 @@
 		q_idx = packet->q_idx;
 		channel = incoming_channel;
 
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,0))
 		tx_stats = &net_device->chan_table[q_idx].tx_stats;
 
 		u64_stats_update_begin(&tx_stats->syncp);
 		tx_stats->packets += packet->total_packets;
 		tx_stats->bytes += packet->total_bytes;
 		u64_stats_update_end(&tx_stats->syncp);
+#else
+		ndev->stats.tx_bytes += packet->total_bytes;
+		ndev->stats.tx_packets += packet->total_packets;
+#endif 
 
 		napi_consume_skb(skb, budget);
 	}
@@ -668,16 +689,17 @@
 		struct netdev_queue *txq = netdev_get_tx_queue(ndev, q_idx);
 
 		if (netif_tx_queue_stopped(txq) &&
-		    (hv_get_avail_to_write_percent(&channel->outbound) >
-		     RING_AVAIL_PERCENT_HIWATER || queue_sends < 1)) {
+		    (hv_ringbuf_avail_percent(&channel->outbound) > RING_AVAIL_PERCENT_HIWATER ||
+		     queue_sends < 1)) {
 			netif_tx_wake_queue(txq);
 			ndev_ctx->eth_stats.wake_queue++;
 		}
+
 	}
 }
 
 static void netvsc_send_completion(struct netvsc_device *net_device,
-				   struct vmbus_channel *incoming_channel,
+                                   struct vmbus_channel *incoming_channel,
 				   struct hv_device *device,
 				   const struct vmpacket_descriptor *desc,
 				   int budget)
@@ -717,7 +739,6 @@
 		if (sync_test_and_set_bit(i, map_addr) == 0)
 			return i;
 	}
-
 	return NETVSC_INVALID_INDEX;
 }
 
@@ -727,7 +748,11 @@
 				    struct hv_netvsc_packet *packet,
 				    struct rndis_message *rndis_msg,
 				    struct hv_page_buffer *pb,
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
 				    bool xmit_more)
+#else
+				    struct sk_buff *skb)
+#endif
 {
 	char *start = net_device->send_buf;
 	char *dest = start + (section_index * net_device->send_section_size)
@@ -740,7 +765,11 @@
 
 	/* Add padding */
 	remain = packet->total_data_buflen & (net_device->pkt_align - 1);
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
 	if (xmit_more && remain) {
+#else
+	if (skb && packet->xmit_more && remain && !packet->cp_partial) {
+#endif
 		padding = net_device->pkt_align - remain;
 		rndis_msg->msg_len += padding;
 		packet->total_data_buflen += padding;
@@ -777,7 +806,7 @@
 	struct netdev_queue *txq = netdev_get_tx_queue(ndev, packet->q_idx);
 	u64 req_id;
 	int ret;
-	u32 ring_avail = hv_get_avail_to_write_percent(&out_channel->outbound);
+	u32 ring_avail = hv_ringbuf_avail_percent(&out_channel->outbound);
 
 	nvmsg.hdr.msg_type = NVSP_MSG1_TYPE_SEND_RNDIS_PKT;
 	if (skb)
@@ -866,8 +895,10 @@
 	struct multi_send_data *msdp;
 	struct hv_netvsc_packet *msd_send = NULL, *cur_send = NULL;
 	struct sk_buff *msd_skb = NULL;
-	bool try_batch, xmit_more;
-
+	bool try_batch;
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
+	bool xmit_more;
+#endif
 	/* If device is rescinded, return error and packet will get dropped. */
 	if (unlikely(!net_device || net_device->destroy))
 		return -ENODEV;
@@ -879,6 +910,7 @@
 	/* Send control message directly without accessing msd (Multi-Send
 	 * Data) field which may be changed during data packet processing.
 	 */
+
 	if (!skb)
 		return netvsc_send_pkt(device, packet, net_device, pb, skb);
 
@@ -907,19 +939,23 @@
 			msd_len = 0;
 		}
 	}
-
 	/* Keep aggregating only if stack says more data is coming
 	 * and not doing mixed modes send and not flow blocked
 	 */
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
 	xmit_more = skb->xmit_more &&
-		!packet->cp_partial &&
-		!netif_xmit_stopped(netdev_get_tx_queue(ndev, packet->q_idx));
+			!packet->cp_partial &&
+			!netif_xmit_stopped(netdev_get_tx_queue(ndev, packet->q_idx));
+#endif
 
 	if (section_index != NETVSC_INVALID_INDEX) {
 		netvsc_copy_to_send_buf(net_device,
 					section_index, msd_len,
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
 					packet, rndis_msg, pb, xmit_more);
-
+#else
+					packet, rndis_msg, pb, skb);
+#endif
 		packet->send_buf_index = section_index;
 
 		if (packet->cp_partial) {
@@ -937,8 +973,11 @@
 
 		if (msdp->skb)
 			dev_consume_skb_any(msdp->skb);
-
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1)) 
 		if (xmit_more) {
+#else
+		if (packet->xmit_more && !packet->cp_partial) {
+#endif
 			msdp->skb = skb;
 			msdp->pkt = packet;
 			msdp->count++;
@@ -1095,28 +1134,13 @@
 
 	/* Each range represents 1 RNDIS pkt that contains 1 ethernet frame */
 	for (i = 0; i < count; i++) {
-		u32 offset = vmxferpage_packet->ranges[i].byte_offset;
+		void *data = recv_buf
+			+ vmxferpage_packet->ranges[i].byte_offset;
 		u32 buflen = vmxferpage_packet->ranges[i].byte_count;
-		void *data;
-		int ret;
-
-		if (unlikely(offset + buflen > net_device->recv_buf_size)) {
-			status = NVSP_STAT_FAIL;
-			netif_err(net_device_ctx, rx_err, ndev,
-				  "Packet offset:%u + len:%u too big\n",
-				  offset, buflen);
-
-			continue;
-		}
-
-		data = recv_buf + offset;
 
 		/* Pass it to the upper layer */
-		ret = rndis_filter_receive(ndev, net_device,
-					   channel, data, buflen);
-
-		if (unlikely(ret != NVSP_STAT_SUCCESS))
-			status = NVSP_STAT_FAIL;
+		status = rndis_filter_receive(ndev, net_device,
+					      channel, data, buflen);
 	}
 
 	enq_receive_complete(ndev, net_device, q_idx,
@@ -1168,6 +1192,7 @@
 	}
 }
 
+
 static int netvsc_process_raw_pkt(struct hv_device *device,
 				  struct vmbus_channel *channel,
 				  struct netvsc_device *net_device,
@@ -1309,7 +1334,7 @@
 	for (i = 0; i < VRSS_CHANNEL_MAX; i++) {
 		struct netvsc_channel *nvchan = &net_device->chan_table[i];
 
-		nvchan->channel = device->channel;
+ 		nvchan->channel = device->channel;
 		nvchan->net_device = net_device;
 		u64_stats_init(&nvchan->tx_stats.syncp);
 		u64_stats_init(&nvchan->rx_stats.syncp);
@@ -1321,8 +1346,8 @@
 
 	/* Open the channel */
 	ret = vmbus_open(device->channel, netvsc_ring_bytes,
-			 netvsc_ring_bytes,  NULL, 0,
-			 netvsc_channel_cb, net_device->chan_table);
+		netvsc_ring_bytes, NULL, 0,
+		netvsc_channel_cb, net_device->chan_table);
 
 	if (ret != 0) {
 		netdev_err(ndev, "unable to open channel: %d\n", ret);
diff -Naur linux-3.10.0-957.el7.orig/drivers/net/hyperv/netvsc_compat.h linux-3.10.0-957.el7.lis/drivers/net/hyperv/netvsc_compat.h
--- linux-3.10.0-957.el7.orig/drivers/net/hyperv/netvsc_compat.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/net/hyperv/netvsc_compat.h	2018-12-12 00:51:40.841784446 +0000
@@ -0,0 +1,23 @@
+/*
+ * Compatiability macros to adapt to older kernel versions
+ */
+
+static inline bool compat_napi_complete_done(struct napi_struct *n, int work_done)
+{
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,3))
+	napi_complete(n);
+#else
+	napi_complete_done(n, work_done);
+#endif
+	return true;
+}
+	
+#define napi_complete_done  compat_napi_complete_done
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+static inline void __napi_schedule_irqoff(struct napi_struct *n)
+{
+	__napi_schedule(n);
+}
+#endif
+
diff -Naur linux-3.10.0-957.el7.orig/drivers/net/hyperv/netvsc_drv.c linux-3.10.0-957.el7.lis/drivers/net/hyperv/netvsc_drv.c
--- linux-3.10.0-957.el7.orig/drivers/net/hyperv/netvsc_drv.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/net/hyperv/netvsc_drv.c	2018-12-12 00:51:40.849784395 +0000
@@ -11,8 +11,7 @@
  * more details.
  *
  * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
- * Place - Suite 330, Boston, MA 02111-1307 USA.
+ * this program; if not, see <http://www.gnu.org/licenses/>.
  *
  * Authors:
  *   Haiyang Zhang <haiyangz@microsoft.com>
@@ -36,10 +35,12 @@
 #include <linux/slab.h>
 #include <linux/rtnetlink.h>
 #include <linux/netpoll.h>
+#include <linux/reciprocal_div.h>
 
 #include <net/arp.h>
 #include <net/route.h>
 #include <net/sock.h>
+#include <net/udp.h>
 #include <net/pkt_sched.h>
 #include <net/checksum.h>
 #include <net/ip6_checksum.h>
@@ -58,17 +59,24 @@
 module_param(ring_size, uint, S_IRUGO);
 MODULE_PARM_DESC(ring_size, "Ring buffer size (# of pages)");
 unsigned int netvsc_ring_bytes;
+
+#if (RHEL_RELEASE_CODE == RHEL_RELEASE_VERSION(7,0))
+u32 netvsc_ring_reciprocal;
+#else
 struct reciprocal_value netvsc_ring_reciprocal;
+#endif
 
 static const u32 default_msg = NETIF_MSG_DRV | NETIF_MSG_PROBE |
-				NETIF_MSG_LINK | NETIF_MSG_IFUP |
-				NETIF_MSG_IFDOWN | NETIF_MSG_RX_ERR |
-				NETIF_MSG_TX_ERR;
+		NETIF_MSG_LINK | NETIF_MSG_IFUP |
+		NETIF_MSG_IFDOWN | NETIF_MSG_RX_ERR |
+		NETIF_MSG_TX_ERR;
 
 static int debug = -1;
 module_param(debug, int, S_IRUGO);
 MODULE_PARM_DESC(debug, "Debug level (0=none,...,16=all)");
 
+static LIST_HEAD(netvsc_dev_list);
+
 static void netvsc_change_rx_flags(struct net_device *net, int change)
 {
 	struct net_device_context *ndev_ctx = netdev_priv(net);
@@ -126,8 +134,10 @@
 	}
 
 	rdev = nvdev->extension;
-	if (!rdev->link_state)
+	if (!rdev->link_state) {
 		netif_carrier_on(net);
+		netif_tx_wake_all_queues(net);
+	}
 
 	if (vf_netdev) {
 		/* Setting synthetic device up transparently sets
@@ -212,7 +222,7 @@
 }
 
 static inline void *init_ppi_data(struct rndis_message *msg,
-				  u32 ppi_size, u32 pkt_type)
+			   u32 ppi_size, u32 pkt_type)
 {
 	struct rndis_packet *rndis_pkt = &msg->msg.pkt;
 	struct rndis_per_packet_info *ppi;
@@ -230,56 +240,93 @@
 	return ppi + 1;
 }
 
-/* Azure hosts don't support non-TCP port numbers in hashing for fragmented
- * packets. We can use ethtool to change UDP hash level when necessary.
+union sub_key {
+	u64 k;
+	struct {
+		u8 pad[3];
+		u8 kb;
+		u32 ka;
+	};
+};
+
+/* Toeplitz hash function
+ * data: network byte order
+ * return: host byte order
  */
-static inline u32 netvsc_get_hash(
-	struct sk_buff *skb,
-	const struct net_device_context *ndc)
+static u32 comp_hash(u8 *key, int klen, void *data, int dlen)
 {
-	struct flow_keys flow;
-	u32 hash, pkt_proto = 0;
-	static u32 hashrnd __read_mostly;
-
-	net_get_random_once(&hashrnd, sizeof(hashrnd));
-
-	if (!skb_flow_dissect_flow_keys(skb, &flow, 0))
-		return 0;
-
-	switch (flow.basic.ip_proto) {
-	case IPPROTO_TCP:
-		if (flow.basic.n_proto == htons(ETH_P_IP))
-			pkt_proto = HV_TCP4_L4HASH;
-		else if (flow.basic.n_proto == htons(ETH_P_IPV6))
-			pkt_proto = HV_TCP6_L4HASH;
-
-		break;
+	union sub_key subk;
+	int k_next = 4;
+	u8 dt;
+	int i, j;
+	u32 ret = 0;
 
-	case IPPROTO_UDP:
-		if (flow.basic.n_proto == htons(ETH_P_IP))
-			pkt_proto = HV_UDP4_L4HASH;
-		else if (flow.basic.n_proto == htons(ETH_P_IPV6))
-			pkt_proto = HV_UDP6_L4HASH;
+	subk.k = 0;
+	subk.ka = ntohl(*(u32 *)key);
 
-		break;
+	for (i = 0; i < dlen; i++) {
+		subk.kb = key[k_next];
+		k_next = (k_next + 1) % klen;
+		dt = ((u8 *)data)[i];
+		for (j = 0; j < 8; j++) {
+			if (dt & 0x80)
+				ret ^= subk.ka;
+			dt <<= 1;
+			subk.k <<= 1;
+		}
 	}
 
-	if (pkt_proto & ndc->l4_hash) {
-		return skb_get_hash(skb);
-	} else {
-		if (flow.basic.n_proto == htons(ETH_P_IP))
-			hash = jhash2((u32 *)&flow.addrs.v4addrs, 2, hashrnd);
-		else if (flow.basic.n_proto == htons(ETH_P_IPV6))
-			hash = jhash2((u32 *)&flow.addrs.v6addrs, 8, hashrnd);
-		else
-			hash = 0;
+	return ret;
+}
 
-		skb_set_hash(skb, hash, PKT_HASH_TYPE_L3);
+/* Continue using Toeplitz hash function.
+ * This implementation is different from the current upstream code.
+ * See more info from this upstream commit:
+ * 757647e10e55c01fb7a9c4356529442e316a7c72
+ */
+bool netvsc_set_hash(u32 *hash, struct sk_buff *skb)
+{
+	struct iphdr *iphdr;
+	struct ipv6hdr *ipv6hdr;
+	__be32 dbuf[9];
+	int data_len;
+
+	if (eth_hdr(skb)->h_proto != htons(ETH_P_IP) &&
+	    eth_hdr(skb)->h_proto != htons(ETH_P_IPV6))
+		return false;
+
+	iphdr = ip_hdr(skb);
+	ipv6hdr = ipv6_hdr(skb);
+
+	if (iphdr->version == 4) {
+		dbuf[0] = iphdr->saddr;
+		dbuf[1] = iphdr->daddr;
+		if (iphdr->protocol == IPPROTO_TCP) {
+			dbuf[2] = *(__be32 *)&tcp_hdr(skb)->source;
+			data_len = 12;
+		} else {
+			data_len = 8;
+		}
+	} else if (ipv6hdr->version == 6) {
+		memcpy(dbuf, &ipv6hdr->saddr, 32);
+		if (ipv6hdr->nexthdr == IPPROTO_TCP) {
+			dbuf[8] = *(__be32 *)&tcp_hdr(skb)->source;
+			data_len = 36;
+		} else {
+			data_len = 32;
+		}
+	} else {
+		return false;
 	}
 
-	return hash;
+	*hash = comp_hash(netvsc_hash_key, HASH_KEYLEN, dbuf, data_len);
+
+	return true;
 }
 
+// skb_get_hash() will include UDP port numbers into hash computation, 
+// which causes UDP loss problem. Comment this out for now.
+#ifdef NOTYET
 static inline int netvsc_get_tx_queue(struct net_device *ndev,
 				      struct sk_buff *skb, int old_idx)
 {
@@ -287,8 +334,8 @@
 	struct sock *sk = skb->sk;
 	int q_idx;
 
-	q_idx = ndc->tx_table[netvsc_get_hash(skb, ndc) &
-			      (VRSS_SEND_TAB_SIZE - 1)];
+	q_idx = ndc->tx_table[skb_get_hash(skb) &
+				   (VRSS_SEND_TAB_SIZE - 1)];
 
 	/* If queue index changed record the new value */
 	if (q_idx != old_idx &&
@@ -297,47 +344,48 @@
 
 	return q_idx;
 }
+#endif
 
-/*
- * Select queue for transmit.
- *
- * If a valid queue has already been assigned, then use that.
- * Otherwise compute tx queue based on hash and the send table.
- *
- * This is basically similar to default (__netdev_pick_tx) with the added step
- * of using the host send_table when no other queue has been assigned.
- *
- * TODO support XPS - but get_xps_queue not exported
- */
 static u16 netvsc_pick_tx(struct net_device *ndev, struct sk_buff *skb)
 {
-	int q_idx = sk_tx_queue_get(skb->sk);
+	struct net_device_context *net_device_ctx = netdev_priv(ndev);
+	u32 hash;
+	u16 q_idx = 0;
 
-	if (q_idx < 0 || skb->ooo_okay || q_idx >= ndev->real_num_tx_queues) {
-		/* If forwarding a packet, we use the recorded queue when
-		 * available for better cache locality.
-		 */
-		if (skb_rx_queue_recorded(skb))
-			q_idx = skb_get_rx_queue(skb);
-		else
-			q_idx = netvsc_get_tx_queue(ndev, skb, q_idx);
+	if (ndev->real_num_tx_queues <= 1)
+		return 0;
+
+	if (netvsc_set_hash(&hash, skb)) {
+		q_idx = net_device_ctx->tx_table[hash % VRSS_SEND_TAB_SIZE] %
+			ndev->real_num_tx_queues;
+		skb_set_hash(skb, hash, PKT_HASH_TYPE_L3);
 	}
 
 	return q_idx;
 }
 
+/*
+ * Select queue for transmit.
+ * 
+ * Backport notice:
+ * Currently lis-next does not use the Linux upstream Jenkins hash.
+ * Instead, it uses Toeplitz Hash which is defined in:
+ * hv_compat.h: netvsc_set_hash().
+ */
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
+// Divergence from upstream commit:
+// 5b54dac856cb5bd6f33f4159012773e4a33704f7
 static u16 netvsc_select_queue(struct net_device *ndev, struct sk_buff *skb,
-			       void *accel_priv,
-			       select_queue_fallback_t fallback)
+			void *accel_priv, select_queue_fallback_t fallback)
 {
-	struct net_device_context *ndc = netdev_priv(ndev);
-	struct net_device *vf_netdev;
-	u16 txq;
-
-	rcu_read_lock();
-	vf_netdev = rcu_dereference(ndc->vf_netdev);
-	if (vf_netdev) {
-		const struct net_device_ops *vf_ops = vf_netdev->netdev_ops;
+         struct net_device_context *ndc = netdev_priv(ndev);
+         struct net_device *vf_netdev;
+         u16 txq;
+    
+         rcu_read_lock();
+         vf_netdev = rcu_dereference(ndc->vf_netdev);
+         if (vf_netdev) {
+			const struct net_device_ops *vf_ops = vf_netdev->netdev_ops;
 
 		if (vf_ops->ndo_select_queue)
 			txq = vf_ops->ndo_select_queue(vf_netdev, skb,
@@ -350,6 +398,30 @@
 		 * the synthetic device.
 		 */
 		qdisc_skb_cb(skb)->slave_dev_queue_mapping = txq;
+
+         } else {
+             txq = netvsc_pick_tx(ndev, skb);
+         }
+         rcu_read_unlock();
+    
+         while (unlikely(txq >= ndev->real_num_tx_queues))
+             txq -= ndev->real_num_tx_queues;
+    
+         return txq;
+     }
+
+#else
+static u16 netvsc_select_queue(struct net_device *ndev, struct sk_buff *skb)
+{
+	struct net_device_context *ndc = netdev_priv(ndev);
+	struct net_device *vf_netdev;
+	u16 txq;
+
+	rcu_read_lock();
+	vf_netdev = rcu_dereference(ndc->vf_netdev);
+	if (vf_netdev) {
+		txq = skb_rx_queue_recorded(skb) ? skb_get_rx_queue(skb) : 0;
+		qdisc_skb_cb(skb)->slave_dev_queue_mapping = skb->queue_mapping;
 	} else {
 		txq = netvsc_pick_tx(ndev, skb);
 	}
@@ -360,6 +432,7 @@
 
 	return txq;
 }
+#endif
 
 static u32 fill_pg_buf(struct page *page, u32 offset, u32 len,
 		       struct hv_page_buffer *pb)
@@ -469,6 +542,7 @@
 			return TRANSPORT_INFO_IPV4_TCP;
 		else if (ip->protocol == IPPROTO_UDP)
 			return TRANSPORT_INFO_IPV4_UDP;
+
 	} else {
 		struct ipv6hdr *ip6 = ipv6_hdr(skb);
 
@@ -528,12 +602,10 @@
 	    !netpoll_tx_running(net))
 		return netvsc_vf_xmit(net, vf_netdev, skb);
 
-	/* We will atmost need two pages to describe the rndis
-	 * header. We can only transmit MAX_PAGE_BUFFER_COUNT number
+	/* We can only transmit MAX_PAGE_BUFFER_COUNT number
 	 * of pages in a single packet. If skb is scattered around
 	 * more pages we try linearizing it.
 	 */
-
 	num_data_pgs = netvsc_get_slots(skb) + 2;
 
 	if (unlikely(num_data_pgs > MAX_PAGE_BUFFER_COUNT)) {
@@ -563,6 +635,12 @@
 			FIELD_SIZEOF(struct sk_buff, cb));
 	packet = (struct hv_netvsc_packet *)skb->cb;
 
+	/* TODO: This will likely evaluate to false, since RH7 and
+	 * below kernels will set next pointer to NULL before calling
+	 * into here. Should find another way to set this flag.
+	 */
+	packet->xmit_more = (skb->next != NULL);
+	
 	packet->q_idx = skb_get_queue_mapping(skb);
 
 	packet->total_data_buflen = skb->len;
@@ -571,6 +649,8 @@
 
 	rndis_msg = (struct rndis_message *)skb->head;
 
+	packet->send_completion_ctx = packet;
+
 	/* Add the rndis header */
 	rndis_msg->ndis_msg_type = RNDIS_MSG_PACKET;
 	rndis_msg->msg_len = packet->total_data_buflen;
@@ -583,7 +663,12 @@
 
 	rndis_msg_size = RNDIS_MESSAGE_SIZE(struct rndis_packet);
 
+#ifdef NOTYET
+	// Divergence from upstream commit:
+	// 307f099520b66504cf6c5638f3f404c48b9fb45b
 	hash = skb_get_hash_raw(skb);
+#endif
+	hash = skb_get_hash(skb);
 	if (hash != 0 && net->real_num_tx_queues > 1) {
 		u32 *hash_info;
 
@@ -612,7 +697,6 @@
 		rndis_msg_size += NDIS_LSO_PPI_SIZE;
 		lso_info = init_ppi_data(rndis_msg, NDIS_LSO_PPI_SIZE,
 					 TCP_LARGESEND_PKTINFO);
-
 		lso_info->value = 0;
 		lso_info->lso_v2_transmit.type = NDIS_TCP_LARGE_SEND_OFFLOAD_V2_TYPE;
 		if (skb->protocol == htons(ETH_P_IP)) {
@@ -633,6 +717,7 @@
 		}
 		lso_info->lso_v2_transmit.tcp_header_offset = skb_transport_offset(skb);
 		lso_info->lso_v2_transmit.mss = skb_shinfo(skb)->gso_size;
+
 	} else if (skb->ip_summed == CHECKSUM_PARTIAL) {
 		if (net_checksum_info(skb) & net_device_ctx->tx_checksum_mask) {
 			struct ndis_tcp_ip_checksum_info *csum_info;
@@ -646,7 +731,6 @@
 
 			if (skb->protocol == htons(ETH_P_IP)) {
 				csum_info->transmit.is_ipv4 = 1;
-
 				if (ip_hdr(skb)->protocol == IPPROTO_TCP)
 					csum_info->transmit.tcp_checksum = 1;
 				else
@@ -748,7 +832,11 @@
 {
 	struct sk_buff *skb;
 
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
 	skb = napi_alloc_skb(napi, buflen);
+#else
+	skb = netdev_alloc_skb_ip_align(net, buflen);
+#endif
 	if (!skb)
 		return skb;
 
@@ -784,7 +872,7 @@
 }
 
 /*
- * netvsc_recv_callback -  Callback when we receive a packet from the
+ * netvsc_recf_callback - Callback when we receive a packet from the
  * "wire" on the specified device.
  */
 int netvsc_recv_callback(struct net_device *net,
@@ -831,7 +919,7 @@
 	u64_stats_update_end(&rx_stats->syncp);
 
 	napi_gro_receive(&nvchan->napi, skb);
-	return NVSP_STAT_SUCCESS;
+	return 0;
 }
 
 static void netvsc_get_drvinfo(struct net_device *net,
@@ -903,8 +991,20 @@
 	if (IS_ERR(nvdev))
 		return PTR_ERR(nvdev);
 
-	/* Note: enable and attach happen when sub-channels setup */
+	if (nvdev->num_chn > 1) {
+		ret = rndis_set_subchannel(ndev, nvdev);
+
+		/* if unavailable, just proceed with one queue */
+		if (ret) {
+			nvdev->max_chn = 1;
+			nvdev->num_chn = 1;
+		}
+	}
+
+	/* In any case device is now ready */
+	netif_device_attach(ndev);
 
+	/* Note: enable and attach happen when sub-channels setup */
 	netif_carrier_off(ndev);
 
 	if (netif_running(ndev)) {
@@ -961,24 +1061,23 @@
 		device_info.num_chn = orig;
 		if (netvsc_attach(net, &device_info))
 			netdev_err(net, "restoring channel setting failed\n");
-	}
+	}	
 
 	return ret;
 }
 
-static bool
-netvsc_validate_ethtool_ss_cmd(const struct ethtool_link_ksettings *cmd)
+static bool netvsc_validate_ethtool_ss_cmd(const struct ethtool_cmd *cmd)
 {
-	struct ethtool_link_ksettings diff1 = *cmd;
-	struct ethtool_link_ksettings diff2 = {};
+	struct ethtool_cmd diff1 = *cmd;
+	struct ethtool_cmd diff2 = {};
 
-	diff1.base.speed = 0;
-	diff1.base.duplex = 0;
+	ethtool_cmd_speed_set(&diff1, 0);
+	diff1.duplex = 0;
 	/* advertising and cmd are usually set */
-	ethtool_link_ksettings_zero_link_mode(&diff1, advertising);
-	diff1.base.cmd = 0;
+	diff1.advertising = 0;
+	diff1.cmd = 0;
 	/* We set port to PORT_OTHER */
-	diff2.base.port = PORT_OTHER;
+	diff2.port = PORT_OTHER;
 
 	return !memcmp(&diff1, &diff2, sizeof(diff1));
 }
@@ -987,38 +1086,34 @@
 {
 	struct net_device_context *ndc = netdev_priv(dev);
 
-	ndc->l4_hash = HV_DEFAULT_L4HASH;
-
 	ndc->speed = SPEED_UNKNOWN;
 	ndc->duplex = DUPLEX_FULL;
 }
 
-static int netvsc_get_link_ksettings(struct net_device *dev,
-				     struct ethtool_link_ksettings *cmd)
+static int netvsc_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
 {
 	struct net_device_context *ndc = netdev_priv(dev);
 
-	cmd->base.speed = ndc->speed;
-	cmd->base.duplex = ndc->duplex;
-	cmd->base.port = PORT_OTHER;
+	ethtool_cmd_speed_set(cmd, ndc->speed);
+	cmd->duplex = ndc->duplex;
+	cmd->port = PORT_OTHER;
 
 	return 0;
 }
 
-static int netvsc_set_link_ksettings(struct net_device *dev,
-				     const struct ethtool_link_ksettings *cmd)
+static int netvsc_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
 {
 	struct net_device_context *ndc = netdev_priv(dev);
 	u32 speed;
 
-	speed = cmd->base.speed;
+	speed = ethtool_cmd_speed(cmd);
 	if (!ethtool_validate_speed(speed) ||
-	    !ethtool_validate_duplex(cmd->base.duplex) ||
+	    !ethtool_validate_duplex(cmd->duplex) ||
 	    !netvsc_validate_ethtool_ss_cmd(cmd))
 		return -EINVAL;
 
 	ndc->speed = speed;
-	ndc->duplex = cmd->base.duplex;
+	ndc->duplex = cmd->duplex;
 
 	return 0;
 }
@@ -1028,13 +1123,20 @@
 	struct net_device_context *ndevctx = netdev_priv(ndev);
 	struct net_device *vf_netdev = rtnl_dereference(ndevctx->vf_netdev);
 	struct netvsc_device *nvdev = rtnl_dereference(ndevctx->nvdev);
-	int orig_mtu = ndev->mtu;
 	struct netvsc_device_info device_info;
+	int orig_mtu = ndev->mtu;
+	int limit = ETH_DATA_LEN;
 	int ret = 0;
 
 	if (!nvdev || nvdev->destroy)
 		return -ENODEV;
 
+	if (nvdev->nvsp_version >= NVSP_PROTOCOL_VERSION_2)
+		limit = NETVSC_MTU - ETH_HLEN;
+
+	if (mtu < NETVSC_MTU_MIN || mtu > limit)
+		return -EINVAL;
+
 	/* Change MTU of underlying VF netdev first. */
 	if (vf_netdev) {
 		ret = dev_set_mtu(vf_netdev, mtu);
@@ -1104,8 +1206,8 @@
 	}
 }
 
-static void netvsc_get_stats64(struct net_device *net,
-			       struct rtnl_link_stats64 *t)
+static struct rtnl_link_stats64 *netvsc_get_stats64(struct net_device *net,
+						    struct rtnl_link_stats64 *t)
 {
 	struct net_device_context *ndev_ctx = netdev_priv(net);
 	struct netvsc_device *nvdev = rcu_dereference_rtnl(ndev_ctx->nvdev);
@@ -1113,7 +1215,7 @@
 	int i;
 
 	if (!nvdev)
-		return;
+		return NULL;
 
 	netdev_stats_to_stats64(t, &net->stats);
 
@@ -1139,7 +1241,7 @@
 
 		t->tx_bytes	+= bytes;
 		t->tx_packets	+= packets;
-
+ 
 		stats = &nvchan->rx_stats;
 		do {
 			start = u64_stats_fetch_begin_irq(&stats->syncp);
@@ -1153,7 +1255,7 @@
 		t->multicast	+= multicast;
 	}
 
-	return;
+	return t;
 }
 
 static int netvsc_set_mac_addr(struct net_device *ndev, void *p)
@@ -1201,8 +1303,8 @@
 	{ "tx_send_full", offsetof(struct netvsc_ethtool_stats, tx_send_full) },
 	{ "rx_comp_busy", offsetof(struct netvsc_ethtool_stats, rx_comp_busy) },
 	{ "rx_no_memory", offsetof(struct netvsc_ethtool_stats, rx_no_memory) },
-	{ "stop_queue", offsetof(struct netvsc_ethtool_stats, stop_queue) },
-	{ "wake_queue", offsetof(struct netvsc_ethtool_stats, wake_queue) },
+	{ "stop_queue",   offsetof(struct netvsc_ethtool_stats, stop_queue) },
+	{ "wake_queue",   offsetof(struct netvsc_ethtool_stats, wake_queue) },
 }, vf_stats[] = {
 	{ "vf_rx_packets", offsetof(struct netvsc_vf_pcpu_stats, rx_packets) },
 	{ "vf_rx_bytes",   offsetof(struct netvsc_vf_pcpu_stats, rx_bytes) },
@@ -1317,38 +1419,17 @@
 }
 
 static int
-netvsc_get_rss_hash_opts(struct net_device_context *ndc,
-			 struct ethtool_rxnfc *info)
+netvsc_get_rss_hash_opts(struct ethtool_rxnfc *info)
 {
-	const u32 l4_flag = RXH_L4_B_0_1 | RXH_L4_B_2_3;
-
 	info->data = RXH_IP_SRC | RXH_IP_DST;
 
 	switch (info->flow_type) {
 	case TCP_V4_FLOW:
-		if (ndc->l4_hash & HV_TCP4_L4HASH)
-			info->data |= l4_flag;
-
-		break;
-
 	case TCP_V6_FLOW:
-		if (ndc->l4_hash & HV_TCP6_L4HASH)
-			info->data |= l4_flag;
-
-		break;
-
+		info->data |= RXH_L4_B_0_1 | RXH_L4_B_2_3;
+		/* fallthrough */
 	case UDP_V4_FLOW:
-		if (ndc->l4_hash & HV_UDP4_L4HASH)
-			info->data |= l4_flag;
-
-		break;
-
 	case UDP_V6_FLOW:
-		if (ndc->l4_hash & HV_UDP6_L4HASH)
-			info->data |= l4_flag;
-
-		break;
-
 	case IPV4_FLOW:
 	case IPV6_FLOW:
 		break;
@@ -1376,76 +1457,8 @@
 		return 0;
 
 	case ETHTOOL_GRXFH:
-		return netvsc_get_rss_hash_opts(ndc, info);
-	}
-	return -EOPNOTSUPP;
-}
-
-static int netvsc_set_rss_hash_opts(struct net_device_context *ndc,
-				    struct ethtool_rxnfc *info)
-{
-	if (info->data == (RXH_IP_SRC | RXH_IP_DST |
-			   RXH_L4_B_0_1 | RXH_L4_B_2_3)) {
-		switch (info->flow_type) {
-		case TCP_V4_FLOW:
-			ndc->l4_hash |= HV_TCP4_L4HASH;
-			break;
-
-		case TCP_V6_FLOW:
-			ndc->l4_hash |= HV_TCP6_L4HASH;
-			break;
-
-		case UDP_V4_FLOW:
-			ndc->l4_hash |= HV_UDP4_L4HASH;
-			break;
-
-		case UDP_V6_FLOW:
-			ndc->l4_hash |= HV_UDP6_L4HASH;
-			break;
-
-		default:
-			return -EOPNOTSUPP;
-		}
-
-		return 0;
+		return netvsc_get_rss_hash_opts(info);
 	}
-
-	if (info->data == (RXH_IP_SRC | RXH_IP_DST)) {
-		switch (info->flow_type) {
-		case TCP_V4_FLOW:
-			ndc->l4_hash &= ~HV_TCP4_L4HASH;
-			break;
-
-		case TCP_V6_FLOW:
-			ndc->l4_hash &= ~HV_TCP6_L4HASH;
-			break;
-
-		case UDP_V4_FLOW:
-			ndc->l4_hash &= ~HV_UDP4_L4HASH;
-			break;
-
-		case UDP_V6_FLOW:
-			ndc->l4_hash &= ~HV_UDP6_L4HASH;
-			break;
-
-		default:
-			return -EOPNOTSUPP;
-		}
-
-		return 0;
-	}
-
-	return -EOPNOTSUPP;
-}
-
-static int
-netvsc_set_rxnfc(struct net_device *ndev, struct ethtool_rxnfc *info)
-{
-	struct net_device_context *ndc = netdev_priv(ndev);
-
-	if (info->cmd == ETHTOOL_SRXFH)
-		return netvsc_set_rss_hash_opts(ndc, info);
-
 	return -EOPNOTSUPP;
 }
 
@@ -1469,16 +1482,19 @@
 }
 #endif
 
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
 static u32 netvsc_get_rxfh_key_size(struct net_device *dev)
 {
 	return NETVSC_HASH_KEYLEN;
 }
+#endif
 
 static u32 netvsc_rss_indir_size(struct net_device *dev)
 {
 	return ITAB_NUM;
 }
 
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
 static int netvsc_get_rxfh(struct net_device *dev, u32 *indir, u8 *key,
 			   u8 *hfunc)
 {
@@ -1538,6 +1554,7 @@
 
 	return rndis_filter_set_rss_param(rndis_dev, key);
 }
+#endif
 
 /* Hyper-V RNDIS protocol does not have ring in the HW sense.
  * It does have pre-allocated receive area which is divided into sections.
@@ -1607,12 +1624,11 @@
 	ret = netvsc_detach(ndev, nvdev);
 	if (ret)
 		return ret;
-
+ 
 	ret = netvsc_attach(ndev, &device_info);
 	if (ret) {
 		device_info.send_sections = orig.tx_pending;
 		device_info.recv_sections = orig.rx_pending;
-
 		if (netvsc_attach(ndev, &device_info))
 			netdev_err(ndev, "restoring ringparam failed");
 	}
@@ -1629,26 +1645,30 @@
 	.get_channels   = netvsc_get_channels,
 	.set_channels   = netvsc_set_channels,
 	.get_ts_info	= ethtool_op_get_ts_info,
+	.get_settings	= netvsc_get_settings,
+	.set_settings	= netvsc_set_settings,
 	.get_rxnfc	= netvsc_get_rxnfc,
-	.set_rxnfc	= netvsc_set_rxnfc,
-	.get_rxfh_key_size = netvsc_get_rxfh_key_size,
 	.get_rxfh_indir_size = netvsc_rss_indir_size,
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
+	.get_rxfh_key_size = netvsc_get_rxfh_key_size,
 	.get_rxfh	= netvsc_get_rxfh,
 	.set_rxfh	= netvsc_set_rxfh,
-	.get_link_ksettings = netvsc_get_link_ksettings,
-	.set_link_ksettings = netvsc_set_link_ksettings,
+#endif
 	.get_ringparam	= netvsc_get_ringparam,
 	.set_ringparam	= netvsc_set_ringparam,
 };
 
 static const struct net_device_ops device_ops = {
-	.ndo_size =			sizeof(struct net_device_ops),
 	.ndo_open =			netvsc_open,
 	.ndo_stop =			netvsc_close,
 	.ndo_start_xmit =		netvsc_start_xmit,
 	.ndo_change_rx_flags =		netvsc_change_rx_flags,
 	.ndo_set_rx_mode =		netvsc_set_rx_mode,
-	.extended.ndo_change_mtu =	netvsc_change_mtu,
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5))
+	.ndo_change_mtu =		netvsc_change_mtu,
+#else
+	.ndo_change_mtu_rh74 =		netvsc_change_mtu,
+#endif
 	.ndo_validate_addr =		eth_validate_addr,
 	.ndo_set_mac_address =		netvsc_set_mac_addr,
 	.ndo_select_queue =		netvsc_select_queue,
@@ -1768,13 +1788,10 @@
 
 static struct net_device *get_netvsc_bymac(const u8 *mac)
 {
-	struct net_device *dev;
-
-	ASSERT_RTNL();
+	struct net_device_context *ndev_ctx;
 
-	for_each_netdev(&init_net, dev) {
-		if (dev->netdev_ops != &device_ops)
-			continue;	/* not a netvsc device */
+	list_for_each_entry(ndev_ctx, &netvsc_dev_list, list) {
+		struct net_device *dev = hv_get_drvdata(ndev_ctx->device_ctx);
 
 		if (ether_addr_equal(mac, dev->perm_addr))
 			return dev;
@@ -1785,25 +1802,18 @@
 
 static struct net_device *get_netvsc_byref(struct net_device *vf_netdev)
 {
+	struct net_device_context *net_device_ctx;
 	struct net_device *dev;
 
-	ASSERT_RTNL();
-
-	for_each_netdev(&init_net, dev) {
-		struct net_device_context *net_device_ctx;
-
-		if (dev->netdev_ops != &device_ops)
-			continue;	/* not a netvsc device */
+	dev = netdev_master_upper_dev_get(vf_netdev);
+	if (!dev || dev->netdev_ops != &device_ops)
+		return NULL;	/* not a netvsc device */
+
+	net_device_ctx = netdev_priv(dev);
+	if (!rtnl_dereference(net_device_ctx->nvdev))
+		return NULL;	/* device is removed */
 
-		net_device_ctx = netdev_priv(dev);
-		if (!rtnl_dereference(net_device_ctx->nvdev))
-			continue;	/* device is removed */
-
-		if (rtnl_dereference(net_device_ctx->vf_netdev) == vf_netdev)
-			return dev;	/* a match */
-	}
-
-	return NULL;
+	return dev;
 }
 
 /* Called when VF is injecting data into network stack.
@@ -1843,7 +1853,12 @@
 		goto rx_handler_failed;
 	}
 
-	ret = netdev_upper_dev_link(vf_netdev, ndev);
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,4))
+        ret = netdev_master_upper_dev_link(vf_netdev, ndev,
+                                           NULL, NULL);
+#else
+        ret = netdev_master_upper_dev_link(vf_netdev, ndev);
+#endif
 	if (ret != 0) {
 		netdev_err(vf_netdev,
 			   "can not set master device %s (err = %d)\n",
@@ -1886,7 +1901,7 @@
 	dev_uc_sync(vf_netdev, ndev);
 	dev_mc_sync(vf_netdev, ndev);
 	netif_addr_unlock_bh(ndev);
-
+	
 	if (netif_running(ndev)) {
 		ret = dev_open(vf_netdev);
 		if (ret)
@@ -1996,6 +2011,8 @@
 	return NOTIFY_OK;
 }
 
+
+
 static int netvsc_probe(struct hv_device *dev,
 			const struct hv_vmbus_device_id *dev_id)
 {
@@ -2011,7 +2028,6 @@
 		goto no_net;
 
 	netif_carrier_off(net);
-
 	netvsc_init_settings(net);
 
 	net_device_ctx = netdev_priv(net);
@@ -2019,7 +2035,7 @@
 	net_device_ctx->msg_enable = netif_msg_init(debug, default_msg);
 	if (netif_msg_probe(net_device_ctx))
 		netdev_dbg(net, "netvsc msg_enable: %d\n",
-			   net_device_ctx->msg_enable);
+			net_device_ctx->msg_enable);
 
 	hv_set_drvdata(dev, net);
 
@@ -2035,7 +2051,7 @@
 		goto no_stats;
 
 	net->netdev_ops = &device_ops;
-	SET_ETHTOOL_OPS(net, &ethtool_ops);
+	net->ethtool_ops = &ethtool_ops;
 	SET_NETDEV_DEV(net, &dev->device);
 
 	/* We always need headroom for rndis header */
@@ -2064,6 +2080,19 @@
 
 	memcpy(net->dev_addr, device_info.mac_adr, ETH_ALEN);
 
+	/* We must get rtnl lock before scheduling nvdev->subchan_work,
+	 * otherwise netvsc_subchan_work() can get rtnl lock first and wait
+	 * all subchannels to show up, but that may not happen because
+	 * netvsc_probe() can't get rtnl lock and as a result vmbus_onoffer()
+	 * -> ... -> device_add() -> ... -> __device_attach() can't get
+	 * the device lock, so all the subchannels can't be processed --
+	 * finally netvsc_subchan_work() hangs for ever.
+	 */
+	rtnl_lock();
+
+	if (nvdev->num_chn > 1)
+		schedule_work(&nvdev->subchan_work);
+
 	/* hw_features computed in rndis_netdev_set_hwcaps() */
 	net->features = net->hw_features |
 		NETIF_F_HIGHDMA | NETIF_F_SG |
@@ -2072,22 +2101,18 @@
 
 	netdev_lockdep_set_classes(net);
 
-	/* MTU range: 68 - 1500 or 65521 */
-	net->extended->min_mtu = NETVSC_MTU_MIN;
-	if (nvdev->nvsp_version >= NVSP_PROTOCOL_VERSION_2)
-		net->extended->max_mtu = NETVSC_MTU - ETH_HLEN;
-	else
-		net->extended->max_mtu = ETH_DATA_LEN;
-
-	ret = register_netdev(net);
+	ret = register_netdevice(net);
 	if (ret != 0) {
 		pr_err("Unable to register netdev.\n");
 		goto register_failed;
 	}
 
-	return ret;
+	list_add(&net_device_ctx->list, &netvsc_dev_list);
+	rtnl_unlock();
+	return 0;
 
 register_failed:
+	rtnl_unlock();
 	rndis_filter_device_remove(dev, nvdev);
 rndis_failed:
 	free_percpu(net_device_ctx->vf_stats);
@@ -2133,6 +2158,7 @@
 		rndis_filter_device_remove(dev, nvdev);
 
 	unregister_netdevice(net);
+	list_del(&ndev_ctx->list);
 
 	rtnl_unlock();
 	rcu_read_unlock();
@@ -2169,8 +2195,12 @@
 static int netvsc_netdev_event(struct notifier_block *this,
 			       unsigned long event, void *ptr)
 {
+#ifdef NOTYET
+	/* Not in RHEL 7.4 kernel - check again when next version releases - alexng */
 	struct net_device *event_dev = netdev_notifier_info_to_dev(ptr);
-
+#else
+	struct net_device *event_dev = ptr;
+#endif
 	/* Skip our own events */
 	if (event_dev->netdev_ops == &device_ops)
 		return NOTIFY_DONE;
@@ -2207,7 +2237,7 @@
 
 static void __exit netvsc_drv_exit(void)
 {
-	unregister_netdevice_notifier_rh(&netvsc_netdev_notifier);
+	unregister_netdevice_notifier(&netvsc_netdev_notifier);
 	vmbus_driver_unregister(&netvsc_drv);
 }
 
@@ -2221,16 +2251,18 @@
 			ring_size);
 	}
 	netvsc_ring_bytes = ring_size * PAGE_SIZE;
+	netvsc_ring_reciprocal = reciprocal_value(netvsc_ring_bytes);
 
 	ret = vmbus_driver_register(&netvsc_drv);
 	if (ret)
 		return ret;
 
-	register_netdevice_notifier_rh(&netvsc_netdev_notifier);
+	register_netdevice_notifier(&netvsc_netdev_notifier);
 	return 0;
 }
 
 MODULE_LICENSE("GPL");
+MODULE_VERSION(HV_DRV_VERSION);
 MODULE_DESCRIPTION("Microsoft Hyper-V network driver");
 
 module_init(netvsc_drv_init);
diff -Naur linux-3.10.0-957.el7.orig/drivers/net/hyperv/rndis_filter.c linux-3.10.0-957.el7.lis/drivers/net/hyperv/rndis_filter.c
--- linux-3.10.0-957.el7.orig/drivers/net/hyperv/rndis_filter.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/net/hyperv/rndis_filter.c	2018-12-12 00:51:40.854784363 +0000
@@ -11,8 +11,7 @@
  * more details.
  *
  * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
- * Place - Suite 330, Boston, MA 02111-1307 USA.
+ * this program; if not, see <http://www.gnu.org/licenses/>.
  *
  * Authors:
  *   Haiyang Zhang <haiyangz@microsoft.com>
@@ -59,7 +58,10 @@
 	u8 request_ext[RNDIS_EXT_LEN];
 };
 
+#ifdef NOTYET
 static const u8 netvsc_hash_key[NETVSC_HASH_KEYLEN] = {
+#endif
+u8 netvsc_hash_key[NETVSC_HASH_KEYLEN] = {
 	0x6d, 0x5a, 0x56, 0xda, 0x25, 0x5b, 0x0e, 0xc2,
 	0x41, 0x67, 0x25, 0x3d, 0x43, 0xa3, 0x8f, 0xb0,
 	0xd0, 0xca, 0x2b, 0xcb, 0xae, 0x7b, 0x30, 0xb4,
@@ -219,7 +221,6 @@
 
 	/* Setup the packet to send it */
 	packet = &req->pkt;
-
 	packet->total_data_buflen = req->request_msg.msg_len;
 	packet->page_buf_cnt = 1;
 
@@ -240,6 +241,8 @@
 		pb[1].len = req->request_msg.msg_len -
 			pb[0].len;
 	}
+	
+	packet->xmit_more = false;
 
 	rcu_read_lock_bh();
 	ret = netvsc_send(dev->ndev, packet, NULL, pb, NULL);
@@ -264,23 +267,13 @@
 	}
 }
 
-static void rndis_filter_receive_response(struct net_device *ndev,
-					  struct netvsc_device *nvdev,
-					  const struct rndis_message *resp)
+static void rndis_filter_receive_response(struct rndis_device *dev,
+				       struct rndis_message *resp)
 {
-	struct rndis_device *dev = nvdev->extension;
 	struct rndis_request *request = NULL;
 	bool found = false;
 	unsigned long flags;
-
-	/* This should never happen, it means control message
-	 * response received after device removed.
-	 */
-	if (dev->state == RNDIS_DEV_UNINITIALIZED) {
-		netdev_err(ndev,
-			   "got rndis message uninitialized\n");
-		return;
-	}
+	struct net_device *ndev = dev->ndev;
 
 	spin_lock_irqsave(&dev->request_lock, flags);
 	list_for_each_entry(request, &dev->req_list, list_ent) {
@@ -362,15 +355,15 @@
 
 static int rndis_filter_receive_data(struct net_device *ndev,
 				     struct netvsc_device *nvdev,
-				     struct vmbus_channel *channel,
+				     struct rndis_device *dev,
 				     struct rndis_message *msg,
-				     u32 data_buflen)
+				     struct vmbus_channel *channel,
+				     void *data, u32 data_buflen)
 {
 	struct rndis_packet *rndis_pkt = &msg->msg.pkt;
 	const struct ndis_tcp_ip_checksum_info *csum_info;
 	const struct ndis_pkt_8021q_info *vlan;
 	u32 data_offset;
-	void *data;
 
 	/* Remove the rndis header and pass it back up the stack */
 	data_offset = RNDIS_HEADER_SIZE + rndis_pkt->data_offset;
@@ -382,7 +375,7 @@
 	 * should be the data packet size plus the trailer padding size
 	 */
 	if (unlikely(data_buflen < rndis_pkt->data_len)) {
-		netdev_err(ndev, "rndis message buffer "
+		netdev_err(dev->ndev, "rndis message buffer "
 			   "overflow detected (got %u, min %u)"
 			   "...dropping this message!\n",
 			   data_buflen, rndis_pkt->data_len);
@@ -391,15 +384,14 @@
 
 	vlan = rndis_get_ppi(rndis_pkt, IEEE_8021Q_INFO);
 
-	csum_info = rndis_get_ppi(rndis_pkt, TCPIP_CHKSUM_PKTINFO);
-
-	data = (void *)msg + data_offset;
-
 	/*
 	 * Remove the rndis trailer padding from rndis packet message
 	 * rndis_pkt->data_len tell us the real data length, we only copy
 	 * the data packet to the stack, without the rndis trailer padding
 	 */
+	data = (void *)((unsigned long)data + data_offset);
+	csum_info = rndis_get_ppi(rndis_pkt, TCPIP_CHKSUM_PKTINFO);
+
 	return netvsc_recv_callback(ndev, nvdev, channel,
 				    data, rndis_pkt->data_len,
 				    csum_info, vlan);
@@ -411,20 +403,35 @@
 			 void *data, u32 buflen)
 {
 	struct net_device_context *net_device_ctx = netdev_priv(ndev);
+	struct rndis_device *rndis_dev = net_dev->extension;
 	struct rndis_message *rndis_msg = data;
 
+	/* Make sure the rndis device state is initialized */
+	if (unlikely(!rndis_dev)) {
+		netif_dbg(net_device_ctx, rx_err, ndev,
+			  "got rndis message but no rndis device!\n");
+		return NVSP_STAT_FAIL;
+	}
+
+	if (unlikely(rndis_dev->state == RNDIS_DEV_UNINITIALIZED)) {
+		netif_dbg(net_device_ctx, rx_err, ndev,
+			  "got rndis message uninitialized\n");
+		return NVSP_STAT_FAIL;
+	}
+
 	if (netif_msg_rx_status(net_device_ctx))
 		dump_rndis_message(ndev, rndis_msg);
 
 	switch (rndis_msg->ndis_msg_type) {
 	case RNDIS_MSG_PACKET:
-		return rndis_filter_receive_data(ndev, net_dev, channel,
-						 rndis_msg, buflen);
+		return rndis_filter_receive_data(ndev, net_dev,
+						 rndis_dev, rndis_msg,
+						 channel, data, buflen);
 	case RNDIS_MSG_INIT_C:
 	case RNDIS_MSG_QUERY_C:
 	case RNDIS_MSG_SET_C:
 		/* completion msgs */
-		rndis_filter_receive_response(ndev, net_dev, rndis_msg);
+		rndis_filter_receive_response(rndis_dev, rndis_msg);
 		break;
 
 	case RNDIS_MSG_INDICATE:
@@ -436,10 +443,10 @@
 			"unhandled rndis message (type %u len %u)\n",
 			   rndis_msg->ndis_msg_type,
 			   rndis_msg->msg_len);
-		return NVSP_STAT_FAIL;
+		break;
 	}
 
-	return NVSP_STAT_SUCCESS;
+	return 0;
 }
 
 static int rndis_filter_query_device(struct rndis_device *dev,
@@ -827,6 +834,7 @@
 	request = get_rndis_request(dev, RNDIS_MSG_SET,
 			RNDIS_MESSAGE_SIZE(struct rndis_set_request) +
 			sizeof(u32));
+
 	if (!request)
 		return -ENOMEM;
 
@@ -867,6 +875,7 @@
 	}
 
 	rndis_filter_set_packet_filter(rdev, filter);
+
 }
 
 void rndis_filter_update(struct netvsc_device *nvdev)
@@ -1045,6 +1054,7 @@
 	ret = vmbus_open(new_sc, netvsc_ring_bytes,
 			 netvsc_ring_bytes, NULL, 0,
 			 netvsc_channel_cb, nvchan);
+
 	if (ret == 0)
 		napi_enable(&nvchan->napi);
 	else
@@ -1058,29 +1068,15 @@
  * This breaks overlap of processing the host message for the
  * new primary channel with the initialization of sub-channels.
  */
-void rndis_set_subchannel(struct work_struct *w)
+int rndis_set_subchannel(struct net_device *ndev, struct netvsc_device *nvdev)
 {
-	struct netvsc_device *nvdev
-		= container_of(w, struct netvsc_device, subchan_work);
 	struct nvsp_message *init_packet = &nvdev->channel_init_pkt;
-	struct net_device_context *ndev_ctx;
-	struct rndis_device *rdev;
-	struct net_device *ndev;
-	struct hv_device *hv_dev;
+	struct net_device_context *ndev_ctx = netdev_priv(ndev);
+	struct hv_device *hv_dev = ndev_ctx->device_ctx;
+	struct rndis_device *rdev = nvdev->extension;
 	int i, ret;
 
-	if (!rtnl_trylock()) {
-		schedule_work(w);
-		return;
-	}
-
-	rdev = nvdev->extension;
-	if (!rdev)
-		goto unlock;	/* device was removed */
-
-	ndev = rdev->ndev;
-	ndev_ctx = netdev_priv(ndev);
-	hv_dev = ndev_ctx->device_ctx;
+	ASSERT_RTNL();
 
 	memset(init_packet, 0, sizeof(struct nvsp_message));
 	init_packet->hdr.msg_type = NVSP_MSG5_TYPE_SUBCHANNEL;
@@ -1094,13 +1090,13 @@
 			       VMBUS_DATA_PACKET_FLAG_COMPLETION_REQUESTED);
 	if (ret) {
 		netdev_err(ndev, "sub channel allocate send failed: %d\n", ret);
-		goto failed;
+		return ret;
 	}
 
 	wait_for_completion(&nvdev->channel_init_wait);
 	if (init_packet->msg.v5_msg.subchn_comp.status != NVSP_STAT_SUCCESS) {
 		netdev_err(ndev, "sub channel request failed\n");
-		goto failed;
+		return -EIO;
 	}
 
 	nvdev->num_chn = 1 +
@@ -1119,21 +1115,7 @@
 	for (i = 0; i < VRSS_SEND_TAB_SIZE; i++)
 		ndev_ctx->tx_table[i] = i % nvdev->num_chn;
 
-	netif_device_attach(ndev);
-	rtnl_unlock();
-	return;
-
-failed:
-	/* fallback to only primary channel */
-	for (i = 1; i < nvdev->num_chn; i++)
-		netif_napi_del(&nvdev->chan_table[i].napi);
-
-	nvdev->max_chn = 1;
-	nvdev->num_chn = 1;
-
-	netif_device_attach(ndev);
-unlock:
-	rtnl_unlock();
+	return 0;
 }
 
 static int rndis_netdev_set_hwcaps(struct rndis_device *rndis_device,
@@ -1283,7 +1265,7 @@
 		   rndis_device->link_state ? "down" : "up");
 
 	if (net_device->nvsp_version < NVSP_PROTOCOL_VERSION_5)
-		goto out;
+		return net_device;
 
 	rndis_filter_query_link_speed(rndis_device, net_device);
 
@@ -1324,21 +1306,12 @@
 		netif_napi_add(net, &net_device->chan_table[i].napi,
 			       netvsc_poll, NAPI_POLL_WEIGHT);
 
-	if (net_device->num_chn > 1)
-		schedule_work(&net_device->subchan_work);
+	return net_device;
 
 out:
-	/* if unavailable, just proceed with one queue */
-	if (ret) {
-		net_device->max_chn = 1;
-		net_device->num_chn = 1;
-	}
-
-	/* No sub channels, device is ready */
-	if (net_device->num_chn == 1)
-		netif_device_attach(net);
-
-	return net_device;
+	/* setting up multiple channels failed */
+	net_device->max_chn = 1;
+	net_device->num_chn = 1;
 
 err_dev_remv:
 	rndis_filter_device_remove(dev, net_device);
@@ -1356,6 +1329,7 @@
 	net_dev->extension = NULL;
 
 	netvsc_device_remove(dev);
+	kfree(rndis_dev);
 }
 
 int rndis_filter_open(struct netvsc_device *nvdev)
diff -Naur linux-3.10.0-957.el7.orig/drivers/pci/pci-hyperv.c linux-3.10.0-957.el7.lis/drivers/pci/pci-hyperv.c
--- linux-3.10.0-957.el7.orig/drivers/pci/pci-hyperv.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/pci/pci-hyperv.c	2018-12-12 00:51:40.892784122 +0000
@@ -49,7 +49,9 @@
 
 #include <linux/kernel.h>
 #include <linux/module.h>
+#include <linux/hyperv.h>
 #include <linux/pci.h>
+#include <linux/delay.h>
 #include <linux/semaphore.h>
 #include <linux/irqdomain.h>
 #include <linux/msi.h>
@@ -66,8 +68,8 @@
 #define PCI_MINOR_VERSION(version) ((u32)(version) & 0xff)
 
 enum pci_protocol_version_t {
-	PCI_PROTOCOL_VERSION_1_1 = PCI_MAKE_VERSION(1, 1),	/* Win10 */
-	PCI_PROTOCOL_VERSION_1_2 = PCI_MAKE_VERSION(1, 2),	/* RS1 */
+	PCI_PROTOCOL_VERSION_1_1 = PCI_MAKE_VERSION(1, 1),	// Win10
+	PCI_PROTOCOL_VERSION_1_2 = PCI_MAKE_VERSION(1, 2),	// RS1
 };
 
 #define CPU_AFFINITY_ALL	-1ULL
@@ -417,10 +419,10 @@
 };
 
 struct retarget_msi_interrupt {
-	u64	partition_id;		/* use "self" */
-	u64	device_id;
+	u64 partition_id;		/* use "self" */
+	u64 device_id;
 	struct hv_interrupt_entry int_entry;
-	u64	reserved2;
+	u64 reserved2;
 	struct hv_device_interrupt_target int_target;
 } __packed;
 
@@ -572,7 +574,7 @@
 	wslot.slot = 0;
 	wslot.bits.dev = PCI_SLOT(devfn);
 	wslot.bits.func = PCI_FUNC(devfn);
-
+ 
 	return wslot.slot;
 }
 
@@ -787,21 +789,26 @@
 }
 
 /* Interrupt management hooks */
-static int hv_set_affinity(struct irq_data *data, const struct cpumask *dest,
+static int hv_set_affinity(struct irq_data *data, const struct cpumask *mask,
 			   bool force)
 {
+	struct msi_desc *msi_desc = data->msi_desc;
 	struct irq_cfg *cfg = irqd_cfg(data);
+	const struct cpumask *dest;
 	struct retarget_msi_interrupt *params;
 	struct hv_pcibus_device *hbus;
 	struct pci_bus *pbus;
 	struct pci_dev *pdev;
-	int cpu, ret;
+	int cpu, ret, cpu_vmbus;
 	unsigned int dest_id;
 	unsigned long flags;
-	u32 var_size = 0;
-	int cpu_vmbus;
 	u64 res;
+	u32 var_size = 0;
 
+	if (cpumask_equal(mask, cpu_online_mask))
+		dest = cfg->domain;
+	else
+		dest = mask;
 	ret = __ioapic_set_affinity(data, dest, &dest_id);
 	if (ret)
 		return ret;
@@ -816,8 +823,8 @@
 	memset(params, 0, sizeof(*params));
 	params->partition_id = HV_PARTITION_ID_SELF;
 	params->int_entry.source = 1; /* MSI(-X) */
-	params->int_entry.address = data->msi_desc->msg.address_lo;
-	params->int_entry.data = data->msi_desc->msg.data;
+	params->int_entry.address = msi_desc->msg.address_lo;
+	params->int_entry.data = msi_desc->msg.data;
 	params->device_id = (hbus->hdev->dev_instance.b[5] << 24) |
 			   (hbus->hdev->dev_instance.b[4] << 16) |
 			   (hbus->hdev->dev_instance.b[7] << 8) |
@@ -851,7 +858,7 @@
 		 */
 		var_size = 1 + HV_VP_SET_BANK_COUNT_MAX;
 
-		for_each_cpu_and(cpu, cfg->domain, cpu_online_mask) {
+		for_each_cpu_and(cpu, dest, cpu_online_mask) {
 			cpu_vmbus = hv_cpu_number_to_vp_number(cpu);
 
 			if (cpu_vmbus >= HV_VP_SET_BANK_COUNT_MAX * 64) {
@@ -860,11 +867,12 @@
 				res = 1;
 				goto exit_unlock;
 			}
+
 			params->int_target.vp_set.masks[cpu_vmbus / 64] |=
 				(1ULL << (cpu_vmbus & 63));
 		}
 	} else {
-		for_each_cpu_and(cpu, cfg->domain, cpu_online_mask) {
+		for_each_cpu_and(cpu, dest, cpu_online_mask) {
 			params->int_target.vp_mask |=
 				(1ULL << hv_cpu_number_to_vp_number(cpu));
 		}
@@ -879,7 +887,7 @@
 	if (res) {
 		dev_err(&hbus->hdev->device,
 			"%s() failed: %#llx", __func__, res);
-		return -EFAULT;
+		return -1;
 	}
 
 	return 0;
@@ -940,8 +948,7 @@
 	int_pkt->int_desc.vector = vector;
 	int_pkt->int_desc.vector_count = 1;
 	int_pkt->int_desc.delivery_mode =
-		(apic->irq_delivery_mode == dest_LowestPrio) ?
-			dest_LowestPrio : dest_Fixed;
+		(apic->irq_delivery_mode == dest_LowestPrio) ? 1 : 0;
 
 	/*
 	 * Create MSI w/ dummy vCPU set, overwritten by subsequent retarget in
@@ -963,12 +970,11 @@
 	int_pkt->int_desc.vector = vector;
 	int_pkt->int_desc.vector_count = 1;
 	int_pkt->int_desc.delivery_mode =
-		(apic->irq_delivery_mode == dest_LowestPrio) ?
-			dest_LowestPrio : dest_Fixed;
+		(apic->irq_delivery_mode == dest_LowestPrio) ? 1 : 0;
 
 	/*
-	 * Create MSI w/ dummy vCPU set targeting just one vCPU, overwritten
-	 * by subsequent retarget in hv_irq_unmask().
+	 * Create MSI targeting just one vCPU, overwritten by subsequent
+	 * retarget in hv_irq_unmask().
 	 */
 	cpu = cpumask_first_and(affinity, cpu_online_mask);
 	int_pkt->int_desc.processor_array[0] =
@@ -992,6 +998,7 @@
 				u8 hpet_id)
 {
 	struct irq_cfg *cfg = irq_get_chip_data(irq);
+	struct cpumask *affinity = cfg->domain;
 	struct hv_pcibus_device *hbus;
 	struct hv_pci_dev *hpdev;
 	struct pci_bus *pbus;
@@ -1004,6 +1011,7 @@
 			struct pci_create_interrupt2 v2;
 		} int_pkts;
 	} __packed ctxt;
+
 	u32 size;
 	int ret;
 
@@ -1025,16 +1033,15 @@
 	switch (pci_protocol_version) {
 	case PCI_PROTOCOL_VERSION_1_1:
 		size = hv_compose_msi_req_v1(&ctxt.int_pkts.v1,
-					     cfg->domain,
-					     hpdev->desc.win_slot.slot,
-					     cfg->vector);
+					affinity,
+					hpdev->desc.win_slot.slot,
+					cfg->vector);
 		break;
-
 	case PCI_PROTOCOL_VERSION_1_2:
 		size = hv_compose_msi_req_v2(&ctxt.int_pkts.v2,
-					     cfg->domain,
-					     hpdev->desc.win_slot.slot,
-					     cfg->vector);
+					affinity,
+					hpdev->desc.win_slot.slot,
+					cfg->vector);
 		break;
 
 	default:
@@ -1051,6 +1058,7 @@
 			       size, (unsigned long)&ctxt.pci_pkt,
 			       VM_PKT_DATA_INBAND,
 			       VMBUS_DATA_PACKET_FLAG_COMPLETION_REQUESTED);
+
 	if (ret) {
 		dev_err(&hbus->hdev->device,
 			"Sending request for interrupt failed: 0x%x",
@@ -1058,7 +1066,12 @@
 		goto free_int_desc;
 	}
 
-	wait_for_completion(&comp.comp_pkt.host_event);
+	/*
+	 * Since this function is called with IRQ locks held, can't
+	 * do normal wait for completion; instead poll.
+	 */
+	while (!try_wait_for_completion(&comp.comp_pkt.host_event))
+		udelay(100);
 
 	if (comp.comp_pkt.completion_status < 0) {
 		pr_err("Request for interrupt failed: 0x%x",
@@ -1442,7 +1455,19 @@
 	get_pcichild(hpdev, hv_pcidev_ref_initial);
 	get_pcichild(hpdev, hv_pcidev_ref_childlist);
 	spin_lock_irqsave(&hbus->device_list_lock, flags);
-
+	/*
+	 * When a device is being added to the bus, we set the PCI domain
+	 * number to be the device serial number, which is non-zero and
+	 * unique on the same VM.  The serial numbers start with 1, and
+	 * increase by 1 for each device.  So device names including this
+	 * can have shorter names than based on the bus instance UUID.
+	 * Only the first device serial number is used for domain, so the
+	 * domain number will not change after the first device is added.
+	 * The lower 16 bits of the serial number is used, otherwise some
+	 * drivers may not be able to handle it.
+	 */
+	if (list_empty(&hbus->children))
+		hbus->sysdata.domain = desc->ser & 0xFFFF;
 	list_add_tail(&hpdev->list_entry, &hbus->children);
 	spin_unlock_irqrestore(&hbus->device_list_lock, flags);
 	return hpdev;
@@ -1901,11 +1926,11 @@
  */
 static int hv_pci_protocol_negotiation(struct hv_device *hdev)
 {
+	size_t i;
 	struct pci_version_request *version_req;
 	struct hv_pci_compl comp_pkt;
 	struct pci_packet *pkt;
 	int ret;
-	int i;
 
 	/*
 	 * Initiate the handshake with the host and negotiate
@@ -1931,7 +1956,7 @@
 				VMBUS_DATA_PACKET_FLAG_COMPLETION_REQUESTED);
 		if (ret) {
 			dev_err(&hdev->device,
-				"PCI Pass-through VSP failed sending version reqquest: %#x",
+				"PCI Pass-through VSP failed sending version request: %#x",
 				ret);
 			goto exit;
 		}
@@ -1948,7 +1973,7 @@
 
 		if (comp_pkt.completion_status != STATUS_REVISION_MISMATCH) {
 			dev_err(&hdev->device,
-				"PCI Pass-through VSP failed version request: %#x",
+				"PCI Pass-through VSP failed version request: %#x\n",
 				comp_pkt.completion_status);
 			ret = -EPROTO;
 			goto exit;
@@ -2260,10 +2285,12 @@
 		}
 		put_pcichild(hpdev, hv_pcidev_ref_by_slot);
 
-		ret = vmbus_sendpacket(hdev->channel, &pkt->message,
-				size_res, (unsigned long)pkt,
-				VM_PKT_DATA_INBAND,
-				VMBUS_DATA_PACKET_FLAG_COMPLETION_REQUESTED);
+		ret = vmbus_sendpacket(
+			hdev->channel, &pkt->message,
+			size_res,
+			(unsigned long)pkt,
+			VM_PKT_DATA_INBAND,
+			VMBUS_DATA_PACKET_FLAG_COMPLETION_REQUESTED);
 		if (ret)
 			break;
 
@@ -2492,7 +2519,7 @@
 static int hv_pci_remove(struct hv_device *hdev)
 {
 	struct hv_pcibus_device *hbus;
-
+ 
 	hbus = hv_get_drvdata(hdev);
 	if (hbus->state == hv_pcibus_installed) {
 		/* Remove the bus from PCI's point of view. */
@@ -2513,6 +2540,7 @@
 	hv_pci_free_bridge_windows(hbus);
 	put_hvpcibus(hbus);
 	wait_for_completion(&hbus->remove_event);
+
 	free_page((unsigned long)hbus);
 	return 0;
 }
@@ -2587,5 +2615,8 @@
 module_init(init_hv_pci_drv);
 module_exit(exit_hv_pci_drv);
 
-MODULE_DESCRIPTION("Hyper-V PCI");
-MODULE_LICENSE("GPL v2");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Microsoft Hyper-V PCI driver");
+MODULE_VERSION(HV_DRV_VERSION);
+MODULE_ALIAS("vmbus:1df6c444444400449d52802e27ede19f");
+
diff -Naur linux-3.10.0-957.el7.orig/drivers/scsi/storvsc_drv.c linux-3.10.0-957.el7.lis/drivers/scsi/storvsc_drv.c
--- linux-3.10.0-957.el7.orig/drivers/scsi/storvsc_drv.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/scsi/storvsc_drv.c	2018-12-12 00:51:40.869784268 +0000
@@ -32,6 +32,12 @@
 #include <linux/module.h>
 #include <linux/device.h>
 #include <linux/hyperv.h>
+/*
+ * Divergence from upstream commit: ead3700d893654d440edcb66fb3767a0c0db54cf
+ * storvsc: use cmd_size to allocate per-command data
+ */
+#include <linux/mempool.h>
+#include <asm/bug.h>
 #include <linux/blkdev.h>
 #include <scsi/scsi.h>
 #include <scsi/scsi_cmnd.h>
@@ -136,8 +142,9 @@
 #define SRB_FLAGS_PORT_DRIVER_RESERVED		0x0F000000
 #define SRB_FLAGS_CLASS_DRIVER_RESERVED		0xF0000000
 
-#define SP_UNTAGGED			((unsigned char) ~0)
-#define SRB_SIMPLE_TAG_REQUEST		0x20
+#define SP_UNTAGGED				((unsigned char) ~0)
+#define SRB_SIMPLE_TAG_REQUEST			0x20
+
 
 /*
  * Platform neutral description of a scsi request -
@@ -160,6 +167,8 @@
  */
 static int sense_buffer_size = PRE_WIN8_STORVSC_SENSE_BUFFER_SIZE;
 
+static struct mutex probe_mutex;
+
 /*
  * The storage protocol version is determined during the
  * initial exchange with the host.  It will indicate which
@@ -167,9 +176,13 @@
 */
 static int vmstor_proto_version;
 
-#define STORVSC_LOGGING_NONE	0
-#define STORVSC_LOGGING_ERROR	1
-#define STORVSC_LOGGING_WARN	2
+/*
+ * Divergence from upstream:
+ * This logging should be added to upstream.
+ */
+#define STORVSC_LOGGING_NONE   0
+#define STORVSC_LOGGING_ERROR  1
+#define STORVSC_LOGGING_WARN   2
 
 static int logging_level = STORVSC_LOGGING_ERROR;
 module_param(logging_level, int, S_IRUGO|S_IWUSR);
@@ -380,11 +393,20 @@
 #define SRB_STATUS_DATA_OVERRUN	0x12
 
 #define SRB_STATUS(status) \
-	(status & ~(SRB_STATUS_AUTOSENSE_VALID | SRB_STATUS_QUEUE_FROZEN))
+	(status & ~(SRB_STATUS_QUEUE_FROZEN))
+
 /*
  * This is the end of Protocol specific defines.
  */
 
+
+/*
+ * We setup a mempool to allocate request structures for this driver
+ * on a per-lun basis. The following define specifies the number of
+ * elements in the pool.
+ */
+
+#define STORVSC_MIN_BUF_NR				64
 static int storvsc_ringbuffer_size = (256 * PAGE_SIZE);
 static u32 max_outstanding_req_per_channel;
 
@@ -395,18 +417,12 @@
 
 module_param(storvsc_vcpus_per_sub_channel, int, S_IRUGO);
 MODULE_PARM_DESC(storvsc_vcpus_per_sub_channel, "Ratio of VCPUs to subchannels");
-
-static int ring_avail_percent_lowater = 10;
-module_param(ring_avail_percent_lowater, int, S_IRUGO);
-MODULE_PARM_DESC(ring_avail_percent_lowater,
-		"Select a channel if available ring size > this in percent");
-
 /*
  * Timeout in seconds for all devices managed by this driver.
  */
 static int storvsc_timeout = 180;
 
-#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+#if defined(CONFIG_SCSI_FC_ATTRS) || defined(CONFIG_SCSI_FC_ATTRS_MODULE)
 static struct scsi_transport_template *fc_transport_template;
 #endif
 
@@ -425,13 +441,25 @@
 #define STORVSC_IDE_MAX_CHANNELS			1
 
 struct storvsc_cmd_request {
+	struct list_head entry;
 	struct scsi_cmnd *cmd;
 
+	/*
+	 * Divergence from upstream commit 81988a0e6b031bc80da15257201810ddcf989e64
+	 * Bounce buffer is needed for RH7 and below, due
+	 * to possible gaps in sg list. Bounce buffers are
+	 * eliminated upstream with calls to blk_queue_virt_boundary().
+	 */
+	unsigned int bounce_sgl_count;
+	struct scatterlist *bounce_sgl;
+
 	struct hv_device *device;
 
 	/* Synchronize the request/response if needed */
 	struct completion wait_event;
 
+	unsigned char *sense_buffer;
+
 	struct vmbus_channel_packet_multipage_buffer mpb;
 	struct vmbus_packet_mpb_array *payload;
 	u32 payload_sz;
@@ -487,12 +515,18 @@
 #endif
 };
 
+struct stor_mem_pools {
+	struct kmem_cache *request_pool;
+	mempool_t *request_mempool;
+};
+
 struct hv_host_device {
 	struct hv_device *dev;
 	unsigned int port;
 	unsigned char path;
 	unsigned char target;
 	struct workqueue_struct *handle_error_wq;
+	struct mutex host_mutex;
 	struct work_struct host_scan_work;
 	struct Scsi_Host *host;
 };
@@ -500,8 +534,8 @@
 struct storvsc_scan_work {
 	struct work_struct work;
 	struct Scsi_Host *host;
-	u8 lun;
-	u8 tgt_id;
+	uint lun;
+	uint tgt_id;
 };
 
 static void storvsc_device_scan(struct work_struct *work)
@@ -521,6 +555,27 @@
 	kfree(wrk);
 }
 
+/*
+ * Divergence from upstream commit: 2a09ed3d97ff8b5b377f86da9b9afd9ebd97b362
+ * storvsc_host_scan() is supposed call scsi_scan_host() instead of
+ * storvsc_bus_scan(). On RH7, this results in kernel oops and soft
+ * lockups in the call path when hot-adding disks. Keeping original.
+ */
+static void storvsc_bus_scan(struct Scsi_Host *host)
+{
+	int id, order_id;
+
+	for (id = 0; id < host->max_id; ++id) {
+		if (host->reverse_ordering)
+			order_id = host->max_id - id - 1;
+		else
+			order_id = id;
+
+		scsi_scan_target(&host->shost_gendev, 0,
+				 order_id, SCAN_WILD_CARD, 1);
+	}
+}
+
 static void storvsc_host_scan(struct work_struct *work)
 {
 	struct Scsi_Host *host;
@@ -547,24 +602,32 @@
 	/*
 	 * Now scan the host to discover LUNs that may have been added.
 	 */
-	scsi_scan_host(host);
+	storvsc_bus_scan(host);
 }
 
 static void storvsc_remove_lun(struct work_struct *work)
 {
 	struct storvsc_scan_work *wrk;
 	struct scsi_device *sdev;
+	struct hv_host_device *host_dev;
 
 	wrk = container_of(work, struct storvsc_scan_work, work);
 	if (!scsi_host_get(wrk->host))
 		goto done;
 
+	host_dev = shost_priv(wrk->host);
+
+	mutex_lock(&host_dev->host_mutex);
+
 	sdev = scsi_device_lookup(wrk->host, 0, wrk->tgt_id, wrk->lun);
 
 	if (sdev) {
-		scsi_remove_device(sdev);
+		if (sdev->sdev_state != SDEV_DEL)
+			scsi_remove_device(sdev);
 		scsi_device_put(sdev);
 	}
+
+	mutex_unlock(&host_dev->host_mutex);
 	scsi_host_put(wrk->host);
 
 done:
@@ -633,6 +696,238 @@
 
 }
 
+static void destroy_bounce_buffer(struct scatterlist *sgl,
+				  unsigned int sg_count)
+{
+	int i;
+	struct page *page_buf;
+
+	for (i = 0; i < sg_count; i++) {
+		page_buf = sg_page((&sgl[i]));
+		if (page_buf != NULL)
+			__free_page(page_buf);
+	}
+
+	kfree(sgl);
+}
+
+static int do_bounce_buffer(struct scatterlist *sgl, unsigned int sg_count)
+{
+	int i;
+
+	/* No need to check */
+	if (sg_count < 2)
+		return -1;
+
+	/* We have at least 2 sg entries */
+	for (i = 0; i < sg_count; i++) {
+		if (i == 0) {
+			/* make sure 1st one does not have hole */
+			if (sgl->offset + sgl->length != PAGE_SIZE)
+				return i;
+		} else if (i == sg_count - 1) {
+			/* make sure last one does not have hole */
+			if (sgl->offset != 0)
+				return i;
+		} else {
+			/* make sure no hole in the middle */
+			if (sgl->length != PAGE_SIZE || sgl->offset != 0)
+				return i;
+		}
+		sgl = sg_next(sgl);
+	}
+	return -1;
+}
+
+static struct scatterlist *create_bounce_buffer(struct scatterlist *sgl,
+						unsigned int sg_count,
+						unsigned int len,
+						int write)
+{
+	int i;
+	int num_pages;
+	struct scatterlist *bounce_sgl;
+	struct page *page_buf;
+	unsigned int buf_len = ((write == WRITE_TYPE) ? 0 : PAGE_SIZE);
+
+	num_pages = ALIGN(len, PAGE_SIZE) >> PAGE_SHIFT;
+
+	bounce_sgl = kcalloc(num_pages, sizeof(struct scatterlist), GFP_ATOMIC);
+	if (!bounce_sgl)
+		return NULL;
+
+	sg_init_table(bounce_sgl, num_pages);
+	for (i = 0; i < num_pages; i++) {
+		page_buf = alloc_page(GFP_ATOMIC);
+		if (!page_buf)
+			goto cleanup;
+		sg_set_page(&bounce_sgl[i], page_buf, buf_len, 0);
+	}
+
+	return bounce_sgl;
+
+cleanup:
+	destroy_bounce_buffer(bounce_sgl, num_pages);
+	return NULL;
+}
+
+/* Assume the original sgl has enough room */
+static unsigned int copy_from_bounce_buffer(struct scatterlist *orig_sgl,
+					    struct scatterlist *bounce_sgl,
+					    unsigned int orig_sgl_count,
+					    unsigned int bounce_sgl_count)
+{
+	int i;
+	int j = 0;
+	unsigned long src, dest;
+	unsigned int srclen, destlen, copylen;
+	unsigned int total_copied = 0;
+	unsigned long bounce_addr = 0;
+	unsigned long dest_addr = 0;
+	unsigned long flags;
+	struct scatterlist *cur_dest_sgl;
+	struct scatterlist *cur_src_sgl;
+
+	local_irq_save(flags);
+	cur_dest_sgl = orig_sgl;
+	cur_src_sgl = bounce_sgl;
+	for (i = 0; i < orig_sgl_count; i++) {
+		dest_addr = (unsigned long)
+				kmap_atomic(sg_page(cur_dest_sgl)) +
+				cur_dest_sgl->offset;
+		dest = dest_addr;
+		destlen = cur_dest_sgl->length;
+
+		if (bounce_addr == 0)
+			bounce_addr = (unsigned long)kmap_atomic(
+							sg_page(cur_src_sgl));
+
+		while (destlen) {
+			src = bounce_addr + cur_src_sgl->offset;
+			srclen = cur_src_sgl->length - cur_src_sgl->offset;
+
+			copylen = min(srclen, destlen);
+			memcpy((void *)dest, (void *)src, copylen);
+
+			total_copied += copylen;
+			cur_src_sgl->offset += copylen;
+			destlen -= copylen;
+			dest += copylen;
+
+			if (cur_src_sgl->offset == cur_src_sgl->length) {
+				/* full */
+				kunmap_atomic((void *)bounce_addr);
+				j++;
+				/*
+				 * It is possible that the number of elements
+				 * in the bounce buffer may not be equal to
+				 * the number of elements in the original
+				 * scatter list. Handle this correctly.
+				 */
+
+				if (j == bounce_sgl_count) {
+					/*
+					 * We are done; cleanup and return.
+					 */
+					kunmap_atomic((void *)(dest_addr -
+						cur_dest_sgl->offset));
+					local_irq_restore(flags);
+					return total_copied;
+				}
+
+				/* if we need to use another bounce buffer */
+				if (destlen || i != orig_sgl_count - 1) {
+					cur_src_sgl = sg_next(cur_src_sgl);
+					bounce_addr = (unsigned long)
+							kmap_atomic(
+							sg_page(cur_src_sgl));
+				}
+			} else if (destlen == 0 && i == orig_sgl_count - 1) {
+				/* unmap the last bounce that is < PAGE_SIZE */
+				kunmap_atomic((void *)bounce_addr);
+			}
+		}
+
+		kunmap_atomic((void *)(dest_addr - cur_dest_sgl->offset));
+		cur_dest_sgl = sg_next(cur_dest_sgl);
+	}
+
+	local_irq_restore(flags);
+	return total_copied;
+}
+
+/* Assume the bounce_sgl has enough room ie using the create_bounce_buffer() */
+static unsigned int copy_to_bounce_buffer(struct scatterlist *orig_sgl,
+					  struct scatterlist *bounce_sgl,
+					  unsigned int orig_sgl_count)
+{
+	int i;
+	unsigned long src, dest;
+	unsigned int srclen, destlen, copylen;
+	unsigned int total_copied = 0;
+	unsigned long bounce_addr = 0;
+	unsigned long src_addr = 0;
+	unsigned long flags;
+
+	struct scatterlist *cur_src_sgl;
+	struct scatterlist *cur_dest_sgl;
+
+	local_irq_save(flags);
+
+	cur_src_sgl = orig_sgl;
+	cur_dest_sgl = bounce_sgl;
+
+	for (i = 0; i < orig_sgl_count; i++) {
+		src_addr = (unsigned long)
+				kmap_atomic(sg_page(cur_src_sgl)) +
+				cur_src_sgl->offset;
+		src = src_addr;
+		srclen = cur_src_sgl->length;
+
+		if (bounce_addr == 0)
+			bounce_addr = (unsigned long)
+					kmap_atomic(sg_page(cur_dest_sgl));
+
+		while (srclen) {
+			/* assume bounce offset always == 0 */
+			dest = bounce_addr + cur_dest_sgl->length;
+			destlen = PAGE_SIZE - cur_dest_sgl->length;
+
+			copylen = min(srclen, destlen);
+			memcpy((void *)dest, (void *)src, copylen);
+
+			total_copied += copylen;
+			cur_dest_sgl->length += copylen;
+			srclen -= copylen;
+			src += copylen;
+
+			if (cur_dest_sgl->length == PAGE_SIZE) {
+				/* full..move to next entry */
+				kunmap_atomic((void *)bounce_addr);
+				bounce_addr = 0;
+			}
+
+			/* if we need to use another bounce buffer */
+			if (srclen && bounce_addr == 0) {
+				cur_dest_sgl = sg_next(cur_dest_sgl);
+				bounce_addr = (unsigned long)
+						kmap_atomic(
+						sg_page(cur_dest_sgl));
+			}
+		}
+
+		kunmap_atomic((void *)(src_addr - cur_src_sgl->offset));
+		cur_src_sgl = sg_next(cur_src_sgl);
+	}
+
+	if (bounce_addr)
+		kunmap_atomic((void *)bounce_addr);
+
+	local_irq_restore(flags);
+
+	return total_copied;
+}
+
 static void handle_sc_creation(struct vmbus_channel *new_sc)
 {
 	struct hv_device *device = new_sc->primary_channel->device_obj;
@@ -748,7 +1043,6 @@
 	}
 }
 
-
 static int storvsc_execute_vstor_op(struct hv_device *device,
 				    struct storvsc_cmd_request *request,
 				    bool status_check)
@@ -826,6 +1120,7 @@
 		 * The revision number is only used in Windows; set it to 0.
 		 */
 		vstor_packet->version.revision = 0;
+
 		ret = storvsc_execute_vstor_op(device, request, false);
 		if (ret != 0)
 			return ret;
@@ -853,6 +1148,7 @@
 
 	memset(vstor_packet, 0, sizeof(struct vstor_packet));
 	vstor_packet->operation = VSTOR_OPERATION_QUERY_PROPERTIES;
+
 	ret = storvsc_execute_vstor_op(device, request, true);
 	if (ret != 0)
 		return ret;
@@ -864,6 +1160,7 @@
 	 */
 	max_chns = vstor_packet->storage_channel_properties.max_channel_cnt;
 
+
 	/*
 	 * Allocate state to manage the sub-channels.
 	 * We allocate an array based on the numbers of possible CPUs
@@ -873,7 +1170,7 @@
 	 * We will however populate all the slots to evenly distribute
 	 * the load.
 	 */
-	stor_device->stor_chns = kzalloc(sizeof(void *) * num_possible_cpus(),
+	stor_device->stor_chns = kcalloc(num_possible_cpus(), sizeof(void *),
 					 GFP_KERNEL);
 	if (stor_device->stor_chns == NULL)
 		return -ENOMEM;
@@ -898,12 +1195,13 @@
 	 */
 	memset(vstor_packet, 0, sizeof(struct vstor_packet));
 	vstor_packet->operation = VSTOR_OPERATION_FCHBA_DATA;
+
 	ret = storvsc_execute_vstor_op(device, request, true);
 	if (ret != 0)
 		return ret;
 
 	/*
-	 * Cache the currently active port and node ww names.
+	 * Cache the currently active port and node wwn names.
 	 */
 	cache_wwn(stor_device, vstor_packet);
 
@@ -911,6 +1209,7 @@
 
 	memset(vstor_packet, 0, sizeof(struct vstor_packet));
 	vstor_packet->operation = VSTOR_OPERATION_END_INITIALIZATION;
+
 	ret = storvsc_execute_vstor_op(device, request, true);
 	if (ret != 0)
 		return ret;
@@ -931,6 +1230,11 @@
 	struct hv_host_device *host_dev = shost_priv(host);
 	bool do_work = false;
 
+	/*
+	 * Divergence from upstream:
+	 * Addresses an error handling bug on older kernels.
+	 */
+
 	switch (SRB_STATUS(vm_srb->srb_status)) {
 	case SRB_STATUS_ERROR:
 		/*
@@ -1004,9 +1308,11 @@
 				       struct storvsc_device *stor_dev)
 {
 	struct scsi_cmnd *scmnd = cmd_request->cmd;
+	void (*scsi_done_fn)(struct scsi_cmnd *);
 	struct scsi_sense_hdr sense_hdr;
 	struct vmscsi_request *vm_srb;
 	u32 data_transfer_length;
+	struct stor_mem_pools *memp = scmnd->device->hostdata;
 	struct Scsi_Host *host;
 	u32 payload_sz = cmd_request->payload_sz;
 	void *payload = cmd_request->payload;
@@ -1016,16 +1322,30 @@
 	vm_srb = &cmd_request->vstor_packet.vm_srb;
 	data_transfer_length = vm_srb->data_transfer_length;
 
+	if (cmd_request->bounce_sgl_count) {
+		if (vm_srb->data_in == READ_TYPE)
+			copy_from_bounce_buffer(scsi_sglist(scmnd),
+					cmd_request->bounce_sgl,
+					scsi_sg_count(scmnd),
+					cmd_request->bounce_sgl_count);
+		destroy_bounce_buffer(cmd_request->bounce_sgl,
+					cmd_request->bounce_sgl_count);
+	}
+
 	scmnd->result = vm_srb->scsi_status;
 
 	if (scmnd->result) {
 		if (scsi_normalize_sense(scmnd->sense_buffer,
 				SCSI_SENSE_BUFFERSIZE, &sense_hdr) &&
-		    !(sense_hdr.sense_key == NOT_READY &&
+                   !(sense_hdr.sense_key == NOT_READY &&
 				 sense_hdr.asc == 0x03A) &&
-		    do_logging(STORVSC_LOGGING_ERROR))
+		   do_logging(STORVSC_LOGGING_ERROR))
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,1))
 			scsi_print_sense_hdr(scmnd->device, "storvsc",
 					     &sense_hdr);
+#else
+			scsi_print_sense_hdr("storvsc", &sense_hdr);
+#endif
 	}
 
 	if (vm_srb->srb_status != SRB_STATUS_SUCCESS) {
@@ -1043,11 +1363,43 @@
 	scsi_set_resid(scmnd,
 		cmd_request->payload->range.len - data_transfer_length);
 
-	scmnd->scsi_done(scmnd);
+	/* If this is an INQUIRY when SCSI is trying to probe a LUN, return a proper 
+	 * SCSI level and vendor/device names to trigger REPORT_LUNS scan
+	 * note: on probing LUN0, SCSI sets all the fields  to zero except for the length at [4]
+	 * the SCSI layer expects at least 36 bytes returned on INQUIRY response
+	 */
+	if (scmnd->cmnd[0] == INQUIRY
+	    && !(scmnd->cmnd[1] | scmnd->cmnd[2] | scmnd->cmnd[3] | scmnd->cmnd[5])
+	    && scsi_bufflen(scmnd) >= 32) {
+
+		struct scatterlist *sgl = scsi_sglist(scmnd);
+		char *data = kmap_atomic(sg_page(sgl)) + sgl->offset;
+
+		/* if the host doesn't return any data (0 length), set them properly */
+		if (!data[4]) {
+			/* if host doesn't return SCSI level, set to SCSI_2 minimal required for REPORT_LUNS */
+			if (!data[2])
+				data[2] = SCSI_2;
+
+			sprintf(&data[8], "MSFT"); 	// vendor name, max 8 bytes
+			sprintf(&data[16], "LUN");	// device name, max 16 bytes
+		}
+
+		kunmap_atomic((void *)data - sgl->offset);
+	}
+
+	scsi_done_fn = scmnd->scsi_done;
+
+	scmnd->host_scribble = NULL;
+	scmnd->scsi_done = NULL;
+
+	scsi_done_fn(scmnd);
 
 	if (payload_sz >
 		sizeof(struct vmbus_channel_packet_multipage_buffer))
 		kfree(payload);
+
+	mempool_free(cmd_request, memp->request_mempool);
 }
 
 static void storvsc_on_io_completion(struct storvsc_device *stor_device,
@@ -1101,7 +1453,7 @@
 				"stor pkt %p autosense data valid - len %d\n",
 				request, vstor_packet->vm_srb.sense_info_length);
 
-			memcpy(request->cmd->sense_buffer,
+			memcpy(request->sense_buffer,
 			       vstor_packet->vm_srb.sense_data,
 			       vstor_packet->vm_srb.sense_info_length);
 
@@ -1136,10 +1488,9 @@
 		queue_work(
 			host_dev->handle_error_wq, &host_dev->host_scan_work);
 		break;
-
 	case VSTOR_OPERATION_FCHBA_DATA:
 		cache_wwn(stor_device, vstor_packet);
-#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+#if defined(CONFIG_SCSI_FC_ATTRS) || defined(CONFIG_SCSI_FC_ATTRS_MODULE)
 		fc_host_node_name(stor_device->host) = stor_device->node_name;
 		fc_host_port_name(stor_device->host) = stor_device->port_name;
 #endif
@@ -1247,7 +1598,7 @@
 {
 	u16 slot = 0;
 	u16 hash_qnum;
-	const struct cpumask *node_mask;
+	struct cpumask alloced_mask;
 	int num_channels, tgt_cpu;
 
 	if (stor_device->num_sc == 0)
@@ -1263,13 +1614,10 @@
 	 * III. Mapping is persistent.
 	 */
 
-	node_mask = cpumask_of_node(cpu_to_node(q_num));
+	cpumask_and(&alloced_mask, &stor_device->alloced_cpus,
+		    cpumask_of_node(cpu_to_node(q_num)));
 
-	num_channels = 0;
-	for_each_cpu(tgt_cpu, &stor_device->alloced_cpus) {
-		if (cpumask_test_cpu(tgt_cpu, node_mask))
-			num_channels++;
-	}
+	num_channels = cpumask_weight(&alloced_mask);
 	if (num_channels == 0)
 		return stor_device->device->channel;
 
@@ -1277,9 +1625,7 @@
 	while (hash_qnum >= num_channels)
 		hash_qnum -= num_channels;
 
-	for_each_cpu(tgt_cpu, &stor_device->alloced_cpus) {
-		if (!cpumask_test_cpu(tgt_cpu, node_mask))
-			continue;
+	for_each_cpu(tgt_cpu, &alloced_mask) {
 		if (slot == hash_qnum)
 			break;
 		slot++;
@@ -1290,15 +1636,14 @@
 	return stor_device->stor_chns[q_num];
 }
 
-
 static int storvsc_do_io(struct hv_device *device,
 			 struct storvsc_cmd_request *request, u16 q_num)
 {
 	struct storvsc_device *stor_device;
 	struct vstor_packet *vstor_packet;
-	struct vmbus_channel *outgoing_channel, *channel;
+	struct vmbus_channel *outgoing_channel;
 	int ret = 0;
-	const struct cpumask *node_mask;
+	struct cpumask alloced_mask;
 	int tgt_cpu;
 
 	vstor_packet = &request->vstor_packet;
@@ -1311,53 +1656,24 @@
 	request->device  = device;
 	/*
 	 * Select an an appropriate channel to send the request out.
+	 * We will base the request based on the CPU that is presenting
+	 * the I/O request.
 	 */
 	if (stor_device->stor_chns[q_num] != NULL) {
 		outgoing_channel = stor_device->stor_chns[q_num];
-		if (outgoing_channel->target_cpu == q_num) {
+		if (outgoing_channel->target_cpu == smp_processor_id()) {
 			/*
 			 * Ideally, we want to pick a different channel if
 			 * available on the same NUMA node.
 			 */
-			node_mask = cpumask_of_node(cpu_to_node(q_num));
-			for_each_cpu_wrap(tgt_cpu,
-				 &stor_device->alloced_cpus, q_num + 1) {
-				if (!cpumask_test_cpu(tgt_cpu, node_mask))
-					continue;
-				if (tgt_cpu == q_num)
-					continue;
-				channel = stor_device->stor_chns[tgt_cpu];
-				if (hv_get_avail_to_write_percent(
-							&channel->outbound)
-						> ring_avail_percent_lowater) {
-					outgoing_channel = channel;
-					goto found_channel;
-				}
-			}
-
-			/*
-			 * All the other channels on the same NUMA node are
-			 * busy. Try to use the channel on the current CPU
-			 */
-			if (hv_get_avail_to_write_percent(
-						&outgoing_channel->outbound)
-					> ring_avail_percent_lowater)
-				goto found_channel;
-
-			/*
-			 * If we reach here, all the channels on the current
-			 * NUMA node are busy. Try to find a channel in
-			 * other NUMA nodes
-			 */
-			for_each_cpu(tgt_cpu, &stor_device->alloced_cpus) {
-				if (cpumask_test_cpu(tgt_cpu, node_mask))
-					continue;
-				channel = stor_device->stor_chns[tgt_cpu];
-				if (hv_get_avail_to_write_percent(
-							&channel->outbound)
-						> ring_avail_percent_lowater) {
-					outgoing_channel = channel;
-					goto found_channel;
+			cpumask_and(&alloced_mask, &stor_device->alloced_cpus,
+				    cpumask_of_node(cpu_to_node(q_num)));
+			for_each_cpu_wrap(tgt_cpu, &alloced_mask,
+					outgoing_channel->target_cpu + 1) {
+				if (tgt_cpu != outgoing_channel->target_cpu) {
+					outgoing_channel =
+					stor_device->stor_chns[tgt_cpu];
+					break;
 				}
 			}
 		}
@@ -1365,7 +1681,6 @@
 		outgoing_channel = get_og_chn(stor_device, q_num);
 	}
 
-found_channel:
 	vstor_packet->flags |= REQUEST_COMPLETION_FLAG;
 
 	vstor_packet->vm_srb.length = (sizeof(struct vmscsi_request) -
@@ -1407,6 +1722,30 @@
 
 static int storvsc_device_alloc(struct scsi_device *sdevice)
 {
+	struct stor_mem_pools *memp;
+	int number = STORVSC_MIN_BUF_NR;
+
+	memp = kzalloc(sizeof(struct stor_mem_pools), GFP_KERNEL);
+	if (!memp)
+		return -ENOMEM;
+
+	memp->request_pool =
+		kmem_cache_create(dev_name(&sdevice->sdev_dev),
+				sizeof(struct storvsc_cmd_request), 0,
+				SLAB_HWCACHE_ALIGN, NULL);
+
+	if (!memp->request_pool)
+		goto err0;
+
+	memp->request_mempool = mempool_create(number, mempool_alloc_slab,
+						mempool_free_slab,
+						memp->request_pool);
+
+	if (!memp->request_mempool)
+		goto err1;
+
+	sdevice->hostdata = memp;
+
 	/*
 	 * Set blist flag to permit the reading of the VPD pages even when
 	 * the target may claim SPC-2 compliance. MSFT targets currently
@@ -1416,19 +1755,40 @@
 	 * Hypervisor reports SCSI_UNKNOWN type for DVD ROM device but
 	 * still supports REPORT LUN.
 	 */
-	sdevice->sdev_bflags = BLIST_REPORTLUN2 | BLIST_TRY_VPD_PAGES;
+	sdevice->sdev_bflags = BLIST_REPORTLUN2;
 
 	return 0;
+
+err1:
+	kmem_cache_destroy(memp->request_pool);
+
+err0:
+	kfree(memp);
+	return -ENOMEM;
+}
+
+static void storvsc_device_destroy(struct scsi_device *sdevice)
+{
+	struct stor_mem_pools *memp = sdevice->hostdata;
+
+	if (!memp)
+		return;
+
+	mempool_destroy(memp->request_mempool);
+	kmem_cache_destroy(memp->request_pool);
+	kfree(memp);
+	sdevice->hostdata = NULL;
 }
 
 static int storvsc_device_configure(struct scsi_device *sdevice)
 {
-	blk_queue_rq_timeout(sdevice->request_queue, (storvsc_timeout * HZ));
 
-	/* Ensure there are no gaps in presented sgls */
-	blk_queue_virt_boundary(sdevice->request_queue, PAGE_SIZE - 1);
+	blk_queue_bounce_limit(sdevice->request_queue, BLK_BOUNCE_ANY);
+
+	blk_queue_rq_timeout(sdevice->request_queue, (storvsc_timeout * HZ));
 
 	sdevice->no_write_same = 1;
+
 	/*
 	 * If the host is WIN8 or WIN8 R2, claim conformance to SPC-3
 	 * if the device is a MSFT virtual device.  If the host is
@@ -1530,6 +1890,10 @@
  */
 static enum blk_eh_timer_return storvsc_eh_timed_out(struct scsi_cmnd *scmnd)
 {
+#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+	if (scmnd->device->host->transportt == fc_transport_template)
+		return fc_eh_timed_out(scmnd);
+#endif
 	return BLK_EH_RESET_TIMER;
 }
 
@@ -1541,12 +1905,21 @@
 	switch (scsi_op) {
 	/* the host does not handle WRITE_SAME, log accident usage */
 	case WRITE_SAME:
+	case WRITE_SAME_16:
 	/*
 	 * smartd sends this command and the host does not handle
 	 * this. So, don't send it.
 	 */
 	case SET_WINDOW:
-		scmnd->result = ILLEGAL_REQUEST << 16;
+		/*
+		 * This returned result is an expected divergence from
+		 * upstream code.
+		 */
+                scsi_build_sense_buffer(0, scmnd->sense_buffer, ILLEGAL_REQUEST,
+                    0x20, 0);
+                scmnd->result = SAM_STAT_CHECK_CONDITION;
+                set_driver_byte(scmnd, DRIVER_SENSE);
+                set_host_byte(scmnd, DID_ABORT);
 		allowed = false;
 		break;
 	default:
@@ -1555,22 +1928,35 @@
 	return allowed;
 }
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,32))
+static int storvsc_queuecommand(struct scsi_cmnd *scmnd,
+	void (*done)(struct scsi_cmnd *scmnd))
+{
+	struct Scsi_Host *host = scmnd->device->host;
+
+#else
+
 static int storvsc_queuecommand(struct Scsi_Host *host, struct scsi_cmnd *scmnd)
 {
+#endif
+
 	int ret;
 	struct hv_host_device *host_dev = shost_priv(host);
 	struct hv_device *dev = host_dev->dev;
-	struct storvsc_cmd_request *cmd_request =
-		(struct storvsc_cmd_request *)(scmnd + 1);
+	struct storvsc_cmd_request *cmd_request;
+	unsigned int request_size = 0;
 	int i;
 	struct scatterlist *sgl;
 	unsigned int sg_count = 0;
 	struct vmscsi_request *vm_srb;
+	struct stor_mem_pools *memp = scmnd->device->hostdata;
 	struct scatterlist *cur_sgl;
+
 	struct vmbus_packet_mpb_array  *payload;
 	u32 payload_sz;
 	u32 length;
 
+
 	if (vmstor_proto_version <= VMSTOR_PROTO_VERSION_WIN8) {
 		/*
 		 * On legacy hosts filter unimplemented commands.
@@ -1586,18 +1972,33 @@
 		}
 	}
 
+	request_size = sizeof(struct storvsc_cmd_request);
+
+	cmd_request = mempool_alloc(memp->request_mempool,
+				       GFP_ATOMIC);
+
+	/*
+	 * We might be invoked in an interrupt context; hence
+	 * mempool_alloc() can fail.
+	 */
+	if (!cmd_request)
+		return SCSI_MLQUEUE_DEVICE_BUSY;
+
+	memset(cmd_request, 0, sizeof(struct storvsc_cmd_request));
+
 	/* Setup the cmd request */
 	cmd_request->cmd = scmnd;
 
+	scmnd->host_scribble = (unsigned char *)cmd_request;
+
 	vm_srb = &cmd_request->vstor_packet.vm_srb;
 	vm_srb->win8_extension.time_out_value = 60;
 
 	vm_srb->win8_extension.srb_flags |=
 		SRB_FLAGS_DISABLE_SYNCH_TRANSFER;
 
-	if (scmnd->device->tagged_supported) {
-		vm_srb->win8_extension.srb_flags |=
-		(SRB_FLAGS_QUEUE_ACTION_ENABLE | SRB_FLAGS_NO_QUEUE_FREEZE);
+	if(scmnd->device->tagged_supported) {
+		vm_srb->win8_extension.srb_flags |= (SRB_FLAGS_QUEUE_ACTION_ENABLE | SRB_FLAGS_NO_QUEUE_FREEZE);
 		vm_srb->win8_extension.queue_tag = SP_UNTAGGED;
 		vm_srb->win8_extension.queue_action = SRB_SIMPLE_TAG_REQUEST;
 	}
@@ -1636,6 +2037,9 @@
 
 	memcpy(vm_srb->cdb, scmnd->cmnd, vm_srb->cdb_length);
 
+	cmd_request->sense_buffer = scmnd->sense_buffer;
+
+
 	sgl = (struct scatterlist *)scsi_sglist(scmnd);
 	sg_count = scsi_sg_count(scmnd);
 
@@ -1644,18 +2048,46 @@
 	payload_sz = sizeof(cmd_request->mpb);
 
 	if (sg_count) {
+		/* check if we need to bounce the sgl */
+		if (do_bounce_buffer(sgl, scsi_sg_count(scmnd)) != -1) {
+			cmd_request->bounce_sgl =
+				create_bounce_buffer(sgl, sg_count,
+						     length,
+						     vm_srb->data_in);
+			if (!cmd_request->bounce_sgl) {
+				ret = SCSI_MLQUEUE_HOST_BUSY;
+				goto queue_error;
+			}
+
+			cmd_request->bounce_sgl_count =
+				ALIGN(length, PAGE_SIZE) >> PAGE_SHIFT;
+
+			if (vm_srb->data_in == WRITE_TYPE)
+				copy_to_bounce_buffer(sgl,
+					cmd_request->bounce_sgl, sg_count);
+
+			sgl = cmd_request->bounce_sgl;
+			sg_count = cmd_request->bounce_sgl_count;
+		}
+
+
 		if (sg_count > MAX_PAGE_BUFFER_COUNT) {
 
-			payload_sz = (sg_count * sizeof(void *) +
+			payload_sz = (sg_count * sizeof(u64) +
 				      sizeof(struct vmbus_packet_mpb_array));
-			payload = kmalloc(payload_sz, GFP_ATOMIC);
-			if (!payload)
+			payload = kzalloc(payload_sz, GFP_ATOMIC);
+			if (!payload) {
+				if (cmd_request->bounce_sgl_count)
+					destroy_bounce_buffer(
+					cmd_request->bounce_sgl,
+					cmd_request->bounce_sgl_count);
+
 				return SCSI_MLQUEUE_DEVICE_BUSY;
+			}
 		}
 
 		payload->range.len = length;
 		payload->range.offset = sgl[0].offset;
-
 		cur_sgl = sgl;
 		for (i = 0; i < sg_count; i++) {
 			payload->range.pfn_array[i] =
@@ -1666,7 +2098,6 @@
 
 	cmd_request->payload = payload;
 	cmd_request->payload_sz = payload_sz;
-
 	/* Invokes the vsc to start an IO */
 	ret = storvsc_do_io(dev, cmd_request, get_cpu());
 	put_cpu();
@@ -1675,29 +2106,48 @@
 		if (payload_sz > sizeof(cmd_request->mpb))
 			kfree(payload);
 		/* no more space */
-		return SCSI_MLQUEUE_DEVICE_BUSY;
+
+		if (cmd_request->bounce_sgl_count)
+			destroy_bounce_buffer(cmd_request->bounce_sgl,
+					cmd_request->bounce_sgl_count);
+
+		ret = SCSI_MLQUEUE_DEVICE_BUSY;
+		goto queue_error;
 	}
 
 	return 0;
+
+queue_error:
+	mempool_free(cmd_request, memp->request_mempool);
+	scmnd->host_scribble = NULL;
+	return ret;
 }
 
+#ifdef CONFIG_X86_64
+#define STORVSC_TABLE_SEZE 512
+#else
+#define STORVSC_TABLE_SEZE 32
+#endif
+
 static struct scsi_host_template scsi_driver = {
 	.module	=		THIS_MODULE,
 	.name =			"storvsc_host_t",
-	.cmd_size =             sizeof(struct storvsc_cmd_request),
 	.bios_param =		storvsc_get_chs,
 	.queuecommand =		storvsc_queuecommand,
 	.eh_host_reset_handler =	storvsc_host_reset_handler,
-	.proc_name =		"storvsc_host",
 	.eh_timed_out =		storvsc_eh_timed_out,
 	.slave_alloc =		storvsc_device_alloc,
+	.slave_destroy =	storvsc_device_destroy,
 	.slave_configure =	storvsc_device_configure,
 	.cmd_per_lun =		2048,
 	.this_id =		-1,
+	.sg_tablesize = STORVSC_TABLE_SEZE,
 	.use_clustering =	ENABLE_CLUSTERING,
 	/* Make sure we dont get a sg segment crosses a page boundary */
 	.dma_boundary =		PAGE_SIZE-1,
-	.no_write_same =	1,
+#ifdef NOTYET
+	.track_queue_depth =	1,
+#endif
 };
 
 enum {
@@ -1741,6 +2191,7 @@
 	int max_channels;
 	int max_sub_channels = 0;
 
+	mutex_lock(&probe_mutex);
 	/*
 	 * Based on the windows host we are running on,
 	 * set state to properly communicate with the host.
@@ -1755,24 +2206,22 @@
 		max_targets = STORVSC_MAX_TARGETS;
 		max_channels = STORVSC_MAX_CHANNELS;
 		/*
-		 * On Windows8 and above, we support sub-channels for storage
-		 * on SCSI and FC controllers.
+		 * On Windows8 and above, we support sub-channels for storage.
 		 * The number of sub-channels offerred is based on the number of
 		 * VCPUs in the guest.
 		 */
-		if (!dev_is_ide)
-			max_sub_channels =
-				(num_cpus - 1) / storvsc_vcpus_per_sub_channel;
+		max_sub_channels = (num_cpus / storvsc_vcpus_per_sub_channel);
 	}
 
-	scsi_driver.can_queue = max_outstanding_req_per_channel *
-				(max_sub_channels + 1) *
-				(100 - ring_avail_percent_lowater) / 100;
+	scsi_driver.can_queue = (max_outstanding_req_per_channel *
+				 (max_sub_channels + 1));
 
 	host = scsi_host_alloc(&scsi_driver,
 			       sizeof(struct hv_host_device));
-	if (!host)
+	if (!host) {
+		mutex_unlock(&probe_mutex);
 		return -ENOMEM;
+	}
 
 	host_dev = shost_priv(host);
 	memset(host_dev, 0, sizeof(struct hv_host_device));
@@ -1780,6 +2229,7 @@
 	host_dev->port = host->host_no;
 	host_dev->dev = device;
 	host_dev->host = host;
+	mutex_init(&host_dev->host_mutex);
 
 
 	stor_device = kzalloc(sizeof(struct storvsc_device), GFP_KERNEL);
@@ -1808,7 +2258,7 @@
 		host->max_lun = STORVSC_FC_MAX_LUNS_PER_TARGET;
 		host->max_id = STORVSC_FC_MAX_TARGETS;
 		host->max_channel = STORVSC_FC_MAX_CHANNELS - 1;
-#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+#if defined(CONFIG_SCSI_FC_ATTRS) || defined(CONFIG_SCSI_FC_ATTRS_MODULE)
 		host->transportt = fc_transport_template;
 #endif
 		break;
@@ -1830,16 +2280,20 @@
 
 	/*
 	 * set the table size based on the info we got
-	 * from the host.
+	 * from the host; but only for 64 bit guests.
 	 */
+#ifdef CONFIG_X86_64
 	host->sg_tablesize = (stor_device->max_transfer_bytes >> PAGE_SHIFT);
+#endif
+
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
 	/*
 	 * Set the number of HW queues we are supporting.
 	 */
 	if (stor_device->num_sc != 0)
 		host->nr_hw_queues = stor_device->num_sc + 1;
-
-	/*
+#endif
+    /*
 	 * Set the error handler work queue.
 	 */
 	host_dev->handle_error_wq =
@@ -1848,6 +2302,7 @@
 						host->host_no);
 	if (!host_dev->handle_error_wq)
 		goto err_out2;
+
 	INIT_WORK(&host_dev->host_scan_work, storvsc_host_scan);
 	/* Register the HBA and start the scsi bus scan */
 	ret = scsi_add_host(host, &device->device);
@@ -1863,7 +2318,7 @@
 		if (ret)
 			goto err_out4;
 	}
-#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+#if defined(CONFIG_SCSI_FC_ATTRS) || defined(CONFIG_SCSI_FC_ATTRS_MODULE)
 	if (host->transportt == fc_transport_template) {
 		struct fc_rport_identifiers ids = {
 			.roles = FC_PORT_ROLE_FCP_DUMMY_INITIATOR,
@@ -1872,12 +2327,11 @@
 		fc_host_node_name(host) = stor_device->node_name;
 		fc_host_port_name(host) = stor_device->port_name;
 		stor_device->rport = fc_remote_port_add(host, 0, &ids);
-		if (!stor_device->rport) {
-			ret = -ENOMEM;
+		if (!stor_device->rport)
 			goto err_out4;
-		}
 	}
 #endif
+	mutex_unlock(&probe_mutex);
 	return 0;
 
 err_out4:
@@ -1902,6 +2356,7 @@
 
 err_out0:
 	scsi_host_put(host);
+	mutex_unlock(&probe_mutex);
 	return ret;
 }
 
@@ -1911,7 +2366,7 @@
 	struct Scsi_Host *host = stor_device->host;
 	struct hv_host_device *host_dev = shost_priv(host);
 
-#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+#if defined(CONFIG_SCSI_FC_ATTRS) || defined(CONFIG_SCSI_FC_ATTRS_MODULE)
 	if (host->transportt == fc_transport_template) {
 		fc_remote_port_delete(stor_device->rport);
 		fc_remove_host(host);
@@ -1932,10 +2387,16 @@
 	.remove = storvsc_remove,
 };
 
-#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+static int  storvsc_issue_fc_host_lip(struct Scsi_Host *shost)
+{
+                return 0;
+}
+
+#if defined(CONFIG_SCSI_FC_ATTRS) || defined(CONFIG_SCSI_FC_ATTRS_MODULE)
 static struct fc_function_template fc_transport_functions = {
 	.show_host_node_name = 1,
 	.show_host_port_name = 1,
+	.issue_fc_host_lip = storvsc_issue_fc_host_lip,
 };
 #endif
 
@@ -1956,15 +2417,23 @@
 		vmscsi_size_delta,
 		sizeof(u64)));
 
-#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+
+#if defined(CONFIG_SCSI_FC_ATTRS) || defined(CONFIG_SCSI_FC_ATTRS_MODULE)
 	fc_transport_template = fc_attach_transport(&fc_transport_functions);
 	if (!fc_transport_template)
 		return -ENODEV;
+
+	/*
+	 * Install Hyper-V specific timeout handler.
+	 */
+	fc_transport_template->eh_timed_out = storvsc_eh_timed_out;
+	fc_transport_template->user_scan = NULL;
 #endif
 
+	mutex_init(&probe_mutex);
 	ret = vmbus_driver_register(&storvsc_drv);
 
-#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+#if defined(CONFIG_SCSI_FC_ATTRS) || defined(CONFIG_SCSI_FC_ATTRS_MODULE)
 	if (ret)
 		fc_release_transport(fc_transport_template);
 #endif
@@ -1975,12 +2444,17 @@
 static void __exit storvsc_drv_exit(void)
 {
 	vmbus_driver_unregister(&storvsc_drv);
-#if IS_ENABLED(CONFIG_SCSI_FC_ATTRS)
+#if defined(CONFIG_SCSI_FC_ATTRS) || defined(CONFIG_SCSI_FC_ATTRS_MODULE)
 	fc_release_transport(fc_transport_template);
 #endif
 }
 
 MODULE_LICENSE("GPL");
 MODULE_DESCRIPTION("Microsoft Hyper-V virtual storage driver");
+MODULE_VERSION(HV_DRV_VERSION);
+MODULE_ALIAS("vmbus:d96361baa104294db60572e2ffb1dc7f");
+MODULE_ALIAS("vmbus:32264132cb86a2449b5c50d1417354f5");
+MODULE_ALIAS("vmbus:4acc9b2f6900f34ab76b6fd0be528cda");
+
 module_init(storvsc_drv_init);
 module_exit(storvsc_drv_exit);
diff -Naur linux-3.10.0-957.el7.orig/drivers/uio/hv_trace.h linux-3.10.0-957.el7.lis/drivers/uio/hv_trace.h
--- linux-3.10.0-957.el7.orig/drivers/uio/hv_trace.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/uio/hv_trace.h	2018-12-12 00:51:40.912783995 +0000
@@ -0,0 +1,327 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM hyperv
+
+#if !defined(_HV_TRACE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _HV_TRACE_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(vmbus_hdr_msg,
+	TP_PROTO(const struct vmbus_channel_message_header *hdr),
+	TP_ARGS(hdr),
+	TP_STRUCT__entry(__field(unsigned int, msgtype)),
+	TP_fast_assign(__entry->msgtype = hdr->msgtype;),
+	TP_printk("msgtype=%u", __entry->msgtype)
+);
+
+DEFINE_EVENT(vmbus_hdr_msg, vmbus_on_msg_dpc,
+	TP_PROTO(const struct vmbus_channel_message_header *hdr),
+	TP_ARGS(hdr)
+);
+
+DEFINE_EVENT(vmbus_hdr_msg, vmbus_on_message,
+	TP_PROTO(const struct vmbus_channel_message_header *hdr),
+	TP_ARGS(hdr)
+);
+
+TRACE_EVENT(vmbus_onoffer,
+	    TP_PROTO(const struct vmbus_channel_offer_channel *offer),
+	    TP_ARGS(offer),
+	    TP_STRUCT__entry(
+		    __field(u32, child_relid)
+		    __field(u8, monitorid)
+		    __field(u16, is_ddc_int)
+		    __field(u32, connection_id)
+		    __array(char, if_type, 16)
+		    __array(char, if_instance, 16)
+		    __field(u16, chn_flags)
+		    __field(u16, mmio_mb)
+		    __field(u16, sub_idx)
+		    ),
+	    TP_fast_assign(__entry->child_relid = offer->child_relid;
+			   __entry->monitorid = offer->monitorid;
+			   __entry->is_ddc_int = offer->is_dedicated_interrupt;
+			   __entry->connection_id = offer->connection_id;
+			   memcpy(__entry->if_type,
+				  &offer->offer.if_type.b, 16);
+			   memcpy(__entry->if_instance,
+				  &offer->offer.if_instance.b, 16);
+			   __entry->chn_flags = offer->offer.chn_flags;
+			   __entry->mmio_mb = offer->offer.mmio_megabytes;
+			   __entry->sub_idx = offer->offer.sub_channel_index;
+		    ),
+	    TP_printk("child_relid 0x%x, monitorid 0x%x, is_dedicated %d, "
+		      "connection_id 0x%x, if_type %pUl, if_instance %pUl, "
+		      "chn_flags 0x%x, mmio_megabytes %d, sub_channel_index %d",
+		      __entry->child_relid, __entry->monitorid,
+		      __entry->is_ddc_int, __entry->connection_id,
+		      __entry->if_type, __entry->if_instance,
+		      __entry->chn_flags, __entry->mmio_mb,
+		      __entry->sub_idx
+		    )
+	);
+
+TRACE_EVENT(vmbus_onoffer_rescind,
+	    TP_PROTO(const struct vmbus_channel_rescind_offer *offer),
+	    TP_ARGS(offer),
+	    TP_STRUCT__entry(__field(u32, child_relid)),
+	    TP_fast_assign(__entry->child_relid = offer->child_relid),
+	    TP_printk("child_relid 0x%x", __entry->child_relid)
+	);
+
+TRACE_EVENT(vmbus_onopen_result,
+	    TP_PROTO(const struct vmbus_channel_open_result *result),
+	    TP_ARGS(result),
+	    TP_STRUCT__entry(
+		    __field(u32, child_relid)
+		    __field(u32, openid)
+		    __field(u32, status)
+		    ),
+	    TP_fast_assign(__entry->child_relid = result->child_relid;
+			   __entry->openid = result->openid;
+			   __entry->status = result->status;
+		    ),
+	    TP_printk("child_relid 0x%x, openid %d, status %d",
+		      __entry->child_relid,  __entry->openid,  __entry->status
+		    )
+	);
+
+TRACE_EVENT(vmbus_ongpadl_created,
+	    TP_PROTO(const struct vmbus_channel_gpadl_created *gpadlcreated),
+	    TP_ARGS(gpadlcreated),
+	    TP_STRUCT__entry(
+		    __field(u32, child_relid)
+		    __field(u32, gpadl)
+		    __field(u32, status)
+		    ),
+	    TP_fast_assign(__entry->child_relid = gpadlcreated->child_relid;
+			   __entry->gpadl = gpadlcreated->gpadl;
+			   __entry->status = gpadlcreated->creation_status;
+		    ),
+	    TP_printk("child_relid 0x%x, gpadl 0x%x, creation_status %d",
+		      __entry->child_relid,  __entry->gpadl,  __entry->status
+		    )
+	);
+
+TRACE_EVENT(vmbus_ongpadl_torndown,
+	    TP_PROTO(const struct vmbus_channel_gpadl_torndown *gpadltorndown),
+	    TP_ARGS(gpadltorndown),
+	    TP_STRUCT__entry(__field(u32, gpadl)),
+	    TP_fast_assign(__entry->gpadl = gpadltorndown->gpadl),
+	    TP_printk("gpadl 0x%x", __entry->gpadl)
+	);
+
+TRACE_EVENT(vmbus_onversion_response,
+	    TP_PROTO(const struct vmbus_channel_version_response *response),
+	    TP_ARGS(response),
+	    TP_STRUCT__entry(
+		    __field(u8, ver)
+		    ),
+	    TP_fast_assign(__entry->ver = response->version_supported;
+		    ),
+	    TP_printk("version_supported %d", __entry->ver)
+	);
+
+TRACE_EVENT(vmbus_request_offers,
+	    TP_PROTO(int ret),
+	    TP_ARGS(ret),
+	    TP_STRUCT__entry(__field(int, ret)),
+	    TP_fast_assign(__entry->ret = ret),
+	    TP_printk("sending ret %d", __entry->ret)
+	);
+
+TRACE_EVENT(vmbus_open,
+	    TP_PROTO(const struct vmbus_channel_open_channel *msg, int ret),
+	    TP_ARGS(msg, ret),
+	    TP_STRUCT__entry(
+		    __field(u32, child_relid)
+		    __field(u32, openid)
+		    __field(u32, gpadlhandle)
+		    __field(u32, target_vp)
+		    __field(u32, offset)
+		    __field(int, ret)
+		    ),
+	    TP_fast_assign(
+		    __entry->child_relid = msg->child_relid;
+		    __entry->openid = msg->openid;
+		    __entry->gpadlhandle = msg->ringbuffer_gpadlhandle;
+		    __entry->target_vp = msg->target_vp;
+		    __entry->offset = msg->downstream_ringbuffer_pageoffset;
+		    __entry->ret = ret;
+		    ),
+	    TP_printk("sending child_relid 0x%x, openid %d, "
+		      "gpadlhandle 0x%x, target_vp 0x%x, offset 0x%x, ret %d",
+		      __entry->child_relid,  __entry->openid,
+		      __entry->gpadlhandle, __entry->target_vp,
+		      __entry->offset, __entry->ret
+		    )
+	);
+
+TRACE_EVENT(vmbus_close_internal,
+	    TP_PROTO(const struct vmbus_channel_close_channel *msg, int ret),
+	    TP_ARGS(msg, ret),
+	    TP_STRUCT__entry(
+		    __field(u32, child_relid)
+		    __field(int, ret)
+		    ),
+	    TP_fast_assign(
+		    __entry->child_relid = msg->child_relid;
+		    __entry->ret = ret;
+		    ),
+	    TP_printk("sending child_relid 0x%x, ret %d", __entry->child_relid,
+		    __entry->ret)
+	);
+
+TRACE_EVENT(vmbus_establish_gpadl_header,
+	    TP_PROTO(const struct vmbus_channel_gpadl_header *msg, int ret),
+	    TP_ARGS(msg, ret),
+	    TP_STRUCT__entry(
+		    __field(u32, child_relid)
+		    __field(u32, gpadl)
+		    __field(u16, range_buflen)
+		    __field(u16, rangecount)
+		    __field(int, ret)
+		    ),
+	    TP_fast_assign(
+		    __entry->child_relid = msg->child_relid;
+		    __entry->gpadl = msg->gpadl;
+		    __entry->range_buflen = msg->range_buflen;
+		    __entry->rangecount = msg->rangecount;
+		    __entry->ret = ret;
+		    ),
+	    TP_printk("sending child_relid 0x%x, gpadl 0x%x, range_buflen %d "
+		      "rangecount %d, ret %d",
+		      __entry->child_relid, __entry->gpadl,
+		      __entry->range_buflen, __entry->rangecount, __entry->ret
+		    )
+	);
+
+TRACE_EVENT(vmbus_establish_gpadl_body,
+	    TP_PROTO(const struct vmbus_channel_gpadl_body *msg, int ret),
+	    TP_ARGS(msg, ret),
+	    TP_STRUCT__entry(
+		    __field(u32, msgnumber)
+		    __field(u32, gpadl)
+		    __field(int, ret)
+		    ),
+	    TP_fast_assign(
+		    __entry->msgnumber = msg->msgnumber;
+		    __entry->gpadl = msg->gpadl;
+		    __entry->ret = ret;
+		    ),
+	    TP_printk("sending msgnumber %d, gpadl 0x%x, ret %d",
+		      __entry->msgnumber, __entry->gpadl, __entry->ret
+		    )
+	);
+
+TRACE_EVENT(vmbus_teardown_gpadl,
+	    TP_PROTO(const struct vmbus_channel_gpadl_teardown *msg, int ret),
+	    TP_ARGS(msg, ret),
+	    TP_STRUCT__entry(
+		    __field(u32, child_relid)
+		    __field(u32, gpadl)
+		    __field(int, ret)
+		    ),
+	    TP_fast_assign(
+		    __entry->child_relid = msg->child_relid;
+		    __entry->gpadl = msg->gpadl;
+		    __entry->ret = ret;
+		    ),
+	    TP_printk("sending child_relid 0x%x, gpadl 0x%x, ret %d",
+		      __entry->child_relid, __entry->gpadl, __entry->ret
+		    )
+	);
+
+TRACE_EVENT(vmbus_negotiate_version,
+	    TP_PROTO(const struct vmbus_channel_initiate_contact *msg, int ret),
+	    TP_ARGS(msg, ret),
+	    TP_STRUCT__entry(
+		    __field(u32, ver)
+		    __field(u32, target_vcpu)
+		    __field(int, ret)
+		    __field(u64, int_page)
+		    __field(u64, mon_page1)
+		    __field(u64, mon_page2)
+		    ),
+	    TP_fast_assign(
+		    __entry->ver = msg->vmbus_version_requested;
+		    __entry->target_vcpu = msg->target_vcpu;
+		    __entry->int_page = msg->interrupt_page;
+		    __entry->mon_page1 = msg->monitor_page1;
+		    __entry->mon_page2 = msg->monitor_page2;
+		    __entry->ret = ret;
+		    ),
+	    TP_printk("sending vmbus_version_requested %d, target_vcpu 0x%x, "
+		      "pages %llx:%llx:%llx, ret %d",
+		      __entry->ver, __entry->target_vcpu, __entry->int_page,
+		      __entry->mon_page1, __entry->mon_page2, __entry->ret
+		    )
+	);
+
+TRACE_EVENT(vmbus_release_relid,
+	    TP_PROTO(const struct vmbus_channel_relid_released *msg, int ret),
+	    TP_ARGS(msg, ret),
+	    TP_STRUCT__entry(
+		    __field(u32, child_relid)
+		    __field(int, ret)
+		    ),
+	    TP_fast_assign(
+		    __entry->child_relid = msg->child_relid;
+		    __entry->ret = ret;
+		    ),
+	    TP_printk("sending child_relid 0x%x, ret %d",
+		      __entry->child_relid, __entry->ret
+		    )
+	);
+
+TRACE_EVENT(vmbus_send_tl_connect_request,
+	    TP_PROTO(const struct vmbus_channel_tl_connect_request *msg,
+		     int ret),
+	    TP_ARGS(msg, ret),
+	    TP_STRUCT__entry(
+		    __array(char, guest_id, 16)
+		    __array(char, host_id, 16)
+		    __field(int, ret)
+		    ),
+	    TP_fast_assign(
+		    memcpy(__entry->guest_id, &msg->guest_endpoint_id.b, 16);
+		    memcpy(__entry->host_id, &msg->host_service_id.b, 16);
+		    __entry->ret = ret;
+		    ),
+	    TP_printk("sending guest_endpoint_id %pUl, host_service_id %pUl, "
+		      "ret %d",
+		      __entry->guest_id, __entry->host_id, __entry->ret
+		    )
+	);
+
+DECLARE_EVENT_CLASS(vmbus_channel,
+	TP_PROTO(const struct vmbus_channel *channel),
+	TP_ARGS(channel),
+	TP_STRUCT__entry(__field(u32, relid)),
+	TP_fast_assign(__entry->relid = channel->offermsg.child_relid),
+	TP_printk("relid 0x%x", __entry->relid)
+);
+
+DEFINE_EVENT(vmbus_channel, vmbus_chan_sched,
+	    TP_PROTO(const struct vmbus_channel *channel),
+	    TP_ARGS(channel)
+);
+
+DEFINE_EVENT(vmbus_channel, vmbus_setevent,
+	    TP_PROTO(const struct vmbus_channel *channel),
+	    TP_ARGS(channel)
+);
+
+DEFINE_EVENT(vmbus_channel, vmbus_on_event,
+	    TP_PROTO(const struct vmbus_channel *channel),
+	    TP_ARGS(channel)
+);
+
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH .
+#undef TRACE_INCLUDE_FILE
+#define TRACE_INCLUDE_FILE hv_trace
+#endif /* _HV_TRACE_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff -Naur linux-3.10.0-957.el7.orig/drivers/uio/hyperv_vmbus.h linux-3.10.0-957.el7.lis/drivers/uio/hyperv_vmbus.h
--- linux-3.10.0-957.el7.orig/drivers/uio/hyperv_vmbus.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/uio/hyperv_vmbus.h	2018-12-12 00:51:40.911784001 +0000
@@ -0,0 +1,461 @@
+/*
+ *
+ * Copyright (c) 2011, Microsoft Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
+ * Place - Suite 330, Boston, MA 02111-1307 USA.
+ *
+ * Authors:
+ *   Haiyang Zhang <haiyangz@microsoft.com>
+ *   Hank Janssen  <hjanssen@microsoft.com>
+ *   K. Y. Srinivasan <kys@microsoft.com>
+ *
+ */
+
+#ifndef _HYPERV_VMBUS_H
+#define _HYPERV_VMBUS_H
+
+#include <linux/list.h>
+#include <asm/sync_bitops.h>
+#include <linux/atomic.h>
+#include <linux/interrupt.h>
+#include <linux/hyperv.h>
+#include "hv_trace.h"
+
+/*
+ * Timeout for services such as KVP and fcopy.
+ */
+#define HV_UTIL_TIMEOUT 30
+
+/*
+ * Timeout for guest-host handshake for services.
+ */
+#define HV_UTIL_NEGO_TIMEOUT 55
+
+
+#define HV_SYNIC_VERSION_1		(0x1)
+
+/* Define synthetic interrupt controller flag constants. */
+#define HV_EVENT_FLAGS_COUNT		(256 * 8)
+#define HV_EVENT_FLAGS_LONG_COUNT	(256 / sizeof(unsigned long))
+
+/*
+ * Timer configuration register.
+ */
+union hv_timer_config {
+	u64 as_uint64;
+	struct {
+		u64 enable:1;
+		u64 periodic:1;
+		u64 lazy:1;
+		u64 auto_enable:1;
+		u64 reserved_z0:12;
+		u64 sintx:4;
+		u64 reserved_z1:44;
+	};
+};
+
+
+/* Define the synthetic interrupt controller event flags format. */
+union hv_synic_event_flags {
+	unsigned long flags[HV_EVENT_FLAGS_LONG_COUNT];
+};
+
+/* Define SynIC control register. */
+union hv_synic_scontrol {
+	u64 as_uint64;
+	struct {
+		u64 enable:1;
+		u64 reserved:63;
+	};
+};
+
+/* Define synthetic interrupt source. */
+union hv_synic_sint {
+	u64 as_uint64;
+	struct {
+		u64 vector:8;
+		u64 reserved1:8;
+		u64 masked:1;
+		u64 auto_eoi:1;
+		u64 reserved2:46;
+	};
+};
+
+/* Define the format of the SIMP register */
+union hv_synic_simp {
+	u64 as_uint64;
+	struct {
+		u64 simp_enabled:1;
+		u64 preserved:11;
+		u64 base_simp_gpa:52;
+	};
+};
+
+/* Define the format of the SIEFP register */
+union hv_synic_siefp {
+	u64 as_uint64;
+	struct {
+		u64 siefp_enabled:1;
+		u64 preserved:11;
+		u64 base_siefp_gpa:52;
+	};
+};
+
+/* Definitions for the monitored notification facility */
+union hv_monitor_trigger_group {
+	u64 as_uint64;
+	struct {
+		u32 pending;
+		u32 armed;
+	};
+};
+
+struct hv_monitor_parameter {
+	union hv_connection_id connectionid;
+	u16 flagnumber;
+	u16 rsvdz;
+};
+
+union hv_monitor_trigger_state {
+	u32 asu32;
+
+	struct {
+		u32 group_enable:4;
+		u32 rsvdz:28;
+	};
+};
+
+/* struct hv_monitor_page Layout */
+/* ------------------------------------------------------ */
+/* | 0   | TriggerState (4 bytes) | Rsvd1 (4 bytes)     | */
+/* | 8   | TriggerGroup[0]                              | */
+/* | 10  | TriggerGroup[1]                              | */
+/* | 18  | TriggerGroup[2]                              | */
+/* | 20  | TriggerGroup[3]                              | */
+/* | 28  | Rsvd2[0]                                     | */
+/* | 30  | Rsvd2[1]                                     | */
+/* | 38  | Rsvd2[2]                                     | */
+/* | 40  | NextCheckTime[0][0]    | NextCheckTime[0][1] | */
+/* | ...                                                | */
+/* | 240 | Latency[0][0..3]                             | */
+/* | 340 | Rsvz3[0]                                     | */
+/* | 440 | Parameter[0][0]                              | */
+/* | 448 | Parameter[0][1]                              | */
+/* | ...                                                | */
+/* | 840 | Rsvd4[0]                                     | */
+/* ------------------------------------------------------ */
+struct hv_monitor_page {
+	union hv_monitor_trigger_state trigger_state;
+	u32 rsvdz1;
+
+	union hv_monitor_trigger_group trigger_group[4];
+	u64 rsvdz2[3];
+
+	s32 next_checktime[4][32];
+
+	u16 latency[4][32];
+	u64 rsvdz3[32];
+
+	struct hv_monitor_parameter parameter[4][32];
+
+	u8 rsvdz4[1984];
+};
+
+#define HV_HYPERCALL_PARAM_ALIGN	sizeof(u64)
+
+/* Definition of the hv_post_message hypercall input structure. */
+struct hv_input_post_message {
+	union hv_connection_id connectionid;
+	u32 reserved;
+	u32 message_type;
+	u32 payload_size;
+	u64 payload[HV_MESSAGE_PAYLOAD_QWORD_COUNT];
+};
+
+enum {
+	VMBUS_MESSAGE_CONNECTION_ID	= 1,
+	VMBUS_MESSAGE_PORT_ID		= 1,
+	VMBUS_EVENT_CONNECTION_ID	= 2,
+	VMBUS_EVENT_PORT_ID		= 2,
+	VMBUS_MONITOR_CONNECTION_ID	= 3,
+	VMBUS_MONITOR_PORT_ID		= 3,
+	VMBUS_MESSAGE_SINT		= 2,
+};
+
+
+/*
+ * Per cpu state for channel handling
+ */
+struct hv_per_cpu_context {
+	void *synic_message_page;
+	void *synic_event_page;
+	/*
+	 * buffer to post messages to the host.
+	 */
+	void *post_msg_page;
+
+	/*
+	 * Starting with win8, we can take channel interrupts on any CPU;
+	 * we will manage the tasklet that handles events messages on a per CPU
+	 * basis.
+	 */
+	struct tasklet_struct msg_dpc;
+
+	/*
+	 * To optimize the mapping of relid to channel, maintain
+	 * per-cpu list of the channels based on their CPU affinity.
+	 */
+	struct list_head chan_list;
+};
+
+struct hv_context {
+	/* We only support running on top of Hyper-V
+	 * So at this point this really can only contain the Hyper-V ID
+	 */
+	u64 guestid;
+
+	void *tsc_page;
+
+	bool synic_initialized;
+
+	struct hv_per_cpu_context __percpu *cpu_context;
+
+	struct clock_event_device *clk_evt[NR_CPUS];
+
+	/*
+         * To manage allocations in a NUMA node.
+         * Array indexed by numa node ID.
+         */
+        struct cpumask *hv_numa_map;
+};
+
+extern struct hv_context hv_context;
+
+extern int affinity_mode;
+
+/* Hv Interface */
+
+extern int hv_init(void);
+
+extern int hv_post_message(union hv_connection_id connection_id,
+			 enum hv_message_type message_type,
+			 void *payload, size_t payload_size);
+
+extern int hv_synic_alloc(void);
+
+extern void hv_synic_free(void);
+
+#if (RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(7,3))
+extern void hv_synic_init(unsigned int cpu);
+
+extern void hv_synic_cleanup(unsigned int cpu);
+#else
+extern void hv_synic_init(void *cpu);
+
+extern void hv_synic_cleanup(void *cpu);
+#endif
+
+extern void hv_synic_clockevents_cleanup(void);
+
+extern void hv_clockevents_bind(int cpu);
+
+extern void hv_clockevents_unbind(int cpu);
+
+extern int hv_synic_cpu_used(unsigned int cpu);
+
+/* Interface */
+
+
+int hv_ringbuffer_init(struct hv_ring_buffer_info *ring_info,
+		       struct page *pages, u32 pagecnt);
+
+void hv_ringbuffer_cleanup(struct hv_ring_buffer_info *ring_info);
+
+int hv_ringbuffer_write(struct vmbus_channel *channel,
+			const struct kvec *kv_list, u32 kv_count);
+
+void hv_get_ringbuffer_available_space(struct hv_ring_buffer_info *inring_info,
+				       u32 *bytes_avail_toread,
+				       u32 *bytes_avail_towrite);
+
+int hv_ringbuffer_read(struct vmbus_channel *channel,
+		   void *buffer, u32 buflen, u32 *buffer_actual_len,
+		   u64 *requestid, bool raw);
+
+/*
+ * Maximum channels is determined by the size of the interrupt page
+ * which is PAGE_SIZE. 1/2 of PAGE_SIZE is for send endpoint interrupt
+ * and the other is receive endpoint interrupt
+ */
+#define MAX_NUM_CHANNELS	((PAGE_SIZE >> 1) << 3)	/* 16348 channels */
+
+/* The value here must be in multiple of 32 */
+/* TODO: Need to make this configurable */
+#define MAX_NUM_CHANNELS_SUPPORTED	256
+
+
+enum vmbus_connect_state {
+	DISCONNECTED,
+	CONNECTING,
+	CONNECTED,
+	DISCONNECTING
+};
+
+#define MAX_SIZE_CHANNEL_MESSAGE	HV_MESSAGE_PAYLOAD_BYTE_COUNT
+
+struct vmbus_connection {
+	/*
+	 * CPU on which the initial host contact was made.
+	 */
+	int connect_cpu;
+
+	atomic_t offer_in_progress;
+
+	enum vmbus_connect_state conn_state;
+
+	atomic_t next_gpadl_handle;
+
+	struct completion  unload_event;
+	/*
+	 * Represents channel interrupts. Each bit position represents a
+	 * channel.  When a channel sends an interrupt via VMBUS, it finds its
+	 * bit in the sendInterruptPage, set it and calls Hv to generate a port
+	 * event. The other end receives the port event and parse the
+	 * recvInterruptPage to see which bit is set
+	 */
+	void *int_page;
+	void *send_int_page;
+	void *recv_int_page;
+
+	/*
+	 * 2 pages - 1st page for parent->child notification and 2nd
+	 * is child->parent notification
+	 */
+	struct hv_monitor_page *monitor_pages[2];
+	struct list_head chn_msg_list;
+	spinlock_t channelmsg_lock;
+
+	/* List of channels */
+	struct list_head chn_list;
+	struct mutex channel_mutex;
+
+	/*
+	 * An offer message is handled first on the work_queue, and then
+	 * is further handled on handle_primary_chan_wq or
+	 * handle_sub_chan_wq.
+	 */
+	struct workqueue_struct *work_queue;
+	struct workqueue_struct *handle_primary_chan_wq;
+	struct workqueue_struct *handle_sub_chan_wq;
+};
+
+
+struct vmbus_msginfo {
+	/* Bookkeeping stuff */
+	struct list_head msglist_entry;
+
+	/* The message itself */
+	unsigned char msg[0];
+};
+
+
+extern struct vmbus_connection vmbus_connection;
+
+static inline void vmbus_send_interrupt(u32 relid)
+{
+	sync_set_bit(relid, vmbus_connection.send_int_page);
+}
+
+enum vmbus_message_handler_type {
+	/* The related handler can sleep. */
+	VMHT_BLOCKING = 0,
+
+	/* The related handler must NOT sleep. */
+	VMHT_NON_BLOCKING = 1,
+};
+
+struct vmbus_channel_message_table_entry {
+	enum vmbus_channel_message_type message_type;
+	enum vmbus_message_handler_type handler_type;
+	void (*message_handler)(struct vmbus_channel_message_header *msg);
+};
+
+extern const struct vmbus_channel_message_table_entry
+	channel_message_table[CHANNELMSG_COUNT];
+
+
+/* General vmbus interface */
+
+struct hv_device *vmbus_device_create(const uuid_le *type,
+				      const uuid_le *instance,
+				      struct vmbus_channel *channel);
+
+int vmbus_device_register(struct hv_device *child_device_obj);
+void vmbus_device_unregister(struct hv_device *device_obj);
+int vmbus_add_channel_kobj(struct hv_device *device_obj,
+			   struct vmbus_channel *channel);
+
+struct vmbus_channel *relid2channel(u32 relid);
+
+void vmbus_free_channels(void);
+
+/* Connection interface */
+
+int vmbus_connect(void);
+void vmbus_disconnect(void);
+
+int vmbus_post_msg(void *buffer, size_t buflen, bool can_sleep);
+
+void vmbus_on_event(unsigned long data);
+void vmbus_on_msg_dpc(unsigned long data);
+
+int hv_kvp_init(struct hv_util_service *srv);
+void hv_kvp_deinit(void);
+void hv_kvp_onchannelcallback(void *context);
+
+int hv_vss_init(struct hv_util_service *srv);
+void hv_vss_deinit(void);
+void hv_vss_onchannelcallback(void *context);
+
+int hv_fcopy_init(struct hv_util_service *srv);
+void hv_fcopy_deinit(void);
+void hv_fcopy_onchannelcallback(void *context);
+
+void vmbus_initiate_unload(bool crash);
+
+static inline void hv_poll_channel(struct vmbus_channel *channel,
+				   void (*cb)(void *))
+{
+	if (!channel)
+		return;
+
+	if ((irqs_disabled() || in_interrupt()) &&
+	    (channel->target_cpu == smp_processor_id())) {
+		cb(channel);
+		return;
+	}
+
+	smp_call_function_single(channel->target_cpu, cb, channel, true);
+}
+
+enum hvutil_device_state {
+	HVUTIL_DEVICE_INIT = 0,  /* driver is loaded, waiting for userspace */
+	HVUTIL_READY,            /* userspace is registered */
+	HVUTIL_HOSTMSG_RECEIVED, /* message from the host was received */
+	HVUTIL_USERSPACE_REQ,    /* request to userspace was sent */
+	HVUTIL_USERSPACE_RECV,   /* reply from userspace was received */
+	HVUTIL_DEVICE_DYING,     /* driver unload is in progress */
+};
+
+#endif /* _HYPERV_VMBUS_H */
diff -Naur linux-3.10.0-957.el7.orig/drivers/uio/uio_hv_generic.c linux-3.10.0-957.el7.lis/drivers/uio/uio_hv_generic.c
--- linux-3.10.0-957.el7.orig/drivers/uio/uio_hv_generic.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/uio/uio_hv_generic.c	2018-12-12 00:51:40.907784026 +0000
@@ -10,11 +10,13 @@
  * Since the driver does not declare any device ids, you must allocate
  * id and bind the device to the driver yourself.  For example:
  *
+ * Associate Network GUID with UIO device
  * # echo "f8615163-df3e-46c5-913f-f2d2f965ed0e" \
- *    > /sys/bus/vmbus/drivers/uio_hv_generic
- * # echo -n vmbus-ed963694-e847-4b2a-85af-bc9cfc11d6f3 \
+ *    > /sys/bus/vmbus/drivers/uio_hv_generic/new_id
+ * Then rebind
+ * # echo -n "ed963694-e847-4b2a-85af-bc9cfc11d6f3" \
  *    > /sys/bus/vmbus/drivers/hv_netvsc/unbind
- * # echo -n vmbus-ed963694-e847-4b2a-85af-bc9cfc11d6f3 \
+ * # echo -n "ed963694-e847-4b2a-85af-bc9cfc11d6f3" \
  *    > /sys/bus/vmbus/drivers/uio_hv_generic/bind
  */
 
@@ -27,15 +29,18 @@
 #include <linux/netdevice.h>
 #include <linux/if_ether.h>
 #include <linux/skbuff.h>
-#include <linux/hyperv.h>
 #include <linux/vmalloc.h>
 #include <linux/slab.h>
 
-#include "../hv/hyperv_vmbus.h"
+#include <linux/hyperv.h>
+#include "hyperv_vmbus.h"
 
 #define DRIVER_VERSION	"0.02.0"
 #define DRIVER_AUTHOR	"Stephen Hemminger <sthemmin at microsoft.com>"
 #define DRIVER_DESC	"Generic UIO driver for VMBus devices"
+#define HV_RING_SIZE	 512	/* pages */
+#define SEND_BUFFER_SIZE (15 * 1024 * 1024)
+#define RECV_BUFFER_SIZE (15 * 1024 * 1024)
 
 /*
  * List of resources to be mapped to user space
@@ -45,33 +50,22 @@
 	TXRX_RING_MAP = 0,
 	INT_PAGE_MAP,
 	MON_PAGE_MAP,
+	RECV_BUF_MAP,
+	SEND_BUF_MAP
 };
 
-#define HV_RING_SIZE	512
-
 struct hv_uio_private_data {
 	struct uio_info info;
 	struct hv_device *device;
+	void	*recv_buf;
+	u32	recv_gpadl;
+	char	recv_name[32];	/* "recv_4294967295" */
+
+	void	*send_buf;
+	u32	send_gpadl;
+	char	send_name[32];
 };
 
-static int
-hv_uio_mmap(struct uio_info *info, struct vm_area_struct *vma)
-{
-	int mi;
-
-	if (vma->vm_pgoff >= MAX_UIO_MAPS)
-		return -EINVAL;
-
-	if (info->mem[vma->vm_pgoff].size == 0)
-		return -EINVAL;
-
-	mi = (int)vma->vm_pgoff;
-
-	return remap_pfn_range(vma, vma->vm_start,
-			info->mem[mi].addr >> PAGE_SHIFT,
-			vma->vm_end - vma->vm_start, vma->vm_page_prot);
-}
-
 /*
  * This is the irqcontrol callback to be registered to uio_info.
  * It can be used to disable/enable interrupt from user space processes.
@@ -107,6 +101,36 @@
 	uio_event_notify(&pdata->info);
 }
 
+/*
+ * Callback from vmbus_event when channel is rescinded.
+ */
+static void hv_uio_rescind(struct vmbus_channel *channel)
+{
+	struct hv_device *hv_dev = channel->primary_channel->device_obj;
+	struct hv_uio_private_data *pdata = hv_get_drvdata(hv_dev);
+
+	/*
+	 * Turn off the interrupt file handle
+	 * Next read for event will return -EIO
+	 */
+	pdata->info.irq = 0;
+
+	/* Wake up reader */
+	uio_event_notify(&pdata->info);
+}
+
+static void
+hv_uio_cleanup(struct hv_device *dev, struct hv_uio_private_data *pdata)
+{
+	if (pdata->send_gpadl)
+		vmbus_teardown_gpadl(dev->channel, pdata->send_gpadl);
+	vfree(pdata->send_buf);
+
+	if (pdata->recv_gpadl)
+		vmbus_teardown_gpadl(dev->channel, pdata->recv_gpadl);
+	vfree(pdata->recv_buf);
+}
+
 static int
 hv_uio_probe(struct hv_device *dev,
 	     const struct hv_vmbus_device_id *dev_id)
@@ -124,35 +148,80 @@
 	if (ret)
 		goto fail;
 
+	/* Communicating with host has to be via shared memory not hypercall */
+	if (!dev->channel->offermsg.monitor_allocated) {
+		dev_err(&dev->device, "vmbus channel requires hypercall\n");
+		ret = -ENOTSUPP;
+		goto fail_close;
+	}
+
 	dev->channel->inbound.ring_buffer->interrupt_mask = 1;
-	set_channel_read_mode(dev->channel, HV_CALL_DIRECT);
+	set_channel_read_mode(dev->channel, HV_CALL_ISR);
 
 	/* Fill general uio info */
 	pdata->info.name = "uio_hv_generic";
 	pdata->info.version = DRIVER_VERSION;
 	pdata->info.irqcontrol = hv_uio_irqcontrol;
-	pdata->info.mmap = hv_uio_mmap;
 	pdata->info.irq = UIO_IRQ_CUSTOM;
 
 	/* mem resources */
 	pdata->info.mem[TXRX_RING_MAP].name = "txrx_rings";
 	pdata->info.mem[TXRX_RING_MAP].addr
-		= virt_to_phys(dev->channel->ringbuffer_pages);
+		= (phys_addr_t)dev->channel->ringbuffer_pages;
 	pdata->info.mem[TXRX_RING_MAP].size
-		= dev->channel->ringbuffer_pagecount * PAGE_SIZE;
+		= dev->channel->ringbuffer_pagecount << PAGE_SHIFT;
 	pdata->info.mem[TXRX_RING_MAP].memtype = UIO_MEM_LOGICAL;
 
 	pdata->info.mem[INT_PAGE_MAP].name = "int_page";
-	pdata->info.mem[INT_PAGE_MAP].addr =
-		virt_to_phys(vmbus_connection.int_page);
+	pdata->info.mem[INT_PAGE_MAP].addr
+		= (phys_addr_t)vmbus_connection.int_page;
 	pdata->info.mem[INT_PAGE_MAP].size = PAGE_SIZE;
 	pdata->info.mem[INT_PAGE_MAP].memtype = UIO_MEM_LOGICAL;
 
-	pdata->info.mem[MON_PAGE_MAP].name = "monitor_pages";
-	pdata->info.mem[MON_PAGE_MAP].addr =
-		virt_to_phys(vmbus_connection.monitor_pages[1]);
+	pdata->info.mem[MON_PAGE_MAP].name = "monitor_page";
+	pdata->info.mem[MON_PAGE_MAP].addr
+		= (phys_addr_t)vmbus_connection.monitor_pages[1];
 	pdata->info.mem[MON_PAGE_MAP].size = PAGE_SIZE;
 	pdata->info.mem[MON_PAGE_MAP].memtype = UIO_MEM_LOGICAL;
+	pdata->recv_buf = vzalloc(RECV_BUFFER_SIZE);
+	if (pdata->recv_buf == NULL) {
+		ret = -ENOMEM;
+		goto fail_close;
+	}
+
+	ret = vmbus_establish_gpadl(dev->channel, pdata->recv_buf,
+				    RECV_BUFFER_SIZE, &pdata->recv_gpadl);
+	if (ret)
+		goto fail_close;
+
+	/* put Global Physical Address Label in name */
+	snprintf(pdata->recv_name, sizeof(pdata->recv_name),
+		 "recv:%u", pdata->recv_gpadl);
+	pdata->info.mem[RECV_BUF_MAP].name = pdata->recv_name;
+	pdata->info.mem[RECV_BUF_MAP].addr
+		= (phys_addr_t)pdata->recv_buf;
+	pdata->info.mem[RECV_BUF_MAP].size = RECV_BUFFER_SIZE;
+	pdata->info.mem[RECV_BUF_MAP].memtype = UIO_MEM_VIRTUAL;
+
+
+	pdata->send_buf = vzalloc(SEND_BUFFER_SIZE);
+	if (pdata->send_buf == NULL) {
+		ret = -ENOMEM;
+		goto fail_close;
+	}
+
+	ret = vmbus_establish_gpadl(dev->channel, pdata->send_buf,
+				    SEND_BUFFER_SIZE, &pdata->send_gpadl);
+	if (ret)
+		goto fail_close;
+
+	snprintf(pdata->send_name, sizeof(pdata->send_name),
+		 "send:%u", pdata->send_gpadl);
+	pdata->info.mem[SEND_BUF_MAP].name = pdata->send_name;
+	pdata->info.mem[SEND_BUF_MAP].addr
+		= (phys_addr_t)pdata->send_buf;
+	pdata->info.mem[SEND_BUF_MAP].size = SEND_BUFFER_SIZE;
+	pdata->info.mem[SEND_BUF_MAP].memtype = UIO_MEM_VIRTUAL;
 
 	pdata->info.priv = pdata;
 	pdata->device = dev;
@@ -163,11 +232,14 @@
 		goto fail_close;
 	}
 
+	vmbus_set_chn_rescind_callback(dev->channel, hv_uio_rescind);
+
 	hv_set_drvdata(dev, pdata);
 
 	return 0;
 
 fail_close:
+	hv_uio_cleanup(dev, pdata);
 	vmbus_close(dev->channel);
 fail:
 	kfree(pdata);
@@ -184,6 +256,7 @@
 		return 0;
 
 	uio_unregister_device(&pdata->info);
+	hv_uio_cleanup(dev, pdata);
 	hv_set_drvdata(dev, NULL);
 	vmbus_close(dev->channel);
 	kfree(pdata);
@@ -200,8 +273,6 @@
 static int __init
 hyperv_module_init(void)
 {
-	mark_tech_preview("Hyper-V UIO generic driver", THIS_MODULE);
-
 	return vmbus_driver_register(&hv_uio_drv);
 }
 
diff -Naur linux-3.10.0-957.el7.orig/drivers/video/hyperv_fb.c linux-3.10.0-957.el7.lis/drivers/video/hyperv_fb.c
--- linux-3.10.0-957.el7.orig/drivers/video/hyperv_fb.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/drivers/video/hyperv_fb.c	2018-12-12 00:51:40.898784084 +0000
@@ -958,4 +958,5 @@
 module_exit(hvfb_drv_exit);
 
 MODULE_LICENSE("GPL");
+MODULE_VERSION(HV_DRV_VERSION);
 MODULE_DESCRIPTION("Microsoft Hyper-V Synthetic Video Frame Buffer Driver");
diff -Naur linux-3.10.0-957.el7.orig/include/linux/hv_compat.h linux-3.10.0-957.el7.lis/include/linux/hv_compat.h
--- linux-3.10.0-957.el7.orig/include/linux/hv_compat.h	1970-01-01 00:00:00.000000000 +0000
+++ linux-3.10.0-957.el7.lis/include/linux/hv_compat.h	2018-12-12 00:51:40.920783944 +0000
@@ -0,0 +1,709 @@
+
+#ifndef _HV_COMPAT_H
+#define _HV_COMPAT_H
+
+#include <linux/version.h>
+
+/*
+ *  * Helpers for determining EXTRAVERSION info on RHEL/CentOS update kernels
+ *   */
+#if defined(RHEL_RELEASE_VERSION)
+#define KERNEL_EXTRAVERSION(a,b) (((a) << 16) + (b))
+
+#define RHEL_RELEASE_UPDATE_VERSION(a,b,c,d) \
+	(((RHEL_RELEASE_VERSION(a,b)) << 32) + (KERNEL_EXTRAVERSION(c,d)))
+
+#if defined(EXTRAVERSION1) && defined (EXTRAVERSION2)
+#define RHEL_RELEASE_UPDATE_CODE \
+	RHEL_RELEASE_UPDATE_VERSION(RHEL_MAJOR,RHEL_MINOR,EXTRAVERSION1,EXTRAVERSION2)
+#else
+#define RHEL_RELEASE_UPDATE_CODE \
+	RHEL_RELEASE_UPDATE_VERSION(RHEL_MAJOR,RHEL_MINOR,0,0)
+#endif
+#endif
+
+
+//#if LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 35)
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(3, 10, 0)
+
+#define CN_KVP_IDX	0x9
+#define CN_KVP_VAL	0x1
+
+#define CN_VSS_IDX	0xA
+#define CN_VSS_VAL	0x1
+
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,2))
+#define rdtscll(now)    do { (now) = rdtsc_ordered(); } while (0)
+#endif
+
+#define HV_DRV_VERSION	"4.2.6"
+#define _HV_DRV_VERSION 0x1AA
+
+#ifdef __KERNEL__
+
+#include <linux/rcupdate.h>
+#include <linux/version.h>
+#include <linux/netdevice.h>
+#include <linux/inetdevice.h>
+#include <asm/pgtable_types.h>
+#include <net/arp.h>
+#include <scsi/scsi.h>
+#include <scsi/scsi_cmnd.h>
+#include <scsi/scsi_dbg.h>
+#include <scsi/scsi_device.h>
+#include <scsi/scsi_driver.h>
+#include <scsi/scsi_eh.h>
+#include <scsi/scsi_host.h>
+#include <scsi/scsi_transport_fc.h>
+#include <net/tcp_states.h>
+#include <net/sock.h>
+
+#define CN_KVP_IDX	0x9
+
+#define CN_VSS_IDX	0xA
+#define CN_VSS_VAL	0x1
+
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(6,4))
+#ifdef CONFIG_MEMORY_HOTPLUG
+#undef CONFIG_MEMORY_HOTPLUG
+#endif
+#endif
+
+#ifndef pr_warn
+#define pr_warn(fmt, arg...) printk(KERN_WARNING fmt, ##arg)
+#endif
+
+#ifndef HV_STATUS_INSUFFICIENT_BUFFERS
+#define HV_STATUS_INSUFFICIENT_BUFFERS	19
+#endif
+
+#ifndef RNDIS_STATUS_NETWORK_CHANGE
+#define RNDIS_STATUS_NETWORK_CHANGE 0x40010018
+#endif
+
+#ifndef NETIF_F_HW_VLAN_CTAG_TX
+#define NETIF_F_HW_VLAN_CTAG_TX 0
+#endif
+
+#ifndef DID_TARGET_FAILURE
+#define DID_TARGET_FAILURE	0x10
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+#define skb_vlan_tag_present(__skb)	((__skb)->vlan_tci & VLAN_TAG_PRESENT)
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+#define napi_consume_skb(skb, budget)     dev_consume_skb_any(skb)
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,3))
+static inline struct page *skb_frag_page(const skb_frag_t *frag)
+{
+	return frag->page;
+}
+
+static inline unsigned int skb_frag_size(const skb_frag_t *frag)
+{
+	return frag->size;
+}
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,4))
+#define hid_err(x, y)
+#endif
+
+#define blk_queue_max_segments(a, b)
+
+extern bool using_null_legacy_pic;
+
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(6,0))
+static inline void *vzalloc(unsigned long size)
+{
+	void *ptr;
+	ptr = vmalloc(size);
+	memset(ptr, 0, size);
+	return ptr;
+}
+#endif
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,2))
+#define NETIF_F_RXCSUM 0
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,6))
+static inline void
+skb_set_hash(struct sk_buff *skb, __u32 hash, int type)
+{
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(6,6))
+        skb->rxhash = hash;
+#endif
+}
+#endif
+
+bool netvsc_set_hash(u32 *hash, struct sk_buff *skb);
+
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,1))
+static inline __u32
+skb_get_hash(struct sk_buff *skb)
+{
+#if (RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(7,0))
+        return skb->hash;
+#else
+	__u32 hash;
+	if (netvsc_set_hash(&hash, skb))
+		return hash;
+	return 0;
+#endif
+}
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+struct pcpu_sw_netstats {
+	u64     rx_packets;
+	u64     rx_bytes;
+	u64     tx_packets;
+	u64     tx_bytes;
+	struct u64_stats_sync   syncp;
+};
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))
+static inline void pm_wakeup_event(struct device *dev, unsigned int msec)
+{
+}
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,4))
+static inline int kstrtouint(const char *s, unsigned int base, unsigned int *res)
+{
+	int result;
+	char *endbufp = NULL;
+
+	result = (int)simple_strtoul(s, &endbufp, 10);
+	return result;
+}
+
+#endif
+
+#define PTE_SHIFT ilog2(PTRS_PER_PTE)
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,1))
+static inline void reinit_completion(struct completion *x)
+{
+	x->done = 0;
+}
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))
+static inline int page_level_shift(int level)
+{
+        return (PAGE_SHIFT - PTE_SHIFT) + level * PTE_SHIFT;
+}
+#endif
+
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))
+static inline unsigned long page_level_size(int level)
+{
+	return 1UL << page_level_shift(level);
+}
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))
+static inline unsigned long page_level_mask(int level)
+{
+	return ~(page_level_size(level) - 1);
+}
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,0))
+static inline phys_addr_t slow_virt_to_phys(void *__virt_addr)
+{
+	unsigned long virt_addr = (unsigned long)__virt_addr;
+	phys_addr_t phys_addr;
+	unsigned long offset;
+	int level;
+	unsigned long psize;
+	unsigned long pmask;
+	pte_t *pte;
+
+	pte = lookup_address(virt_addr, &level);
+	BUG_ON(!pte);
+	psize = page_level_size(level);
+	pmask = page_level_mask(level);
+	offset = virt_addr & ~pmask;
+	phys_addr = (phys_addr_t)pte_pfn(*pte) << PAGE_SHIFT;
+	return (phys_addr | offset);
+}
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(6,4))
+/*
+ * For Hyper-V devices we use the device guid as the id.
+ * This was introduced in Linux 3.2 (/include/linux/mod_devicetable.h)
+ */
+struct hv_vmbus_device_id {
+	__u8 guid[16];
+	unsigned long driver_data;
+};
+
+#ifndef netdev_err
+static inline void netdev_err(struct net_device *net, const char *fmt, ...)
+{
+	va_list args;
+
+	va_start(args, fmt);
+	vprintk(fmt, args);
+	va_end(args);
+}
+
+#endif
+#endif
+
+#ifndef netdev_dbg
+#if defined(DEBUG)
+#define netdev_dbg(dev, fmt, ...)  netdev_err(dev, fmt, ...)
+#else
+#define netdev_dbg(__dev, format, args...)                      \
+({                                                              \
+	if (0)                                                  \
+		netdev_err(__dev, format, ##args); \
+	0;                                                      \
+})
+
+#endif
+#endif
+
+
+#if (RHEL_RELEASE_CODE == RHEL_RELEASE_VERSION(6,0)) && \
+LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 32)
+static inline void  netif_notify_peers(struct net_device *net)
+{
+	struct in_device *idev;
+
+	rcu_read_lock();
+	if (((idev = __in_dev_get_rcu(net)) != NULL) &&
+		idev->ifa_list != NULL) {
+		arp_send(ARPOP_REQUEST, ETH_P_ARP,
+		idev->ifa_list->ifa_address, net,
+		idev->ifa_list->ifa_address, NULL,
+		net->dev_addr, NULL);
+	}
+	rcu_read_unlock();
+}
+
+#endif
+
+/* 
+ * The following snippets are from include/linux/u64_stats_sync.h
+ *
+ *  * In case irq handlers can update u64 counters, readers can use following helpers
+ *   * - SMP 32bit arches use seqcount protection, irq safe.
+ *    * - UP 32bit must disable irqs.
+ *     * - 64bit have no problem atomically reading u64 values, irq safe.
+ *      */
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+static inline unsigned int u64_stats_fetch_begin_irq(const struct u64_stats_sync *syncp)
+{
+#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+	return read_seqcount_begin(&syncp->seq);
+#else
+#if BITS_PER_LONG==32
+	local_irq_disable();
+#endif
+	return 0;
+#endif
+}
+
+static inline bool u64_stats_fetch_retry_irq(const struct u64_stats_sync *syncp,
+					 unsigned int start)
+{
+#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+	return read_seqcount_retry(&syncp->seq, start);
+#else
+#if BITS_PER_LONG==32
+	local_irq_enable();
+#endif
+	return false;
+#endif
+}
+#endif
+
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,0))
+static inline void u64_stats_init(struct u64_stats_sync *syncp)
+{
+#if BITS_PER_LONG == 32 && defined(CONFIG_SMP)
+	seqcount_init(&syncp->seq);
+#endif
+}
+
+#define netdev_alloc_pcpu_stats(type)				\
+({								\
+	typeof(type) __percpu *pcpu_stats = alloc_percpu(type); \
+	if (pcpu_stats)	{					\
+		int __cpu;					\
+		for_each_possible_cpu(__cpu) {			\
+			typeof(type) *stat;			\
+			stat = per_cpu_ptr(pcpu_stats, __cpu);	\
+			u64_stats_init(&stat->syncp);		\
+		}						\
+	}							\
+	pcpu_stats;						\
+})
+#endif
+
+/*
+ * Define Infiniband MLX4 dependencies for RDMA driver
+ */
+struct mlx4_ib_create_cq {
+	__u64	buf_addr;
+	__u64	db_addr;
+};
+
+struct mlx4_ib_create_qp {
+	__u64	buf_addr;
+	__u64	db_addr;
+	__u8	log_sq_bb_count;
+	__u8	log_sq_stride;
+	__u8	sq_no_prefetch;
+	__u8	reserved[5];
+};
+
+struct mlx4_ib_alloc_ucontext_resp {
+	__u32	dev_caps;
+	__u32	qp_tab_size;
+	__u16	bf_reg_size;
+	__u16	bf_regs_per_page;
+	__u32	cqe_size;
+};
+
+/*
+ * The following READ_ONCE macro is included from
+ * tools/include/linux/compiler.h from upstream.
+ */
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+
+#define __READ_ONCE_SIZE                                                \
+({                                                                      \
+        switch (size) {                                                 \
+        case 1: *(__u8 *)res = *(volatile __u8 *)p; break;              \
+        case 2: *(__u16 *)res = *(volatile __u16 *)p; break;            \
+        case 4: *(__u32 *)res = *(volatile __u32 *)p; break;            \
+        case 8: *(__u64 *)res = *(volatile __u64 *)p; break;            \
+        default:                                                        \
+                barrier();                                              \
+                __builtin_memcpy((void *)res, (const void *)p, size);   \
+                barrier();                                              \
+        }                                                               \
+})
+
+static __always_inline
+void __read_once_size(const volatile void *p, void *res, int size)
+{
+        __READ_ONCE_SIZE;
+}
+
+/*
+ * Prevent the compiler from merging or refetching reads or writes. The
+ * compiler is also forbidden from reordering successive instances of
+ * READ_ONCE, WRITE_ONCE and ACCESS_ONCE (see below), but only when the
+ * compiler is aware of some particular ordering.  One way to make the
+ * compiler aware of ordering is to put the two invocations of READ_ONCE,
+ * WRITE_ONCE or ACCESS_ONCE() in different C statements.
+ * 
+ * In contrast to ACCESS_ONCE these two macros will also work on aggregate
+ * data types like structs or unions. If the size of the accessed data
+ * type exceeds the word size of the machine (e.g., 32 bits or 64 bits)
+ * READ_ONCE() and WRITE_ONCE()  will fall back to memcpy and print a
+ * compile-time warning.
+ * 
+ * Their two major use cases are: (1) Mediating communication between
+ * process-level code and irq/NMI handlers, all running on the same CPU,
+ * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
+ * mutilate accesses that either do not require ordering or that interact
+ * with an explicit memory barrier or atomic instruction that provides the
+ * required ordering.
+ */
+
+#define READ_ONCE(x) \
+        ({ union { typeof(x) __val; char __c[1]; } __u; __read_once_size(&(x), __u.__c, sizeof(x)); __u.__val; })
+#endif
+
+/*
+ * Define VMSock driver dependencies here
+ */
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,4))
+static inline int memcpy_from_msg(void *data, struct msghdr *msg, int len)
+{
+	return memcpy_fromiovec(data, msg->msg_iov, len);
+}
+#endif
+
+static inline int memcpy_to_msg(struct msghdr *msg, void *data, int len)
+{
+	return memcpy_toiovec(msg->msg_iov, data, len);
+}
+
+/*
+ * Define ethtool dependencies here.
+ */
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+static inline int ethtool_validate_speed(__u32 speed)
+{
+        return speed <= INT_MAX || speed == SPEED_UNKNOWN;
+}
+#endif
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+static inline int ethtool_validate_duplex(__u8 duplex)
+{
+        switch (duplex) {
+        case DUPLEX_HALF:
+        case DUPLEX_FULL:
+        case DUPLEX_UNKNOWN:
+                return 1;
+        }
+
+        return 0;
+}
+#endif
+
+/*
+ * Define balloon driver dependencies here.
+ */
+
+// In-kernel memory onlining is not supported in older kernels.
+#define memhp_auto_online 0;
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+static inline long si_mem_available(void)
+{
+	struct sysinfo val;
+	si_meminfo(&val);
+	return val.freeram;
+}
+#endif
+
+/*
+ * The function dev_consume_skb_any() was exposed in RHEL 7.2.
+ * Provide an inline function for the older versions.
+ */
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,2))
+static inline void dev_consume_skb_any(struct sk_buff *skb)
+{
+	dev_kfree_skb_any(skb);
+}
+#endif
+
+
+#if defined(RHEL_RELEASE_UPDATE_CODE) && \
+(RHEL_RELEASE_UPDATE_CODE < RHEL_RELEASE_UPDATE_VERSION(7, 2, 327, 36))
+/* This helper checks if a socket is a full socket,
+ * ie _not_ a timewait socket.
+ */
+static inline bool sk_fullsock(const struct sock *sk)
+{
+        return (1 << sk->sk_state) & ~(TCPF_TIME_WAIT);
+}
+#endif
+
+static inline struct cpumask *irq_data_get_affinity_mask(struct irq_data *d)
+{
+	return d->affinity;
+}
+
+
+#define timespec64 timespec
+#define ns_to_timespec64 ns_to_timespec
+#define do_settimeofday64 do_settimeofday
+
+/**
+ * fc_eh_timed_out - FC Transport I/O timeout intercept handler
+ * @scmd:	The SCSI command which timed out
+ *
+ * This routine protects against error handlers getting invoked while a
+ * rport is in a blocked state, typically due to a temporarily loss of
+ * connectivity. If the error handlers are allowed to proceed, requests
+ * to abort i/o, reset the target, etc will likely fail as there is no way
+ * to communicate with the device to perform the requested function. These
+ * failures may result in the midlayer taking the device offline, requiring
+ * manual intervention to restore operation.
+ *
+ * This routine, called whenever an i/o times out, validates the state of
+ * the underlying rport. If the rport is blocked, it returns
+ * EH_RESET_TIMER, which will continue to reschedule the timeout.
+ * Eventually, either the device will return, or devloss_tmo will fire,
+ * and when the timeout then fires, it will be handled normally.
+ * If the rport is not blocked, normal error handling continues.
+ *
+ * Notes:
+ *	This routine assumes no locks are held on entry.
+ */
+static inline enum blk_eh_timer_return fc_eh_timed_out(struct scsi_cmnd *scmd)
+{
+	struct fc_rport *rport = starget_to_rport(scsi_target(scmd->device));
+
+	if (rport && rport->port_state == FC_PORTSTATE_BLOCKED)
+		return BLK_EH_RESET_TIMER;
+
+	return BLK_EH_NOT_HANDLED;
+}
+
+/**
+ * required for daf0cd445a218314f9461d67d4f2b9c24cdd534b
+ */
+#define FC_PORT_ROLE_FCP_DUMMY_INITIATOR        0x08
+
+/**
+ * refcount_t - variant of atomic_t specialized for reference counts
+ * @refs: atomic_t counter field
+ *
+ * The counter saturates at UINT_MAX and will not move once
+ * there. This avoids wrapping the counter and causing 'spurious'
+ * use-after-free bugs.
+ */
+typedef struct refcount_struct {
+	atomic_t refs;
+} refcount_t;
+
+static inline bool refcount_sub_and_test(unsigned int i, refcount_t *r)
+{
+	unsigned int old, new, val = atomic_read(&r->refs);
+
+	do {
+		if (unlikely(val == UINT_MAX))
+			return false;
+
+		new = val - i;
+		if (new > val) {
+			WARN_ONCE(new > val, "refcount_t: underflow; use-after-free.\n");
+			return false;
+		}
+
+		old = atomic_cmpxchg(&r->refs, val, new);
+		if (old == val)
+			break;
+
+		val = old;
+	} while (1);
+
+	return !new;
+}
+
+static inline bool refcount_dec_and_test(refcount_t *r)
+{
+	return refcount_sub_and_test(1, r);
+}
+
+static inline void refcount_set(refcount_t *r, unsigned int n)
+{
+	atomic_set(&r->refs, n);
+}
+
+#define netdev_lockdep_set_classes(dev)				\
+{								\
+	static struct lock_class_key qdisc_tx_busylock_key;	\
+	static struct lock_class_key qdisc_xmit_lock_key;	\
+	static struct lock_class_key dev_addr_list_lock_key;	\
+	unsigned int i;						\
+								\
+	(dev)->qdisc_tx_busylock = &qdisc_tx_busylock_key;	\
+	lockdep_set_class(&(dev)->addr_list_lock,		\
+			  &dev_addr_list_lock_key); 		\
+	for (i = 0; i < (dev)->num_tx_queues; i++)		\
+		lockdep_set_class(&(dev)->_tx[i]._xmit_lock,	\
+				  &qdisc_xmit_lock_key);	\
+}
+
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,5))
+static inline int cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap)
+{
+        int next;
+
+again:
+        next = cpumask_next(n, mask);
+
+        if (wrap && n < start && next >= start) {
+                return nr_cpumask_bits;
+
+        } else if (next >= nr_cpumask_bits) {
+                wrap = true;
+                n = -1;
+                goto again;
+        }
+
+        return next;
+}
+#endif
+
+#define for_each_cpu_wrap(cpu, mask, start)                                     \
+        for ((cpu) = cpumask_next_wrap((start)-1, (mask), (start), false);      \
+             (cpu) < nr_cpumask_bits;                                           \
+             (cpu) = cpumask_next_wrap((cpu), (mask), (start), true))
+
+
+#ifndef GENMASK_ULL
+#define GENMASK_ULL(h, l) \
+	(((~0ULL) - (1ULL << (l)) + 1) & \
+	 (~0ULL >> (BITS_PER_LONG_LONG - 1 - (h))))
+#endif
+
+#ifndef U64_MAX
+#define U64_MAX                ((u64)~0ULL)
+#endif
+
+#ifndef for_each_cpu_wrap
+/**
+ * for_each_cpu_wrap - iterate over every cpu in a mask, starting at a specified location
+ * @cpu: the (optionally unsigned) integer iterator
+ * @mask: the cpumask poiter
+ * @start: the start location
+ *
+ * The implementation does not assume any bit in @mask is set (including @start).
+ *
+ * After the loop, cpu is >= nr_cpu_ids.
+ */
+extern int cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap);
+
+#define for_each_cpu_wrap(cpu, mask, start)					\
+	for ((cpu) = cpumask_next_wrap((start)-1, (mask), (start), false);	\
+	     (cpu) < nr_cpumask_bits;						\
+	     (cpu) = cpumask_next_wrap((cpu), (mask), (start), true))
+#endif
+
+#ifndef __ASSEMBLY__
+
+#ifndef __ASM_FORM_RAW
+#define __ASM_FORM_RAW(x)     #x
+
+#ifndef __x86_64__
+/* 32 bit */
+# define __ASM_SEL_RAW(a,b) __ASM_FORM_RAW(a)
+#else
+/* 64 bit */
+# define __ASM_SEL_RAW(a,b) __ASM_FORM_RAW(b)
+#endif
+
+#ifdef __ASM_REG
+#undef __ASM_REG
+#define __ASM_REG(reg)         __ASM_SEL_RAW(e##reg, r##reg)
+#endif
+
+#endif
+
+#define _ASM_SP                __ASM_REG(sp)
+
+/*
+ * This output constraint should be used for any inline asm which has a "call"
+ * instruction.  Otherwise the asm may be inserted before the frame pointer
+ * gets set up by the containing function.  If you forget to do this, objtool
+ * may print a "call without frame pointer save/setup" warning.
+ */
+register unsigned long current_stack_pointer asm(_ASM_SP);
+#define ASM_CALL_CONSTRAINT "+r" (current_stack_pointer)
+#endif
+
+#endif //#ifdef __KERNEL__
+#endif //#if LINUX_VERSION_CODE <= KERNEL_VERSION(3, 10, 0)
+#endif //#ifndef _HV_COMPAT_H
diff -Naur linux-3.10.0-957.el7.orig/include/linux/hyperv.h linux-3.10.0-957.el7.lis/include/linux/hyperv.h
--- linux-3.10.0-957.el7.orig/include/linux/hyperv.h	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/include/linux/hyperv.h	2018-12-12 00:51:40.930783880 +0000
@@ -25,8 +25,8 @@
 #ifndef _HYPERV_H
 #define _HYPERV_H
 
-#include <uapi/linux/hyperv.h>
-#include <uapi/asm/hyperv.h>
+#include "../uapi/linux/hyperv.h"
+#include <asm/hyperv.h>
 
 #include <linux/types.h>
 #include <linux/scatterlist.h>
@@ -36,7 +36,6 @@
 #include <linux/device.h>
 #include <linux/mod_devicetable.h>
 #include <linux/interrupt.h>
-#include <linux/reciprocal_div.h>
 
 #define MAX_PAGE_BUFFER_COUNT				32
 #define MAX_MULTIPAGE_BUFFER_COUNT			32 /* 128K */
@@ -122,12 +121,9 @@
 struct hv_ring_buffer_info {
 	struct hv_ring_buffer *ring_buffer;
 	u32 ring_size;			/* Include the shared header */
-	struct reciprocal_value ring_size_div10_reciprocal;
 	spinlock_t ring_lock;
 
 	u32 ring_datasize;		/* < ring_size */
-	u32 ring_data_startoffset;
-	u32 priv_write_index;
 	u32 priv_read_index;
 };
 
@@ -159,16 +155,6 @@
 	return write;
 }
 
-static inline u32 hv_get_avail_to_write_percent(
-		const struct hv_ring_buffer_info *rbi)
-{
-	u32 avail_write = hv_get_bytes_to_write(rbi);
-
-	return reciprocal_divide(
-			(avail_write  << 3) + (avail_write << 1),
-			rbi->ring_size_div10_reciprocal);
-}
-
 /*
  * VMBUS version is 32 bit entity broken up into
  * two 16 bit quantities: major_number. minor_number.
@@ -576,6 +562,7 @@
 	u32 gpadl;
 } __packed;
 
+
 struct vmbus_channel_relid_released {
 	struct vmbus_channel_message_header header;
 	u32 child_relid;
@@ -658,6 +645,11 @@
 	HV_LOCALIZED,
 };
 
+enum hv_channel_affinity_mode {
+	HV_KEEP_HT_CPU = 0,
+	HV_SKIP_HT_CPU,
+};
+
 enum vmbus_device_type {
 	HV_IDE = 0,
 	HV_SCSI,
@@ -680,11 +672,17 @@
 
 struct vmbus_device {
 	u16  dev_type;
-	uuid_le guid;
+	/* deviation from upstream - NHM */
+#if (RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(7,3))
+	__u8 guid[16];
+#else
+		uuid_le guid;
+#endif
 	bool perf_device;
 };
 
 struct vmbus_channel {
+
 	struct list_head listentry;
 
 	struct hv_device *device_obj;
@@ -712,10 +710,6 @@
 
 	struct vmbus_close_msg close_msg;
 
-	/* Statistics */
-	u64	interrupts;	/* Host to Guest interrupts */
-	u64	sig_events;	/* Guest to Host events */
-
 	/* Channel callback's invoked in softirq context */
 	struct tasklet_struct callback_event;
 	void (*onchannel_callback)(void *context);
@@ -783,7 +777,7 @@
 	 * Channel rescind callback. Some channels (the hvsock ones), need to
 	 * register a callback which is invoked in vmbus_onoffer_rescind().
 	 */
-	void (*chn_rescind_callback)(struct vmbus_channel *channel);
+	void (*chn_rescind_callback) (struct vmbus_channel *);
 
 	/*
 	 * The spinlock to protect the structure. It is being used to protect
@@ -795,15 +789,7 @@
 	 * All Sub-channels of a primary channel are linked here.
 	 */
 	struct list_head sc_list;
-	/*
-	 * Current number of sub-channels.
-	 */
-	int num_sc;
-	/*
-	 * Number of a sub-channel (position within sc_list) which is supposed
-	 * to be used as the next outgoing channel.
-	 */
-	int next_oc;
+
 	/*
 	 * The primary channel this sub-channel belongs to.
 	 * This will be NULL for the primary channel.
@@ -870,6 +856,13 @@
 
 	bool probe_done;
 
+	/*
+	 * We must offload the handling of the primary/sub channels
+	 * from the single-threaded vmbus_connection.work_queue to
+	 * two different workqueue, otherwise we can block
+	 * vmbus_connection.work_queue and hang: see vmbus_process_offer().
+	 */
+	struct work_struct add_channel_work;
 };
 
 static inline bool is_hvsock_channel(const struct vmbus_channel *c)
@@ -928,15 +921,7 @@
 			void (*sc_cr_cb)(struct vmbus_channel *new_sc));
 
 void vmbus_set_chn_rescind_callback(struct vmbus_channel *channel,
-		void (*chn_rescind_cb)(struct vmbus_channel *));
-
-/*
- * Retrieve the (sub) channel on which to send an outgoing request.
- * When a primary channel has multiple sub-channels, we choose a
- * channel whose VCPU binding is closest to the VCPU on which
- * this call is being made.
- */
-struct vmbus_channel *vmbus_get_outgoing_channel(struct vmbus_channel *primary);
+		void (*chn_rescind_cb) (struct vmbus_channel *));
 
 /*
  * Check if sub-channels have already been offerred. This API will be useful
@@ -1039,7 +1024,6 @@
 				     u32 *buffer_actual_len,
 				     u64 *requestid);
 
-
 extern void vmbus_ontimer(unsigned long data);
 
 /* Base driver object */
@@ -1066,12 +1050,6 @@
 
 	struct device_driver driver;
 
-	/* dynamic device GUID's */
-	struct  {
-		spinlock_t lock;
-		struct list_head list;
-	} dynids;
-
 	int (*probe)(struct hv_device *, const struct hv_vmbus_device_id *);
 	int (*remove)(struct hv_device *);
 	void (*shutdown)(struct hv_device *);
@@ -1140,6 +1118,7 @@
 			resource_size_t min, resource_size_t max,
 			resource_size_t size, resource_size_t align,
 			bool fb_overlap_ok);
+
 void vmbus_free_mmio(resource_size_t start, resource_size_t size);
 
 /**
@@ -1153,13 +1132,169 @@
 	.guid = { g0, g1, g2, g3, g4, g5, g6, g7,	\
 		  g8, g9, ga, gb, gc, gd, ge, gf },
 
+/*
+ * GUID definitions of various offer types - services offered to the guest.
+ */
 
+#if (RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(7,2))
 
 /*
- * GUID definitions of various offer types - services offered to the guest.
+ * Network GUID
+ * {f8615163-df3e-46c5-913f-f2d2f965ed0e}
+ */
+#define HV_NIC_GUID \
+	.guid = { \
+			0x63, 0x51, 0x61, 0xf8, 0x3e, 0xdf, 0xc5, 0x46, \
+			0x91, 0x3f, 0xf2, 0xd2, 0xf9, 0x65, 0xed, 0x0e \
+		}
+
+/*
+ * IDE GUID
+ * {32412632-86cb-44a2-9b5c-50d1417354f5}
+ */
+#define HV_IDE_GUID \
+	.guid = { \
+			0x32, 0x26, 0x41, 0x32, 0xcb, 0x86, 0xa2, 0x44, \
+			0x9b, 0x5c, 0x50, 0xd1, 0x41, 0x73, 0x54, 0xf5 \
+		}
+
+/*
+ * SCSI GUID
+ * {ba6163d9-04a1-4d29-b605-72e2ffb1dc7f}
  */
+#define HV_SCSI_GUID \
+	.guid = { \
+			0xd9, 0x63, 0x61, 0xba, 0xa1, 0x04, 0x29, 0x4d, \
+			0xb6, 0x05, 0x72, 0xe2, 0xff, 0xb1, 0xdc, 0x7f \
+		}
 
 /*
+ * Shutdown GUID
+ * {0e0b6031-5213-4934-818b-38d90ced39db}
+ */
+#define HV_SHUTDOWN_GUID \
+	.guid = { \
+			0x31, 0x60, 0x0b, 0x0e, 0x13, 0x52, 0x34, 0x49, \
+			0x81, 0x8b, 0x38, 0xd9, 0x0c, 0xed, 0x39, 0xdb \
+		}
+
+/*
+ * Time Synch GUID
+ * {9527E630-D0AE-497b-ADCE-E80AB0175CAF}
+ */
+#define HV_TS_GUID \
+	.guid = { \
+			0x30, 0xe6, 0x27, 0x95, 0xae, 0xd0, 0x7b, 0x49, \
+			0xad, 0xce, 0xe8, 0x0a, 0xb0, 0x17, 0x5c, 0xaf \
+		}
+
+/*
+ * Heartbeat GUID
+ * {57164f39-9115-4e78-ab55-382f3bd5422d}
+ */
+#define HV_HEART_BEAT_GUID \
+	.guid = { \
+			0x39, 0x4f, 0x16, 0x57, 0x15, 0x91, 0x78, 0x4e, \
+			0xab, 0x55, 0x38, 0x2f, 0x3b, 0xd5, 0x42, 0x2d \
+		}
+
+/*
+ * KVP GUID
+ * {a9a0f4e7-5a45-4d96-b827-8a841e8c03e6}
+ */
+#define HV_KVP_GUID \
+	.guid = { \
+			0xe7, 0xf4, 0xa0, 0xa9, 0x45, 0x5a, 0x96, 0x4d, \
+			0xb8, 0x27, 0x8a, 0x84, 0x1e, 0x8c, 0x3,  0xe6 \
+		}
+
+/*
+ * Dynamic memory GUID
+ * {525074dc-8985-46e2-8057-a307dc18a502}
+ */
+#define HV_DM_GUID \
+	.guid = { \
+			0xdc, 0x74, 0x50, 0X52, 0x85, 0x89, 0xe2, 0x46, \
+			0x80, 0x57, 0xa3, 0x07, 0xdc, 0x18, 0xa5, 0x02 \
+		}
+
+/*
+ * Mouse GUID
+ * {cfa8b69e-5b4a-4cc0-b98b-8ba1a1f3f95a}
+ */
+#define HV_MOUSE_GUID \
+	.guid = { \
+			0x9e, 0xb6, 0xa8, 0xcf, 0x4a, 0x5b, 0xc0, 0x4c, \
+			0xb9, 0x8b, 0x8b, 0xa1, 0xa1, 0xf3, 0xf9, 0x5a \
+		}
+/*
+ * Keyboard GUID
+ * {f912ad6d-2b17-48ea-bd65-f927a61c7684}
+ */
+#define HV_KBD_GUID \
+	.guid = { \
+			0x6d, 0xad, 0x12, 0xf9, 0x17, 0x2b, 0xea, 0x48, \
+			0xbd, 0x65, 0xf9, 0x27, 0xa6, 0x1c, 0x76, 0x84 \
+		}
+
+/*
+ * VSS (Backup/Restore) GUID
+ */
+#define HV_VSS_GUID \
+	.guid = { \
+			0x29, 0x2e, 0xfa, 0x35, 0x23, 0xea, 0x36, 0x42, \
+			0x96, 0xae, 0x3a, 0x6e, 0xba, 0xcb, 0xa4,  0x40 \
+		}
+/*
+ * Synthetic Video GUID
+ * {DA0A7802-E377-4aac-8E77-0558EB1073F8}
+ */
+#define HV_SYNTHVID_GUID \
+	.guid = { \
+			0x02, 0x78, 0x0a, 0xda, 0x77, 0xe3, 0xac, 0x4a, \
+			0x8e, 0x77, 0x05, 0x58, 0xeb, 0x10, 0x73, 0xf8 \
+		}
+
+/*
+ * Synthetic FC GUID
+ * {2f9bcc4a-0069-4af3-b76b-6fd0be528cda}
+ */
+#define HV_SYNTHFC_GUID \
+	.guid = { \
+			0x4A, 0xCC, 0x9B, 0x2F, 0x69, 0x00, 0xF3, 0x4A, \
+			0xB7, 0x6B, 0x6F, 0xD0, 0xBE, 0x52, 0x8C, 0xDA \
+		}
+
+/*
+ * Guest File Copy Service
+ * {34D14BE3-DEE4-41c8-9AE7-6B174977C192}
+ */
+
+#define HV_FCOPY_GUID \
+	.guid = { \
+			0xE3, 0x4B, 0xD1, 0x34, 0xE4, 0xDE, 0xC8, 0x41, \
+			0x9A, 0xE7, 0x6B, 0x17, 0x49, 0x77, 0xC1, 0x92 \
+		}
+/*
+ * NetworkDirect. This is the guest RDMA service.
+ * {8c2eaf3d-32a7-4b09-ab99-bd1f1c86b501}
+ */
+#define HV_ND_GUID \
+	.guid = { \
+			0x3d, 0xaf, 0x2e, 0x8c, 0xa7, 0x32, 0x09, 0x4b, \
+			0xab, 0x99, 0xbd, 0x1f, 0x1c, 0x86, 0xb5, 0x01 \
+		}
+/*
+ * PCI Express Pass Through
+ * {44C4F61D-4444-4400-9D52-802E27EDE19F}
+ */
+#define HV_PCIE_GUID \
+	.guid = { \
+			0x1D, 0xF6, 0xC4, 0x44, 0x44, 0x44, 0x00, 0x44, \
+			0x9D, 0x52, 0x80, 0x2E, 0x27, 0xED, 0xE1, 0x9F \
+		}
+#else
+/*
  * Network GUID
  * {f8615163-df3e-46c5-913f-f2d2f965ed0e}
  */
@@ -1287,6 +1422,9 @@
 	.guid = UUID_LE(0x44c4f61d, 0x4444, 0x4400, 0x9d, 0x52, \
 			0x80, 0x2e, 0x27, 0xed, 0xe1, 0x9f)
 
+#endif
+
+
 /*
  * Linux doesn't support the 3 devices: the first two are for
  * Automatic Virtual Machine Activation, and the third is for
@@ -1422,6 +1560,7 @@
 void hv_process_channel_removal(u32 relid);
 
 void vmbus_setevent(struct vmbus_channel *channel);
+
 /*
  * Negotiated version with the Host.
  */
@@ -1430,6 +1569,7 @@
 
 int vmbus_send_tl_connect_request(const uuid_le *shv_guest_servie_id,
 				  const uuid_le *shv_host_servie_id);
+
 void vmbus_set_event(struct vmbus_channel *channel);
 
 /* Get the start of the ring buffer. */
@@ -1485,7 +1625,6 @@
 	return (desc->len8 << 3) - (desc->offset8 << 3);
 }
 
-
 struct vmpacket_descriptor *
 hv_pkt_iter_first(struct vmbus_channel *channel);
 
diff -Naur linux-3.10.0-957.el7.orig/include/uapi/linux/hyperv.h linux-3.10.0-957.el7.lis/include/uapi/linux/hyperv.h
--- linux-3.10.0-957.el7.orig/include/uapi/linux/hyperv.h	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/include/uapi/linux/hyperv.h	2018-12-12 00:51:40.917783963 +0000
@@ -26,6 +26,12 @@
 #define _UAPI_HYPERV_H
 
 #include <linux/uuid.h>
+#include <linux/socket.h>
+
+#include <linux/version.h>
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(3,10,0)
+#include "../../linux/hv_compat.h"
+#endif
 
 /*
  * Framework version for util services.
@@ -140,10 +146,11 @@
 
 struct hv_do_fcopy {
 	struct hv_fcopy_hdr hdr;
+	__u32   pad;
 	__u64	offset;
 	__u32	size;
 	__u8	data[DATA_FRAGMENT];
-};
+} __attribute__((packed));
 
 /*
  * An implementation of HyperV key value pair (KVP) functionality for Linux.
diff -Naur linux-3.10.0-957.el7.orig/net/vmw_vsock/hyperv_transport.c linux-3.10.0-957.el7.lis/net/vmw_vsock/hyperv_transport.c
--- linux-3.10.0-957.el7.orig/net/vmw_vsock/hyperv_transport.c	2018-10-04 20:18:19.000000000 +0000
+++ linux-3.10.0-957.el7.lis/net/vmw_vsock/hyperv_transport.c	2018-12-12 00:51:40.860784325 +0000
@@ -19,10 +19,14 @@
  */
 #include <linux/module.h>
 #include <linux/vmalloc.h>
-#include <linux/hyperv.h>
 #include <net/sock.h>
 #include <net/af_vsock.h>
 
+#include <linux/hyperv.h>
+
+/* vsock-specific sock->sk_state constants */
+#define VSOCK_SS_LISTEN 255
+
 /* The host side's design of the feature requires 6 exact 4KB pages for
  * recv/send rings respectively -- this is suboptimal considering memory
  * consumption, however unluckily we have to live with it, before the
@@ -312,7 +316,7 @@
 
 	lock_sock(sk);
 
-	sk->sk_state = TCP_CLOSE;
+	sk->sk_state = SS_UNCONNECTED;
 	sock_set_flag(sk, SOCK_DONE);
 	vsk->peer_shutdown |= SEND_SHUTDOWN | RCV_SHUTDOWN;
 
@@ -350,8 +354,8 @@
 
 	lock_sock(sk);
 
-	if ((conn_from_host && sk->sk_state != TCP_LISTEN) ||
-	    (!conn_from_host && sk->sk_state != TCP_SYN_SENT))
+	if ((conn_from_host && sk->sk_state != VSOCK_SS_LISTEN) ||
+	    (!conn_from_host && sk->sk_state != SS_CONNECTING))
 		goto out;
 
 	if (conn_from_host) {
@@ -363,7 +367,7 @@
 		if (!new)
 			goto out;
 
-		new->sk_state = TCP_SYN_SENT;
+		new->sk_state = SS_CONNECTING;
 		vnew = vsock_sk(new);
 		hvs_new = vnew->trans;
 		hvs_new->chan = chan;
@@ -390,7 +394,7 @@
 	vmbus_set_chn_rescind_callback(chan, hvs_close_connection);
 
 	if (conn_from_host) {
-		new->sk_state = TCP_ESTABLISHED;
+		new->sk_state = SS_CONNECTED;
 		sk->sk_ack_backlog++;
 
 		hvs_addr_init(&vnew->local_addr, if_type);
@@ -403,7 +407,7 @@
 
 		vsock_enqueue_accept(sk, new);
 	} else {
-		sk->sk_state = TCP_ESTABLISHED;
+		sk->sk_state = SS_CONNECTED;
 		sk->sk_socket->state = SS_CONNECTED;
 
 		vsock_insert_connected(vsock_sk(sk));
@@ -488,7 +492,7 @@
 
 	lock_sock(sk);
 
-	sk->sk_state = TCP_CLOSING;
+	sk->sk_state = SS_DISCONNECTING;
 	vsock_remove_sock(vsk);
 
 	release_sock(sk);
@@ -574,8 +578,7 @@
 
 	recv_buf = (struct hvs_recv_buf *)(hvs->recv_desc + 1);
 	to_read = min_t(u32, len, hvs->recv_data_len);
-	ret = memcpy_toiovec(msg->msg_iov, recv_buf->data + hvs->recv_data_off,
-			     to_read);
+	ret = memcpy_to_msg(msg, recv_buf->data + hvs->recv_data_off, to_read);
 	if (ret != 0)
 		return ret;
 
@@ -888,9 +891,6 @@
 {
 	int ret;
 
-	mark_tech_preview("Hyper-V Virtual Sockets transport driver",
-			   THIS_MODULE);
-
 	if (vmbus_proto_version < VERSION_WIN10)
 		return -ENODEV;
 
